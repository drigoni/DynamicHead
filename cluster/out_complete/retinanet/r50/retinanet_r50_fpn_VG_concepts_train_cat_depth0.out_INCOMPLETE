
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 
Date:  gio 10 nov 2022, 05.53.25, CET
Directory:  /ceph/hpc/home/eudavider/repository/DynamicHead
Nodelist:  gn[22,24-25,29]
Number of nodes:  4
Ntasks per node:  1
NGPUs per node:  4
CUDA_VISIBLE_DEVICES:  0,1,2,3
TORCH_DEVICE_COUNT:  4
SLURM_MASTER_PORT:  11820
SLURM_MASTER_NODE:  gn22
SLURM_MASTER_ADDR:  gn22
SLURM_MASTER_URL:  tcp://gn22:11820
--------------------------------------------- 
MODEL_NUM_GPUS:  4
MODEL_NUM_MACHINES:  4
MODEL_BATCH_SIZE:  16
MODEL_MAX_ITER:  90000
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 

srun: error: WARNING: Multiple leaf switches contain nodes: gn[01-60]
[nltk_data] Downloading package omw-1.4 to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Command Line Args: Namespace(config_file='./configs/VG/retinanet/r50/retinanet_r50_fpn_VG_concepts_train_cat_depth0.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=4, machine_rank=0, dist_url='tcp://gn22:11820', opts=['SOLVER.IMS_PER_BATCH', '16', 'SOLVER.MAX_ITER', '90000'])
[nltk_data] Downloading package omw-1.4 to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data] Downloading package omw-1.4 to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data] Downloading package omw-1.4 to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data]   Package omw-1.4 is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data]   Package wordnet is already up-to-date!
Command Line Args: Namespace(config_file='./configs/VG/retinanet/r50/retinanet_r50_fpn_VG_concepts_train_cat_depth0.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=4, machine_rank=1, dist_url='tcp://gn22:11820', opts=['SOLVER.IMS_PER_BATCH', '16', 'SOLVER.MAX_ITER', '90000'])
Command Line Args: Namespace(config_file='./configs/VG/retinanet/r50/retinanet_r50_fpn_VG_concepts_train_cat_depth0.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=4, machine_rank=2, dist_url='tcp://gn22:11820', opts=['SOLVER.IMS_PER_BATCH', '16', 'SOLVER.MAX_ITER', '90000'])
Command Line Args: Namespace(config_file='./configs/VG/retinanet/r50/retinanet_r50_fpn_VG_concepts_train_cat_depth0.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=4, machine_rank=3, dist_url='tcp://gn22:11820', opts=['SOLVER.IMS_PER_BATCH', '16', 'SOLVER.MAX_ITER', '90000'])
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/VG/retinanet/r50/../base_retinanet_VG_concepts.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
[32m[11/10 05:54:19 detectron2]: [0mRank of current process: 0. World size: 16
[32m[11/10 05:54:24 detectron2]: [0mEnvironment info:
----------------------  ------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) [GCC 10.3.0]
numpy                   1.23.1
detectron2              0.6 @/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.3
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.0 @/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA A100-SXM4-40GB (arch=8.0)
Driver version          510.47.03
CUDA_HOME               /usr/local/cuda
Pillow                  9.2.0
torchvision             0.11.0 @/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[32m[11/10 05:54:24 detectron2]: [0mCommand line arguments: Namespace(config_file='./configs/VG/retinanet/r50/retinanet_r50_fpn_VG_concepts_train_cat_depth0.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=4, machine_rank=0, dist_url='tcp://gn22:11820', opts=['SOLVER.IMS_PER_BATCH', '16', 'SOLVER.MAX_ITER', '90000'])
[32m[11/10 05:54:24 detectron2]: [0mContents of args.config_file=./configs/VG/retinanet/r50/retinanet_r50_fpn_VG_concepts_train_cat_depth0.yaml:
_BASE_: "../base_retinanet_VG_concepts.yaml"
MODEL:
  WEIGHTS: "detectron2://ImageNetPretrained/MSRA/R-50.pkl"
  MASK_ON: False
  RESNETS:
    DEPTH: 50
CONCEPT:
  CONCEPT_FUSION: "cat" # ["cat", "mul", "add", "zeros"]
  DEPTH: 0
OUTPUT_DIR: "./results/VG/retinanet/retinanet_r50_fpn_VG_concepts_cat_depth0/"
[32m[11/10 05:54:24 detectron2]: [0mRunning with full config:
CONCEPT:
  ACTIVATE_CONCEPT_GENERATOR: true
  APPLY_CONDITION: true
  APPLY_CONDITION_FROM_FILE: false
  CONCEPT_FUSION: cat
  DEPTH: 0
  EXTERNAL_CONCEPTS_FOLDER: ./datasets/ewiser_concepts_oid_valid/
  FILE: ./concept/vg_to_synset.json
  ONLY_NAME: true
  UNIQUE: true
  VOCAB: ./concept/vocab.json
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 16
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - vg_val
  TRAIN:
  - vg_train
DEEPSETS:
  AGGREGATE: sum
  EMB: wordnet
  EMB_DIM: 150
  FILE: ./concept/wn30_holE_500_150_0.1_0.2_embeddings.pickle
  FREEZE: false
  MLP1_LAYERS:
  - 150
  - 256
  MLP1_OUTPUT_DIM: 256
  MLP2_LAYERS:
  - 256
  - 256
  OUTPUT_DIM: 256
EVALUATOR_TYPE: default
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 40.31747359663594
      - 50.79683366298238
    - - 64
      - 80.63494719327188
      - 101.59366732596476
    - - 128
      - 161.26989438654377
      - 203.18733465192952
    - - 256
      - 322.53978877308754
      - 406.37466930385904
    - - 512
      - 645.0795775461751
      - 812.7493386077181
  ATSS:
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CHANNELS: 256
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    INFERENCE_TH: 0.05
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_TH: 0.6
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    REG_LOSS_WEIGHT: 2.0
    TOPK: 9
    USE_GN: true
  BACKBONE:
    FREEZE_AT: -1
    NAME: build_retinanet_resnet_fpn_backbone
  DEVICE: cuda
  DYHEAD:
    CHANNELS: 256
    NUM_CONVS: 6
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: ConceptRetinaNet
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 103.53
  - 116.28
  - 123.675
  PIXEL_STD:
  - 1.0
  - 1.0
  - 1.0
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 1600
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.0
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 1600
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWINT:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 96
    MLP_RATIO: 4
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - stage2
    - stage3
    - stage4
    - stage5
    OUT_NORM: true
    VERSION: 1
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./results/VG/retinanet/retinanet_r50_fpn_VG_concepts_cat_depth0/
SEED: 2022
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: false
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: SGD
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 60000
  - 80000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[32m[11/10 05:54:24 detectron2]: [0mFull config saved to ./results/VG/retinanet/retinanet_r50_fpn_VG_concepts_cat_depth0/config.yaml
wandb: Currently logged in as: drigoni. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /ceph/hpc/scratch/user/eudavider/repository/DynamicHead/wandb/run-20221110_055427-m9od95xh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-resonance-483
wandb: ⭐️ View project at https://wandb.ai/drigoni/CATSS
wandb: 🚀 View run at https://wandb.ai/drigoni/CATSS/runs/m9od95xh
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
[32m[11/10 05:59:57 d2.engine.defaults]: [0mModel:
ConceptRetinaNet(
  (backbone): FPN(
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelP6P7(
      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (head): RetinaNetHead(
    (cls_subnet): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): ReLU()
    )
    (bbox_subnet): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): ReLU()
    )
    (cls_score): Conv2d(512, 14400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bbox_pred): Conv2d(512, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (anchor_generator): DefaultAnchorGenerator(
    (cell_anchors): BufferList()
  )
  (concept_net): ConceptNet(
    (concept_vocab): Vocab()
    (concept_emb): Embedding(82115, 150)
    (deepset): _DeepSets(
      (mlp1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=150, out_features=150, bias=True)
          (1): Linear(in_features=150, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mlp2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
)
[32m[11/10 06:00:20 d2.data.datasets.coco]: [0mLoading datasets/visual_genome/annotations/visual_genome_train.json takes 23.03 seconds.
[5m[31mWARNING[0m [32m[11/10 06:00:20 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[11/10 06:00:21 d2.data.datasets.coco]: [0mLoaded 98077 images in COCO format from datasets/visual_genome/annotations/visual_genome_train.json
[32m[11/10 06:00:31 d2.data.build]: [0mRemoved 908 images with no usable annotations. 97169 images left.
[32m[11/10 06:00:36 d2.data.build]: [0mDistribution of instances among all 1600 categories:
[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |
|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|
|     yolk      | 90           |     goal      | 122          |   bathroom    | 2205         |
|   macaroni    | 84           |    umpire     | 1428         |   toothpick   | 247          |
|  alarm clock  | 104          |  ceiling fan  | 125          |    photos     | 123          |
|    parrot     | 208          |   tail fin    | 121          | birthday cake | 107          |
|  calculator   | 87           |    catcher    | 1811         |    toilet     | 4779         |
|    batter     | 1435         |   stop sign   | 1169         |     cone      | 2459         |
|   microwave   | 1472         | skateboard .. | 82           |      tea      | 141          |
|    dugout     | 441          |   products    | 75           |    halter     | 76           |
|    kettle     | 370          |    kitchen    | 1662         | refrigerator  | 2528         |
|    ostrich    | 118          |    bathtub    | 671          |    blinds     | 1274         |
|     court     | 2626         |    urinal     | 623          |   knee pads   | 191          |
|      bed      | 5602         |   flamingo    | 223          |    giraffe    | 10340        |
|    helmet     | 10597        |   giraffes    | 1386         | tennis court  | 1553         |
|  motorcycle   | 7106         |    laptop     | 4695         |    tea pot    | 100          |
|     horse     | 10881        |  television   | 3463         |    shorts     | 10661        |
|    manhole    | 221          |  dishwasher   | 374          |     jeans     | 7076         |
|     sail      | 708          |    monitor    | 1989         |      man      | 85412        |
|     shirt     | 45511        |      car      | 24409        |      cat      | 8574         |
|  garage door  | 156          |      bus      | 10639        |   radiator    | 233          |
|    tights     | 122          |   sailboat    | 600          |    racket     | 4260         |
|     plate     | 16930        |   rock wall   | 124          |     beach     | 5333         |
|    trolley    | 235          |     ocean     | 3879         |   headboard   | 1010         |
|  tea kettle   | 125          |    wetsuit    | 1680         | tennis racket | 1935         |
|     sink      | 5480         |     train     | 14748        |   keyboard    | 3796         |
|      sky      | 38187        |     match     | 108          | train station | 409          |
|    stereo     | 68           |     bats      | 82           | tennis player | 1030         |
| toilet brush  | 106          |    lighter    | 77           | pepper shaker | 126          |
|    gazebo     | 91           |  hair dryer   | 77           |   elephant    | 10276        |
|  toilet seat  | 623          |     zebra     | 9570         |  skateboard   | 7450         |
|    zebras     | 1434         |  floor lamp   | 121          | french fries  | 257          |
|     woman     | 37669        |    player     | 6783         |     tower     | 4278         |
|    bicycle    | 2913         |   magazines   | 209          | christmas t.. | 130          |
|   umbrella    | 10137        |      cow      | 9153         |     pants     | 16011        |
|     bike      | 6631         |     field     | 10320        |  living room  | 395          |
|     latch     | 248          |    bedroom    | 419          |     grape     | 459          |
|    castle     | 174          |     table     | 26742        |     swan      | 179          |
|    blender    | 393          |    orange     | 3366         |  teddy bear   | 2019         |
|      net      | 1440         |     meter     | 806          | baseball fi.. | 486          |
|    runway     | 1396         |    screen     | 2939         |   ski boot    | 175          |
|      dog      | 10152        |     clock     | 11435        |     hair      | 22449        |
|    avocado    | 163          |    highway    | 309          |     skirt     | 1787         |
|    frisbee    | 4527         |   parasail    | 246          |     desk      | 4487         |
|     pizza     | 6685         |     mouse     | 2168         |     sign      | 37698        |
| shower curt.. | 261          |  polar bear   | 401          |   airplane    | 4691         |
|    jersey     | 1843         |    reigns     | 81           |    hot dog    | 2633         |
|   surfboard   | 7949         |     couch     | 3151         |     glass     | 9083         |
|   snowboard   | 2222         |     girl      | 10329        |     plane     | 10419        |
|   elephants   | 1540         |     oven      | 1741         |   dirt bike   | 89           |
|   tail wing   | 112          |   area rug    | 77           |     bear      | 7196         |
|    washer     | 65           |     date      | 164          |    bow tie    | 230          |
|     cows      | 1506         | fire exting.. | 95           |    bamboo     | 83           |
|    wallet     | 173          | tail feathers | 137          |     truck     | 7584         |
|  beach chair  | 117          |     boat      | 11599        |    tablet     | 155          |
|    ceiling    | 4078         |  chandelier   | 275          |     sheep     | 9074         |
|    glasses    | 7190         |      ram      | 218          |     kite      | 8354         |
|     salad     | 794          |    pillow     | 6614         | fire hydrant  | 3531         |
|      mug      | 1250         |    tarmac     | 789          |   computer    | 2307         |
|   swimsuit    | 156          |    tomato     | 2032         |     tire      | 7887         |
|  cauliflower  | 227          |   fireplace   | 730          |     snow      | 13875        |
|   building    | 43035        |   sandwich    | 2685         | weather vane  | 122          |
|     bird      | 10558        |    jacket     | 13558        |     chair     | 14795        |
|     water     | 24142        |     cats      | 261          |  soccer ball  | 423          |
|    horses     | 1365         |    drapes     | 247          |     barn      | 332          |
|    engine     | 3225         |     cake      | 4445         |     head      | 25139        |
|   head band   | 114          |     skier     | 3713         |     town      | 174          |
|   bath tub    | 118          |     bowl      | 7025         |     stove     | 2175         |
|    tongue     | 934          | coffee table  | 533          |     floor     | 14462        |
|    uniform    | 2665         |    ottoman    | 244          |   broccoli    | 3299         |
|     olive     | 673          |     mound     | 544          |    pitcher    | 1088         |
|     food      | 8391         |   paintings   | 75           | traffic light | 2648         |
| parking meter | 603          |    bananas    | 3015         |   mountain    | 4143         |
|     cage      | 469          |     hedge     | 372          |  motorcycles  | 537          |
|   wet suit    | 715          |    radish     | 96           |  teddy bears  | 127          |
|   monitors    | 98           |   suitcase    | 3044         |    drawers    | 638          |
|     grass     | 30808        |     apple     | 3113         |     lamp      | 5525         |
|    goggles    | 1940         |      boy      | 11753        |   armchair    | 120          |
|     ramp      | 1712         |    burner     | 692          |     lamb      | 562          |
|      cup      | 5954         |   tank top    | 804          |     boats     | 1177         |
|      hat      | 14502        |     soup      | 473          |     fence     | 15594        |
|   necklace    | 1391         |     visor     | 706          |    coffee     | 532          |
|    bottle     | 8355         |     stool     | 862          |     shoe      | 11842        |
|    surfer     | 3011         |     stop      | 378          |   backpack    | 3129         |
|  shin guard   | 125          |  wii remote   | 262          |     wall      | 41274        |
|  pizza slice  | 300          |  home plate   | 659          |      van      | 2870         |
|    packet     | 157          |   earrings    | 203          |   wristband   | 1054         |
|    tracks     | 6502         |     mitt      | 751          |     dome      | 381          |
|  snowboarder  | 1163         |    faucet     | 2533         |  toiletries   | 120          |
|   ski boots   | 136          |     room      | 2600         |     fork      | 3410         |
|   snow suit   | 90           | banana slice  | 76           |     bench     | 8991         |
|      tie      | 4324         |    burners    | 162          | stuffed ani.. | 100          |
|      zoo      | 468          | train platf.. | 126          |    cupcake    | 698          |
|    curtain    | 2790         |      ear      | 16484        |  tissue box   | 119          |
|     bread     | 2859         |   scissors    | 1143         |     vase      | 5205         |
|     herd      | 259          |     smoke     | 1029         |   skylight    | 99           |
|      cub      | 69           |     tail      | 11544        | cutting board | 513          |
|     wave      | 5584         |    hedges     | 168          |  windshield   | 3549         |
|    apples     | 901          |    mirror     | 6707         | license plate | 2130         |
|     tree      | 45420        |     wheel     | 13372        |   ski pole    | 1724         |
|  clock tower  | 543          |    freezer    | 249          |    luggage    | 2118         |
| skateboarder  | 1498         |   mousepad    | 198          |     road      | 14239        |
|      bat      | 3408         |  toilet tank  | 160          |    vanity     | 183          |
|     neck      | 5226         |     cliff     | 290          |      tub      | 859          |
|   sprinkles   | 458          |    dresser    | 617          |    street     | 13905        |
|     wing      | 4986         |     suit      | 2455         |    veggie     | 158          |
|  palm trees   | 288          |    urinals    | 111          |     door      | 15373        |
|   propeller   | 1044         |     keys      | 733          |  skate park   | 219          |
|   platform    | 2330         |      pot      | 3528         |     towel     | 3813         |
| computer mo.. | 419          |   flip flop   | 233          |     eggs      | 338          |
|     shed      | 257          |     moped     | 310          |     sand      | 5189         |
|     face      | 9510         |    scissor    | 116          |     carts     | 87           |
|    squash     | 191          |    pillows    | 1299         |    family     | 233          |
|     glove     | 4663         |      rug      | 1968         |     watch     | 2782         |
|   grafitti    | 121          |     dogs      | 417          |  scoreboard   | 155          |
|    basket     | 3286         |    poster     | 1147         |     duck      | 1079         |
|     horns     | 1552         |     bears     | 581          |     jeep      | 285          |
|   painting    | 1267         |  lighthouse   | 269          | remote cont.. | 476          |
|    toaster    | 294          |  vegetables   | 1450         |  surfboards   | 241          |
|     ducks     | 200          |     lane      | 156          |    carrots    | 992          |
|    market     | 216          | paper towels  | 200          |    island     | 361          |
|  blueberries  | 93           |     smile     | 529          |   balloons    | 175          |
|   stroller    | 470          |    napkin     | 2592         |    towels     | 786          |
|    papers     | 833          |    person     | 63085        | train tracks  | 1105         |
|     child     | 4714         |   headband    | 834          |     pool      | 366          |
|     plant     | 5996         |    harbor     | 182          |    counter    | 5680         |
|     hand      | 22531        |     house     | 6309         |     donut     | 4285         |
|     knot      | 415          | soccer player | 218          |    seagull    | 391          |
|    bottles    | 987          |     buses     | 370          |     coat      | 5135         |
|     trees     | 19403        |     geese     | 77           |      bun      | 1493         |
|  toilet bowl  | 411          |     trunk     | 6061         |    station    | 774          |
|    bikini     | 284          |    goatee     | 169          | lounge chair  | 67           |
|   breakfast   | 109          |     nose      | 12294        |     moon      | 166          |
|     river     | 1400         |     racer     | 94           |    picture    | 7067         |
|    shaker     | 340          |   sidewalk    | 11509        |   shutters    | 327          |
|   stove top   | 323          |    church     | 521          |   lampshade   | 325          |
|      map      | 195          |     shop      | 347          |    platter    | 411          |
|    airport    | 799          |    hoodie     | 880          |    oranges    | 937          |
|     woods     | 843          |   enclosure   | 637          |   skatepark   | 121          |
|     vases     | 275          |     city      | 1168         |     park      | 1608         |
|    mailbox    | 220          |    balloon    | 517          |   billboard   | 521          |
|    pasture    | 537          |   portrait    | 118          |   forehead    | 416          |
|     ship      | 720          |    cookie     | 357          |    seaweed    | 210          |
|     sofa      | 1363         |     slats     | 296          | tomato slice  | 119          |
|    tractor    | 161          |     bull      | 462          |   suitcases   | 251          |
|   graffiti    | 1475         |   policeman   | 201          |    remotes    | 94           |
|     pens      | 174          |  window sill  | 275          |  suspenders   | 99           |
|     easel     | 79           |     tray      | 2728         |     straw     | 986          |
|    collar     | 3161         |    shower     | 645          |      bag      | 9404         |
|    scooter    | 777          |     tails     | 136          |  toilet lid   | 205          |
|     panda     | 168          |   comforter   | 588          |    outlet     | 1234         |
|     stems     | 386          |    valley     | 164          |     flag      | 5486         |
|    jockey     | 168          |    gravel     | 1679         |     mouth     | 6216         |
|    window     | 63002        |    bridge     | 2638         |     corn      | 347          |
|   mountains   | 2237         |     beer      | 607          | pitcher's m.. | 68           |
|   palm tree   | 1085         |     crowd     | 952          |     skis      | 2848         |
|     phone     | 4628         | banana bunch  | 124          |  tennis shoe  | 631          |
|    ground     | 35197        |    carpet     | 2084         |      eye      | 14954        |
|      urn      | 108          |     beak      | 2037         | giraffe head  | 71           |
|    steeple    | 513          |   mattress    | 380          | baseball pl.. | 1085         |
|     wine      | 1468         | water bottle  | 536          |    kitten     | 301          |
|    archway    | 436          |    candle     | 2043         |   croissant   | 84           |
|  tennis ball  | 1153         |     dress     | 2859         |    column     | 1281         |
|   utensils    | 462          |  cell phone   | 1686         | computer mo.. | 201          |
|      cap      | 6609         |     lawn      | 692          |   airplanes   | 229          |
|   carriage    | 588          |     snout     | 642          |   cabinets    | 1203         |
|    lemons     | 182          |     grill     | 853          |   umbrellas   | 888          |
|     meat      | 2245         |     wagon     | 378          |     ipod      | 172          |
|   bookshelf   | 490          |     cart      | 1419         |     roof      | 7378         |
|      hay      | 933          |   ski pants   | 175          |     seat      | 4979         |
|     mane      | 2689         |     bikes     | 571          |    drawer     | 2472         |
|     game      | 773          |  clock face   | 467          |     boys      | 452          |
|     rider     | 891          |  fire escape  | 84           |     slope     | 1018         |
|    iphone     | 73           |    pumpkin    | 241          |      pan      | 1372         |
|  chopsticks   | 165          |     hill      | 4161         |   uniforms    | 90           |
|     cleat     | 412          |    costume    | 256          |     cabin     | 300          |
| police offi.. | 194          |     ears      | 2725         |      egg      | 669          |
|   trash can   | 1352         |     horn      | 2106         |     arrow     | 2419         |
|  toothbrush   | 1174         |    carrot     | 2460         |    banana     | 5993         |
|    planes     | 675          |    garden     | 310          |    forest     | 1118         |
|   brocolli    | 217          |   aircraft    | 437          | front window  | 194          |
|   dashboard   | 90           |    statue     | 2053         |    saucer     | 594          |
|    people     | 20049        |  silverware   | 473          |     fruit     | 3285         |
|     drain     | 753          |      jet      | 1546         |    speaker    | 1248         |
|     eyes      | 4180         |    railway    | 148          |      lid      | 2824         |
|     soap      | 613          |     rocks     | 4462         | office chair  | 103          |
|   door knob   | 218          |  banana peel  | 76           | baseball game | 178          |
|   asparagus   | 113          |     spoon     | 2398         | cabinet door  | 366          |
|   pineapple   | 287          | traffic cone  | 180          |  nightstand   | 695          |
|    teapot     | 223          |     taxi      | 431          |    chimney    | 1121         |
|     lake      | 838          |  suit jacket  | 106          | train engine  | 254          |
|     ball      | 5618         |  wrist band   | 422          |    pickle     | 352          |
|    fruits     | 473          |      pad      | 645          |   dispenser   | 537          |
|    bridle     | 305          |    breast     | 132          |     cones     | 447          |
|   headlight   | 4093         |    necktie    | 252          |    skater     | 698          |
| toilet paper  | 991          |  skyscraper   | 304          |   telephone   | 543          |
|      ox       | 112          |    roadway    | 339          |     sock      | 3164         |
|    paddle     | 407          |    dishes     | 344          |     hills     | 747          |
|  street sign  | 1993         |  headlights   | 1396         |    benches    | 492          |
|   fuselage    | 106          |     card      | 423          |    napkins    | 366          |
|     bush      | 5618         |     rice      | 650          | computer sc.. | 186          |
|    spokes     | 396          |    flowers    | 7487         |    bucket     | 1291         |
|     rock      | 8359         |     pole      | 26532        |     pear      | 215          |
|     sauce     | 2212         |     store     | 1042         |     juice     | 519          |
|     knobs     | 702          |    mustard    | 440          |      ski      | 2473         |
|    stands     | 666          |    cabinet    | 4124         |     dirt      | 6221         |
|     goats     | 157          |  wine glass   | 862          |  spectators   | 696          |
|     crate     | 630          |   pancakes    | 83           |     kids      | 520          |
|    engines    | 245          |     shade     | 1721         |    feeder     | 218          |
|   cellphone   | 1411         |    pepper     | 1361         |    blanket    | 2876         |
|  sunglasses   | 4580         |   train car   | 1381         |    magnet     | 511          |
|    donuts     | 1131         |    sweater    | 2751         |    signal     | 868          |
| advertisement | 1058         |      log      | 1226         |     vent      | 1044         |
|   whiskers    | 1253         |     adult     | 290          |     arch      | 626          |
|  locomotive   | 194          | tennis match  | 102          |     tent      | 1251         |
|   motorbike   | 763          |    magnets    | 208          |     night     | 108          |
|    marina     | 80           |     wool      | 395          |     vest      | 2238         |
| railroad tr.. | 162          | stuffed bear  | 138          |   moustache   | 126          |
|      bib      | 182          |     frame     | 3945         |  snow pants   | 137          |
|     tank      | 1066         |  undershirt   | 163          |     icons     | 81           |
|   neck tie    | 113          |     beams     | 209          | baseball bat  | 519          |
|  safety cone  | 142          |  paper towel  | 163          |   bedspread   | 366          |
|      can      | 1982         |   container   | 3876         |    flower     | 10993        |
|    vehicle    | 4612         |   tomatoes    | 698          |  back wheel   | 200          |
| soccer field  | 81           |    nostril    | 573          |      suv      | 1092         |
|   buildings   | 2957         |    canopy     | 765          |     flame     | 173          |
|      kid      | 2386         |   baseball    | 1375         | throw pillow  | 117          |
|     belt      | 2040         |    rainbow    | 104          |     lemon     | 574          |
|   oven door   | 149          |      tag      | 2877         |     books     | 1955         |
|   monument    | 129          |      men      | 2673         |    shadow     | 13071        |
|   bicycles    | 326          |     cars      | 3079         |  lamp shade   | 357          |
|   pine tree   | 510          |    bouquet    | 236          |  toothpaste   | 191          |
|    potato     | 668          |     sinks     | 145          |     hook      | 499          |
|    switch     | 552          |   lamp post   | 839          |     lapel     | 112          |
|    desert     | 207          |     knob      | 3005         |    chairs     | 1529         |
|     pasta     | 311          |   feathers    | 1283         |     hole      | 2444         |
|     meal      | 422          | station wagon | 70           |     kites     | 832          |
|     boots     | 1647         |     baby      | 1408         |     biker     | 288          |
|     gate      | 1516         | signal light  | 138          |  headphones   | 388          |
|     goat      | 814          |     waves     | 3409         |    bumper     | 889          |
|      bud      | 140          |     logo      | 7278         |   curtains    | 1317         |
| american flag | 270          |     yacht     | 114          |      box      | 6430         |
| baseball cap  | 481          |     fries     | 426          |  controller   | 1140         |
|    awning     | 1541         |     path      | 1322         |  front legs   | 307          |
|  life jacket  | 121          |     purse     | 1756         |   outfield    | 122          |
|    pigeon     | 433          |    toddler    | 372          |     beard     | 1284         |
|     thumb     | 985          |  water tank   | 71           |     board     | 5841         |
|    parade     | 71           |     robe      | 199          |   newspaper   | 405          |
|     wires     | 1916         |    camera     | 2436         |   pastries    | 204          |
|     deck      | 507          |  watermelon   | 227          |    clouds     | 19884        |
|     deer      | 207          | motorcyclist  | 154          |    kneepad    | 121          |
|   sneakers    | 1539         |     women     | 1050         |    onions     | 684          |
|    eyebrow    | 711          |  gas station  | 89           |     vane      | 74           |
|     girls     | 351          |     trash     | 784          |   numerals    | 307          |
|     knife     | 3246         |     tags      | 156          |     light     | 25075        |
|     bunch     | 526          |    outfit     | 1074         |     groom     | 147          |
|    infield    | 73           |   frosting    | 1142         |     forks     | 134          |
| entertainme.. | 129          | stuffed ani.. | 549          |     yard      | 557          |
|    numeral    | 718          |    ladder     | 926          |     shoes     | 7007         |
|   bracelet    | 1429         |     teeth     | 1165         |      guy      | 3197         |
| display case  | 85           |    cushion    | 1164         |     post      | 7915         |
|    pathway    | 275          |  tablecloth   | 1129         |    skiers     | 417          |
|    trouser    | 301          |     cloud     | 11772        |     hands     | 3533         |
|    produce    | 180          |     beam      | 703          |    ketchup    | 570          |
|      paw      | 2984         |     dish      | 1990         |     raft      | 188          |
|   crosswalk   | 724          |  front wheel  | 531          |     toast     | 199          |
|    cattle     | 341          |    players    | 789          |     group     | 1108         |
|  coffee pot   | 121          |     track     | 3407         |  cowboy hat   | 116          |
|     petal     | 778          |  eyeglasses   | 844          |    handle     | 11442        |
|  table cloth  | 528          |     jets      | 147          |    shakers    | 99           |
|    remote     | 1698         |   snowsuit    | 157          |    bushes     | 2844         |
|    dessert    | 502          |      leg      | 20294        |     eagle     | 94           |
|  fire truck   | 283          | game contro.. | 195          |  smartphone   | 79           |
|  backsplash   | 129          |    trains     | 366          |     shore     | 1452         |
|     signs     | 1946         |     bell      | 556          |   cupboards   | 125          |
|  sweat band   | 74           |     sack      | 129          |     ankle     | 200          |
|   coin slot   | 72           |     bagel     | 168          |     masts     | 70           |
|    police     | 154          |    drawing    | 414          |    biscuit    | 90           |
|      toy      | 1028         |     legs      | 6274         |   pavement    | 2488         |
|    outside    | 532          |    wheels     | 4049         |    driver     | 508          |
|    numbers    | 2436         |    blazer     | 164          |      pen      | 1419         |
|    cabbage    | 228          |    trucks     | 175          |      key      | 2449         |
|    saddle     | 550          |  pillow case  | 91           |     goose     | 218          |
|     label     | 1681         |    boulder    | 535          |    pajamas    | 109          |
|     wrist     | 2317         |     shelf     | 6452         |     cross     | 729          |
|  coffee cup   | 305          |    foliage    | 629          |      lot      | 687          |
|      fry      | 319          |      air      | 2398         |    officer    | 311          |
|   pepperoni   | 1160         |    cheese     | 2950         |     lady      | 3848         |
|   kickstand   | 354          |  counter top  | 643          |    veggies    | 286          |
| baseball un.. | 118          |  book shelf   | 106          |     bags      | 659          |
|    pickles    | 137          |     stand     | 2734         |    netting    | 233          |
|    lettuce    | 1201         |  facial hair  | 208          |     lime      | 194          |
|    animals    | 1073         |     drape     | 114          |     boot      | 1893         |
|    railing    | 3231         |   end table   | 192          |  shin guards  | 73           |
|     steps     | 1215         |   trashcan    | 470          |     tusk      | 1507         |
|  head light   | 209          |    walkway    | 1461         |    cockpit    | 943          |
|  tennis net   | 194          |    animal     | 4099         |   boardwalk   | 123          |
|    keypad     | 122          |   bookcase    | 307          |   blueberry   | 186          |
|   trash bag   | 90           |   ski poles   | 520          |  parking lot  | 1079         |
|   gas tank    | 173          |     beds      | 112          |      fan      | 883          |
|     base      | 2158         | soap dispen.. | 183          |    banner     | 1895         |
|   life vest   | 100          |  train front  | 83           |     word      | 2605         |
|      cab      | 508          |    liquid     | 998          | exhaust pipe  | 137          |
|    sneaker    | 1770         | light fixture | 525          |  power lines  | 579          |
|     curb      | 2247         |     scene     | 1148         |    buttons    | 1345         |
| roman numer.. | 394          |    muzzle     | 250          |    sticker    | 2900         |
|     bacon     | 493          |    pizzas     | 136          |     paper     | 5045         |
|     feet      | 3382         |    stairs     | 1596         |   triangle    | 335          |
|    plants     | 2276         |     rope      | 2089         |     beans     | 402          |
|     brim      | 107          |   beverage    | 262          |    letters    | 5040         |
|     soda      | 323          |     menu      | 459          |    finger     | 3417         |
|     dvds      | 84           |    candles    | 505          | picnic table  | 219          |
|  wine bottle  | 372          |    pencil     | 228          |  tree trunk   | 1528         |
|     nail      | 733          |    mantle     | 250          |  countertop   | 783          |
|     view      | 297          |     line      | 13069        |  motor bike   | 93           |
|   audience    | 201          | traffic sign  | 386          |      arm      | 9770         |
|  pedestrian   | 360          |  stabilizer   | 163          |     dock      | 889          |
|    doorway    | 1224         |    bedding    | 114          |      end      | 503          |
|    worker     | 254          |     canal     | 117          |     crane     | 451          |
|     grate     | 481          |  little girl  | 219          |     rims      | 97           |
| passenger car | 101          |    plates     | 1038         |  background   | 6829         |
|     peel      | 221          |  brake light  | 246          | roman numeral | 1067         |
|    string     | 1408         |     tines     | 133          |     turf      | 162          |
|    armrest    | 286          |  shower head  | 263          |     leash     | 732          |
|    stones     | 771          |   stoplight   | 272          |  handle bars  | 195          |
|     front     | 2060         |     scarf     | 1521         |     band      | 1528         |
|     jean      | 172          |    tennis     | 613          |     pile      | 788          |
|   doorknob    | 201          |     foot      | 6834         |    houses     | 663          |
|    windows    | 9447         |  restaurant   | 502          |     booth     | 223          |
| cardboard box | 199          |    fingers    | 951          | mountain ra.. | 223          |
|   bleachers   | 359          |     rail      | 2121         |    pastry     | 696          |
|     canoe     | 273          |      sun      | 1373         |  eye glasses  | 150          |
|  salt shaker  | 215          |    number     | 7701         |     fish      | 611          |
|   knee pad    | 250          |      fur      | 2486         |      she      | 385          |
|  shower door  | 99           |      rod      | 669          |   branches    | 2657         |
|     birds     | 1342         |    printer    | 199          |    sunset     | 225          |
|    median     | 203          |    shutter    | 470          |     slice     | 1243         |
|    heater     | 134          |    prongs     | 89           | bathing suit  | 121          |
|    skiier     | 195          |     rack      | 1906         |     book      | 4650         |
|     blade     | 659          |   apartment   | 97           | manhole cover | 146          |
|    stools     | 93           |   overhang    | 130          |  door handle  | 277          |
|    couple     | 895          | picture frame | 285          |    chicken    | 863          |
|    planter    | 766          |     seats     | 635          |   hour hand   | 185          |
|  dvd player   | 95           |   ski slope   | 229          |  french fry   | 422          |
|     bowls     | 317          |      top      | 6242         | landing gear  | 683          |
| coffee maker  | 210          |     melon     | 109          |   computers   | 120          |
| light switch  | 416          |      jar      | 1312         |   tv stand    | 145          |
|   overalls    | 124          |    garage     | 337          |   tabletop    | 166          |
|    writing    | 5185         |     doors     | 1631         |    stadium    | 254          |
|   placemat    | 375          |   air vent    | 85           |     trick     | 158          |
|     sled      | 74           |     mast      | 436          |     pond      | 1261         |
| steering wh.. | 248          | baseball gl.. | 277          |   watermark   | 229          |
|      pie      | 323          |   sandwhich   | 118          |      cpu      | 73           |
|   mushroom    | 721          |  power pole   | 78           |   dirt road   | 87           |
|    handles    | 1049         |   speakers    | 161          |    fender     | 361          |
| telephone p.. | 274          |  strawberry   | 643          |     mask      | 793          |
|   children    | 573          |     crust     | 1362         |      art      | 430          |
|      rim      | 980          |    branch     | 4533         |    display    | 918          |
|    grasses    | 154          |     photo     | 2751         |    receipt    | 80           |
| instructions  | 69           |     herbs     | 139          |     toys      | 239          |
|  handlebars   | 375          |    trailer    | 575          |    sandal     | 644          |
|     skull     | 173          |    hangar     | 73           |     pipe      | 1970         |
|    office     | 185          |     chest     | 894          |     lamps     | 225          |
|    horizon    | 554          |   calendar    | 177          |     foam      | 880          |
|     stone     | 2712         |     bars      | 732          |    button     | 5598         |
|     poles     | 2211         |     heart     | 536          |     hose      | 543          |
|  jet engine   | 133          |   potatoes    | 497          |     rain      | 243          |
|   magazine    | 442          |     chain     | 1832         |   footboard   | 94           |
|   tee shirt   | 397          |    design     | 3450         |     walls     | 772          |
|   copyright   | 101          |   pictures    | 638          |    pillar     | 1670         |
|     drink     | 750          |    barrier    | 681          |     boxes     | 682          |
|   chocolate   | 390          |     chef      | 156          |     slot      | 138          |
|  sweatpants   | 67           |   face mask   | 132          |     icing     | 677          |
|    wipers     | 359          |    circle     | 2010         |      bin      | 784          |
|     kitty     | 79           |  electronics  | 97           |     wild      | 91           |
|     tiles     | 1791         |     steam     | 258          |   lettering   | 2822         |
| bathroom sink | 95           | laptop comp.. | 160          |    cherry     | 185          |
|     spire     | 332          |   conductor   | 116          |     sheet     | 1046         |
|     slab      | 230          | windshield .. | 149          |  storefront   | 189          |
|   hill side   | 115          |    spatula    | 307          |  tail light   | 943          |
|     bean      | 368          |     wire      | 2713         | intersection  | 415          |
|     pier      | 505          |  snow board   | 147          |    trunks     | 455          |
|    website    | 133          |     bolt      | 1320         |     kayak     | 138          |
|     nuts      | 191          |    holder     | 1272         |    turbine    | 90           |
|  stop light   | 333          |    olives     | 351          |   ball cap    | 94           |
|    burger     | 199          |    barrel     | 470          |     fans      | 246          |
|    beanie     | 310          |     stem      | 2627         |     lines     | 5387         |
| traffic sig.. | 604          |  sweatshirt   | 779          |    handbag    | 322          |
|     mulch     | 176          |     socks     | 2156         |   landscape   | 266          |
|   soda can    | 106          |    shelves    | 642          |   ski lift    | 342          |
|     cord      | 2235         |   vegetable   | 1858         |     apron     | 657          |
|     blind     | 260          |   bracelets   | 184          |   stickers    | 374          |
|    traffic    | 400          |     strip     | 614          | tennis shoes  | 346          |
|  swim trunks  | 105          |   hillside    | 825          |    sandals    | 690          |
|   concrete    | 1326         |     lips      | 603          | butter knife  | 96           |
|     words     | 1844         |    leaves     | 16841        |  train cars   | 148          |
|     spoke     | 396          |    cereal     | 154          |  pine trees   | 182          |
|    cooler     | 450          |     bangs     | 176          |     half      | 151          |
|    sheets     | 447          |   figurine    | 312          |  park bench   | 143          |
|     stack     | 519          | second floor  | 77           |     motor     | 271          |
|  hand towel   | 85           |  wristwatch   | 209          |   spectator   | 1151         |
|    tissues    | 144          |  flip flops   | 226          |     quilt     | 175          |
|    floret     | 110          |     calf      | 293          |   back pack   | 167          |
|    grapes     | 400          |  ski tracks   | 110          |     skin      | 814          |
|      bow      | 723          |   controls    | 133          |    dinner     | 104          |
| baseball pl.. | 86           |      ad       | 317          |    ribbon     | 1002         |
|     hotel     | 166          |      sea      | 542          |     cover     | 1505         |
|     tarp      | 659          |    weather    | 81           |   notebook    | 264          |
|   mustache    | 496          |  stone wall   | 197          |    closet     | 141          |
|    statues    | 123          |     bank      | 294          |  skateboards  | 88           |
|    butter     | 262          |  dress shirt  | 96           |     knee      | 1553         |
|     wood      | 2673         |    laptops    | 133          |     cuff      | 222          |
|    hubcap     | 211          |     wings     | 903          |     range     | 189          |
|   structure   | 1211         |     balls     | 267          |    tunnel     | 304          |
|     globe     | 304          |    utensil    | 808          |   dumpster    | 136          |
|      cd       | 203          |    floors     | 167          |    wrapper    | 328          |
|    folder     | 144          |    pocket     | 1312         |    mother     | 186          |
|  ski goggles  | 91           |     posts     | 614          |  power line   | 421          |
|     wake      | 209          |     roses     | 305          |  train track  | 447          |
|  reflection   | 4762         | air conditi.. | 219          |    referee    | 123          |
|   barricade   | 157          | baseball mitt | 68           |   mouse pad   | 267          |
|  garbage can  | 442          |    buckle     | 330          |  footprints   | 483          |
|    lights     | 6248         |    muffin     | 251          |    bracket    | 323          |
|     plug      | 427          |   taxi cab    | 82           |    drinks     | 139          |
|    surfers    | 204          |    arrows     | 235          | control panel | 87           |
|     ring      | 1975         |     twigs     | 230          |     soil      | 261          |
|     skies     | 692          |  clock hand   | 106          |    caboose    | 82           |
|  playground   | 138          |     mango     | 84           |     stump     | 278          |
|  brick wall   | 403          |     screw     | 815          |    minivan    | 196          |
|     leaf      | 10819        |    fencing    | 629          |     ledge     | 716          |
|    clothes    | 1354         |  grass field  | 155          |   plumbing    | 101          |
|    blouse     | 409          |     patch     | 2129         |  scaffolding  | 100          |
|   hamburger   | 118          | utility pole  | 189          |     teddy     | 149          |
|     rose      | 767          |    skillet    | 117          |     cycle     | 180          |
|     cable     | 748          |    gloves     | 1957         |     bark      | 469          |
|  decoration   | 815          |    tables     | 279          |     palm      | 232          |
|      wii      | 335          | mountain top  | 99           |     shrub     | 856          |
|     hoof      | 1829         |    celery     | 170          |     beads     | 205          |
|    plaque     | 297          |   flooring    | 432          |     surf      | 111          |
|     cloth     | 1976         |   passenger   | 437          |     spot      | 7228         |
|    plastic    | 565          |    knives     | 217          |     case      | 1045         |
|   railroad    | 240          |     pony      | 207          |    muffler    | 129          |
|   hot dogs    | 272          |    stripe     | 8121         |     scale     | 174          |
|     block     | 847          |   recliner    | 82           |     body      | 1582         |
|    shades     | 232          |      tap      | 186          |     tools     | 131          |
|   cupboard    | 438          |   wallpaper   | 267          |   sculpture   | 351          |
|    surface    | 1654         |     sedan     | 320          |   distance    | 1761         |
|    shrubs     | 551          |     skiis     | 243          |     lift      | 233          |
|    bottom     | 1406         |    cleats     | 482          |     roll      | 647          |
|   clothing    | 994          |   bed frame   | 100          |    slacks     | 95           |
|  tail lights  | 119          |     doll      | 453          | traffic lig.. | 365          |
|    symbol     | 1035         |    strings    | 428          |   fixtures    | 96           |
|     short     | 459          |     paint     | 2053         | candle holder | 157          |
|  guard rail   | 161          |    cyclist    | 76           | tree branches | 218          |
|    ripples    | 1602         |     gear      | 488          |     waist     | 309          |
|   trash bin   | 182          |     onion     | 1301         |     home      | 390          |
|  side mirror  | 296          |     brush     | 897          |   sweatband   | 204          |
|   handlebar   | 350          |  light pole   | 611          |  street lamp  | 582          |
|     pads      | 225          |      ham      | 321          |    artwork    | 313          |
|   reflector   | 419          |    figure     | 350          |     tile      | 8392         |
| mountainside  | 98           |     black     | 1743         |    bricks     | 1716         |
|  paper plate  | 165          |     stick     | 1862         |     beef      | 146          |
|     patio     | 280          |     weeds     | 1261         |     back      | 3524         |
|    sausage    | 554          |     paws      | 637          |     farm      | 119          |
|     decal     | 262          |    harness    | 493          |    monkey     | 166          |
|  fence post   | 408          |  door frame   | 146          |    stripes    | 4245         |
|    clocks     | 304          |   ponytail    | 697          |   toppings    | 563          |
|     strap     | 2264         |    carton     | 260          |    greens     | 200          |
|     chin      | 635          |     lunch     | 114          |     name      | 1488         |
|    earring    | 894          |     area      | 1759         |    tshirt     | 2944         |
|     cream     | 319          |     rails     | 574          |   cushions    | 198          |
|    lanyard    | 216          |     brick     | 5625         |    hallway    | 164          |
|   cucumber    | 409          |  wire fence   | 91           |     fern      | 106          |
|   tangerine   | 105          |  windowsill   | 179          |     pipes     | 397          |
|    package    | 322          |  wheelchair   | 96           |     chips     | 289          |
|   driveway    | 323          |    tattoo     | 572          |  side window  | 152          |
|   stairway    | 306          |     basin     | 203          |    machine    | 582          |
|  table lamp   | 101          |     radio     | 151          |   pony tail   | 250          |
|  ocean water  | 161          |    inside     | 261          |     cargo     | 108          |
|   overpass    | 199          |      mat      | 830          |    socket     | 247          |
|  flower pot   | 203          |   tree line   | 90           |   sign post   | 160          |
|     tube      | 382          |     dial      | 257          |    splash     | 335          |
|     male      | 538          |    lantern    | 177          |   lipstick    | 125          |
|      lip      | 271          |     tongs     | 185          |   ski suit    | 106          |
|     trail     | 484          | passenger t.. | 99           |    bandana    | 311          |
|   antelope    | 146          |    designs    | 280          |     tents     | 119          |
|  photograph   | 290          | catcher's m.. | 75           | electrical .. | 143          |
|     tires     | 674          |   boulders    | 131          |   mannequin   | 182          |
|     plain     | 136          |     layer     | 114          |   mushrooms   | 271          |
| strawberries  | 323          |     piece     | 1119         |      oar      | 228          |
|   bike rack   | 122          |    slices     | 232          |     arms      | 1132         |
|      fin      | 409          |    shadows    | 1225         |     hood      | 1142         |
| windshield .. | 393          |    letter     | 11886        |      dot      | 1278         |
|   bus stop    | 298          |   railings    | 231          |    pebbles    | 305          |
|      mud      | 458          |     claws     | 353          |  police car   | 79           |
|     crown     | 166          |    meters     | 77           |   name tag    | 134          |
|   entrance    | 473          |   staircase   | 398          |    shrimp     | 237          |
|    ladies     | 95           |     peak      | 172          |     vines     | 167          |
| computer ke.. | 97           |  glass door   | 106          |     pears     | 102          |
|     pant      | 344          | wine glasses  | 93           |     stall     | 180          |
|    asphalt    | 497          |    columns    | 335          |    sleeve     | 2067         |
|     pack      | 235          |     cheek     | 282          |    baskets    | 162          |
|     land      | 886          |      day      | 401          |    blocks     | 219          |
|   courtyard   | 89           |     pedal     | 331          |     panel     | 1236         |
|     seeds     | 228          |    balcony    | 1613         |    yellow     | 683          |
|     disc      | 176          |   young man   | 183          |   eyebrows    | 228          |
|    crumbs     | 363          |    spinach    | 403          |    emblem     | 612          |
|    object     | 2332         |      bar      | 2166         |   cardboard   | 231          |
|    tissue     | 398          |  light post   | 416          |  ski jacket   | 115          |
|   seasoning   | 170          |    parasol    | 141          |   terminal    | 147          |
|    surfing    | 111          |  streetlight  | 2453         |     alley     | 78           |
|     cords     | 445          |     image     | 635          |      jug      | 304          |
|    antenna    | 431          |     puppy     | 232          |    berries    | 189          |
|    diamond    | 366          |     pans      | 133          |   fountain    | 244          |
|  foreground   | 346          |     syrup     | 117          |     bride     | 166          |
|     spray     | 236          |    license    | 105          |    peppers    | 525          |
|  passengers   | 300          |    cement     | 751          |     flags     | 677          |
|     shack     | 68           |    trough     | 120          |    objects    | 314          |
|    arches     | 132          |   streamer    | 94           |     pots      | 331          |
|    border     | 837          |   baseboard   | 213          |  beer bottle  | 125          |
|  wrist watch  | 144          |  tile floor   | 112          |     page      | 104          |
|      pin      | 160          |     items     | 713          |   baseline    | 83           |
|    hanger     | 289          |  tree branch  | 396          |     tusks     | 439          |
|    donkey     | 97           |  containers   | 387          |  condiments   | 141          |
|    device     | 370          |   envelope    | 105          |   parachute   | 151          |
|     mesh      | 141          |      hut      | 156          |   butterfly   | 232          |
|     salt      | 153          |   restroom    | 132          |     twig      | 369          |
|     pilot     | 246          |      ivy      | 152          |   furniture   | 369          |
|     clay      | 154          |     print     | 801          |  sandwiches   | 192          |
|     lion      | 149          |   shingles    | 146          |    pillars    | 284          |
|   vehicles    | 809          |     panes     | 173          |   shoreline   | 339          |
|    stream     | 181          |    control    | 246          |     lock      | 439          |
|  microphone   | 417          |    blades     | 188          |  towel rack   | 206          |
|    coaster    | 203          |     star      | 1210         |    petals     | 308          |
|     text      | 1151         |    feather    | 704          |     spots     | 2021         |
|     buoy      | 371          |               |              |               |              |
|     total     | 3007302      |               |              |               |              |[0m
[32m[11/10 06:00:36 d2.data.common]: [0mSerializing 97169 elements to byte tensors and concatenating them all ...
[32m[11/10 06:00:39 d2.data.common]: [0mSerialized dataset takes 191.58 MiB
[32m[11/10 06:00:40 fvcore.common.checkpoint]: [0m[Checkpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[32m[11/10 06:00:40 d2.checkpoint.c2_model_loading]: [0mRenaming Caffe2 weights ......
[32m[11/10 06:00:41 d2.checkpoint.c2_model_loading]: [0mFollowing weights matched with submodule backbone.bottom_up:
| Names in Model    | Names in Checkpoint      | Shapes                                          |
|:------------------|:-------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*           | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                  | (64, 3, 7, 7)                                   |
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
[5m[31mWARNING[0m [32m[11/10 06:01:56 fvcore.common.checkpoint]: [0mSome model parameters or buffers are not found in the checkpoint:
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mbackbone.top_block.p6.{bias, weight}[0m
[34mbackbone.top_block.p7.{bias, weight}[0m
[34mconcept_net.concept_emb.weight[0m
[34mconcept_net.deepset.mlp1.layers.0.{bias, weight}[0m
[34mconcept_net.deepset.mlp1.layers.1.{bias, weight}[0m
[34mconcept_net.deepset.mlp1.layers.2.{bias, weight}[0m
[34mconcept_net.deepset.mlp2.layers.0.{bias, weight}[0m
[34mconcept_net.deepset.mlp2.layers.1.{bias, weight}[0m
[34mconcept_net.deepset.mlp2.layers.2.{bias, weight}[0m
[34mhead.bbox_pred.{bias, weight}[0m
[34mhead.bbox_subnet.0.{bias, weight}[0m
[34mhead.bbox_subnet.2.{bias, weight}[0m
[34mhead.bbox_subnet.4.{bias, weight}[0m
[34mhead.bbox_subnet.6.{bias, weight}[0m
[34mhead.cls_score.{bias, weight}[0m
[34mhead.cls_subnet.0.{bias, weight}[0m
[34mhead.cls_subnet.2.{bias, weight}[0m
[34mhead.cls_subnet.4.{bias, weight}[0m
[34mhead.cls_subnet.6.{bias, weight}[0m
[5m[31mWARNING[0m [32m[11/10 06:01:56 fvcore.common.checkpoint]: [0mThe checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
  [35mstem.conv1.bias[0m
[32m[11/10 06:01:56 d2.engine.train_loop]: [0mStarting training from iteration 0
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
Load concept for each category. 
Loading pre-trained concepts embeddings. 
Vocab initialization with 0/82115 elements not found. 
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[32m[11/10 06:03:02 d2.utils.events]: [0m eta: 22:39:46  iter: 19  total_loss: 3.077  loss_cls: 2.183  loss_box_reg: 0.901  time: 0.9931  data_time: 2.2905  lr: 0.00019981  max_mem: 20718M
[32m[11/10 06:03:21 d2.utils.events]: [0m eta: 22:28:12  iter: 39  total_loss: 2.489  loss_cls: 1.784  loss_box_reg: 0.6931  time: 0.9598  data_time: 0.0121  lr: 0.00039961  max_mem: 21643M
[32m[11/10 06:03:36 d2.utils.events]: [0m eta: 21:51:47  iter: 59  total_loss: 2.364  loss_cls: 1.722  loss_box_reg: 0.6476  time: 0.9001  data_time: 0.0021  lr: 0.00059941  max_mem: 21643M
[32m[11/10 06:03:51 d2.utils.events]: [0m eta: 20:35:47  iter: 79  total_loss: 2.369  loss_cls: 1.716  loss_box_reg: 0.6696  time: 0.8583  data_time: 0.0050  lr: 0.00079921  max_mem: 21643M
[32m[11/10 06:04:08 d2.utils.events]: [0m eta: 19:57:09  iter: 99  total_loss: 2.329  loss_cls: 1.668  loss_box_reg: 0.661  time: 0.8507  data_time: 0.0041  lr: 0.00099901  max_mem: 21643M
[32m[11/10 06:04:24 d2.utils.events]: [0m eta: 19:46:21  iter: 119  total_loss: 2.4  loss_cls: 1.727  loss_box_reg: 0.6731  time: 0.8464  data_time: 0.0021  lr: 0.0011988  max_mem: 21643M
[32m[11/10 06:04:42 d2.utils.events]: [0m eta: 19:56:26  iter: 139  total_loss: 2.316  loss_cls: 1.628  loss_box_reg: 0.6579  time: 0.8517  data_time: 0.0021  lr: 0.0013986  max_mem: 21643M
[32m[11/10 06:05:00 d2.utils.events]: [0m eta: 20:01:05  iter: 159  total_loss: 2.237  loss_cls: 1.646  loss_box_reg: 0.5965  time: 0.8560  data_time: 0.0021  lr: 0.0015984  max_mem: 21643M
[32m[11/10 06:05:18 d2.utils.events]: [0m eta: 20:29:39  iter: 179  total_loss: 2.42  loss_cls: 1.733  loss_box_reg: 0.6669  time: 0.8587  data_time: 0.0021  lr: 0.0017982  max_mem: 21643M
[32m[11/10 06:05:35 d2.utils.events]: [0m eta: 20:39:04  iter: 199  total_loss: 2.335  loss_cls: 1.686  loss_box_reg: 0.6384  time: 0.8602  data_time: 0.0022  lr: 0.001998  max_mem: 21675M
[32m[11/10 06:05:53 d2.utils.events]: [0m eta: 20:53:58  iter: 219  total_loss: 2.283  loss_cls: 1.627  loss_box_reg: 0.6481  time: 0.8645  data_time: 0.0024  lr: 0.0021978  max_mem: 21675M
[32m[11/10 06:06:12 d2.utils.events]: [0m eta: 21:13:16  iter: 239  total_loss: 2.359  loss_cls: 1.691  loss_box_reg: 0.6604  time: 0.8713  data_time: 0.0021  lr: 0.0023976  max_mem: 21675M
[32m[11/10 06:06:31 d2.utils.events]: [0m eta: 21:17:36  iter: 259  total_loss: 2.388  loss_cls: 1.729  loss_box_reg: 0.6695  time: 0.8774  data_time: 0.0022  lr: 0.0025974  max_mem: 21675M
[32m[11/10 06:06:51 d2.utils.events]: [0m eta: 21:28:31  iter: 279  total_loss: 2.297  loss_cls: 1.678  loss_box_reg: 0.6478  time: 0.8846  data_time: 0.0021  lr: 0.0027972  max_mem: 21675M
[32m[11/10 06:07:11 d2.utils.events]: [0m eta: 21:36:56  iter: 299  total_loss: 2.407  loss_cls: 1.73  loss_box_reg: 0.654  time: 0.8931  data_time: 0.0042  lr: 0.002997  max_mem: 21675M
[32m[11/10 06:07:31 d2.utils.events]: [0m eta: 21:41:44  iter: 319  total_loss: 2.251  loss_cls: 1.628  loss_box_reg: 0.6232  time: 0.8989  data_time: 0.0022  lr: 0.0031968  max_mem: 21675M
[32m[11/10 06:07:52 d2.utils.events]: [0m eta: 21:47:42  iter: 339  total_loss: 2.173  loss_cls: 1.59  loss_box_reg: 0.6076  time: 0.9060  data_time: 0.0022  lr: 0.0033966  max_mem: 21675M
[32m[11/10 06:08:13 d2.utils.events]: [0m eta: 22:04:17  iter: 359  total_loss: 2.238  loss_cls: 1.629  loss_box_reg: 0.6069  time: 0.9140  data_time: 0.0038  lr: 0.0035964  max_mem: 21690M
[32m[11/10 06:08:34 d2.utils.events]: [0m eta: 22:14:51  iter: 379  total_loss: 2.34  loss_cls: 1.673  loss_box_reg: 0.6507  time: 0.9223  data_time: 0.0035  lr: 0.0037962  max_mem: 21690M
[32m[11/10 06:08:54 d2.utils.events]: [0m eta: 22:18:10  iter: 399  total_loss: 2.209  loss_cls: 1.59  loss_box_reg: 0.616  time: 0.9260  data_time: 0.0046  lr: 0.003996  max_mem: 21690M
[32m[11/10 06:09:14 d2.utils.events]: [0m eta: 22:27:19  iter: 419  total_loss: 1.941  loss_cls: 1.347  loss_box_reg: 0.5844  time: 0.9299  data_time: 0.0021  lr: 0.0041958  max_mem: 21690M
[32m[11/10 06:09:36 d2.utils.events]: [0m eta: 22:31:15  iter: 439  total_loss: 1.892  loss_cls: 1.275  loss_box_reg: 0.6193  time: 0.9370  data_time: 0.0021  lr: 0.0043956  max_mem: 21690M
[32m[11/10 06:09:58 d2.utils.events]: [0m eta: 22:37:03  iter: 459  total_loss: 1.912  loss_cls: 1.292  loss_box_reg: 0.6239  time: 0.9431  data_time: 0.0034  lr: 0.0045954  max_mem: 21690M
[32m[11/10 06:10:18 d2.utils.events]: [0m eta: 22:45:08  iter: 479  total_loss: 1.741  loss_cls: 1.167  loss_box_reg: 0.5692  time: 0.9460  data_time: 0.0022  lr: 0.0047952  max_mem: 21690M
[32m[11/10 06:10:40 d2.utils.events]: [0m eta: 22:54:33  iter: 499  total_loss: 1.796  loss_cls: 1.173  loss_box_reg: 0.6  time: 0.9523  data_time: 0.0022  lr: 0.004995  max_mem: 21690M
[32m[11/10 06:11:01 d2.utils.events]: [0m eta: 23:01:45  iter: 519  total_loss: 1.75  loss_cls: 1.175  loss_box_reg: 0.5742  time: 0.9562  data_time: 0.0022  lr: 0.0051948  max_mem: 21690M
[32m[11/10 06:11:22 d2.utils.events]: [0m eta: 23:09:57  iter: 539  total_loss: 1.702  loss_cls: 1.132  loss_box_reg: 0.5618  time: 0.9587  data_time: 0.0048  lr: 0.0053946  max_mem: 21690M
[32m[11/10 06:11:44 d2.utils.events]: [0m eta: 23:14:21  iter: 559  total_loss: 1.654  loss_cls: 1.059  loss_box_reg: 0.5693  time: 0.9632  data_time: 0.0020  lr: 0.0055944  max_mem: 21690M
[32m[11/10 06:12:05 d2.utils.events]: [0m eta: 23:18:29  iter: 579  total_loss: 1.669  loss_cls: 1.103  loss_box_reg: 0.5682  time: 0.9671  data_time: 0.0022  lr: 0.0057942  max_mem: 21690M
[32m[11/10 06:12:27 d2.utils.events]: [0m eta: 23:21:34  iter: 599  total_loss: 1.668  loss_cls: 1.093  loss_box_reg: 0.5596  time: 0.9703  data_time: 0.0035  lr: 0.005994  max_mem: 21690M
[32m[11/10 06:12:47 d2.utils.events]: [0m eta: 23:26:05  iter: 619  total_loss: 1.703  loss_cls: 1.139  loss_box_reg: 0.5776  time: 0.9719  data_time: 0.0035  lr: 0.0061938  max_mem: 21690M
[32m[11/10 06:13:07 d2.utils.events]: [0m eta: 23:25:55  iter: 639  total_loss: 1.526  loss_cls: 0.9935  loss_box_reg: 0.5214  time: 0.9726  data_time: 0.0022  lr: 0.0063936  max_mem: 21690M
[32m[11/10 06:13:31 d2.utils.events]: [0m eta: 23:30:46  iter: 659  total_loss: 1.623  loss_cls: 1.068  loss_box_reg: 0.5482  time: 0.9787  data_time: 0.0021  lr: 0.0065934  max_mem: 21690M
[32m[11/10 06:13:53 d2.utils.events]: [0m eta: 23:39:26  iter: 679  total_loss: 1.648  loss_cls: 1.09  loss_box_reg: 0.5606  time: 0.9825  data_time: 0.0021  lr: 0.0067932  max_mem: 21690M
[32m[11/10 06:14:14 d2.utils.events]: [0m eta: 23:48:32  iter: 699  total_loss: 1.631  loss_cls: 1.074  loss_box_reg: 0.5658  time: 0.9841  data_time: 0.0022  lr: 0.006993  max_mem: 21690M
[32m[11/10 06:14:35 d2.utils.events]: [0m eta: 23:51:40  iter: 719  total_loss: 1.553  loss_cls: 1.033  loss_box_reg: 0.5292  time: 0.9861  data_time: 0.0046  lr: 0.0071928  max_mem: 21690M
[32m[11/10 06:14:57 d2.utils.events]: [0m eta: 23:59:29  iter: 739  total_loss: 1.547  loss_cls: 1.011  loss_box_reg: 0.5288  time: 0.9894  data_time: 0.0021  lr: 0.0073926  max_mem: 21690M
[32m[11/10 06:15:19 d2.utils.events]: [0m eta: 1 day, 0:04:44  iter: 759  total_loss: 1.618  loss_cls: 1.072  loss_box_reg: 0.542  time: 0.9914  data_time: 0.0021  lr: 0.0075924  max_mem: 21699M
[32m[11/10 06:15:40 d2.utils.events]: [0m eta: 1 day, 0:07:07  iter: 779  total_loss: 1.594  loss_cls: 1.037  loss_box_reg: 0.5529  time: 0.9937  data_time: 0.0021  lr: 0.0077922  max_mem: 21699M
[32m[11/10 06:16:03 d2.utils.events]: [0m eta: 1 day, 0:08:35  iter: 799  total_loss: 1.522  loss_cls: 1.019  loss_box_reg: 0.5186  time: 0.9964  data_time: 0.0021  lr: 0.007992  max_mem: 21699M
[32m[11/10 06:16:24 d2.utils.events]: [0m eta: 1 day, 0:12:35  iter: 819  total_loss: 1.595  loss_cls: 1.042  loss_box_reg: 0.5498  time: 0.9984  data_time: 0.0034  lr: 0.0081918  max_mem: 21699M
[32m[11/10 06:16:46 d2.utils.events]: [0m eta: 1 day, 0:13:23  iter: 839  total_loss: 1.479  loss_cls: 0.9793  loss_box_reg: 0.506  time: 1.0009  data_time: 0.0021  lr: 0.0083916  max_mem: 21699M
[32m[11/10 06:17:08 d2.utils.events]: [0m eta: 1 day, 0:13:53  iter: 859  total_loss: 1.543  loss_cls: 1.009  loss_box_reg: 0.5201  time: 1.0022  data_time: 0.0021  lr: 0.0085914  max_mem: 21699M
[32m[11/10 06:17:29 d2.utils.events]: [0m eta: 1 day, 0:18:23  iter: 879  total_loss: 1.518  loss_cls: 0.9954  loss_box_reg: 0.5279  time: 1.0036  data_time: 0.0022  lr: 0.0087912  max_mem: 21699M
[32m[11/10 06:17:52 d2.utils.events]: [0m eta: 1 day, 0:22:13  iter: 899  total_loss: 1.551  loss_cls: 1.026  loss_box_reg: 0.5181  time: 1.0066  data_time: 0.0022  lr: 0.008991  max_mem: 21699M
[32m[11/10 06:18:13 d2.utils.events]: [0m eta: 1 day, 0:26:04  iter: 919  total_loss: 1.567  loss_cls: 1.034  loss_box_reg: 0.5389  time: 1.0079  data_time: 0.0022  lr: 0.0091908  max_mem: 21699M
[32m[11/10 06:18:35 d2.utils.events]: [0m eta: 1 day, 0:26:07  iter: 939  total_loss: 1.59  loss_cls: 1.052  loss_box_reg: 0.5429  time: 1.0090  data_time: 0.0023  lr: 0.0093906  max_mem: 21699M
[32m[11/10 06:18:57 d2.utils.events]: [0m eta: 1 day, 0:27:29  iter: 959  total_loss: 1.611  loss_cls: 1.067  loss_box_reg: 0.546  time: 1.0108  data_time: 0.0022  lr: 0.0095904  max_mem: 21699M
[32m[11/10 06:19:17 d2.utils.events]: [0m eta: 1 day, 0:29:12  iter: 979  total_loss: 1.467  loss_cls: 0.9709  loss_box_reg: 0.4816  time: 1.0109  data_time: 0.0021  lr: 0.0097902  max_mem: 21699M
[32m[11/10 06:19:39 d2.utils.events]: [0m eta: 1 day, 0:35:22  iter: 999  total_loss: 1.52  loss_cls: 0.9946  loss_box_reg: 0.5246  time: 1.0125  data_time: 0.0024  lr: 0.00999  max_mem: 21699M
[32m[11/10 06:20:00 d2.utils.events]: [0m eta: 1 day, 0:40:55  iter: 1019  total_loss: 1.63  loss_cls: 1.075  loss_box_reg: 0.558  time: 1.0133  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:20:22 d2.utils.events]: [0m eta: 1 day, 0:42:24  iter: 1039  total_loss: 1.419  loss_cls: 0.9231  loss_box_reg: 0.4763  time: 1.0149  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:20:42 d2.utils.events]: [0m eta: 1 day, 0:48:38  iter: 1059  total_loss: 1.501  loss_cls: 0.9594  loss_box_reg: 0.536  time: 1.0148  data_time: 0.0062  lr: 0.01  max_mem: 21699M
[32m[11/10 06:21:03 d2.utils.events]: [0m eta: 1 day, 0:56:19  iter: 1079  total_loss: 1.485  loss_cls: 0.9641  loss_box_reg: 0.5196  time: 1.0154  data_time: 0.0048  lr: 0.01  max_mem: 21699M
[32m[11/10 06:21:25 d2.utils.events]: [0m eta: 1 day, 1:05:56  iter: 1099  total_loss: 1.424  loss_cls: 0.9279  loss_box_reg: 0.501  time: 1.0167  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 06:21:46 d2.utils.events]: [0m eta: 1 day, 1:09:38  iter: 1119  total_loss: 1.505  loss_cls: 0.9848  loss_box_reg: 0.5194  time: 1.0176  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:22:08 d2.utils.events]: [0m eta: 1 day, 1:15:10  iter: 1139  total_loss: 1.547  loss_cls: 1.018  loss_box_reg: 0.5312  time: 1.0186  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:22:30 d2.utils.events]: [0m eta: 1 day, 1:26:10  iter: 1159  total_loss: 1.479  loss_cls: 0.9605  loss_box_reg: 0.5284  time: 1.0198  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:22:52 d2.utils.events]: [0m eta: 1 day, 1:29:38  iter: 1179  total_loss: 1.464  loss_cls: 0.9366  loss_box_reg: 0.5305  time: 1.0211  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:23:14 d2.utils.events]: [0m eta: 1 day, 1:38:32  iter: 1199  total_loss: 1.444  loss_cls: 0.9356  loss_box_reg: 0.495  time: 1.0224  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:23:35 d2.utils.events]: [0m eta: 1 day, 1:40:46  iter: 1219  total_loss: 1.442  loss_cls: 0.935  loss_box_reg: 0.5061  time: 1.0229  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:23:58 d2.utils.events]: [0m eta: 1 day, 1:45:15  iter: 1239  total_loss: 1.503  loss_cls: 0.9837  loss_box_reg: 0.5305  time: 1.0250  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:24:22 d2.utils.events]: [0m eta: 1 day, 1:46:56  iter: 1259  total_loss: 1.465  loss_cls: 0.9393  loss_box_reg: 0.5077  time: 1.0272  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:24:42 d2.utils.events]: [0m eta: 1 day, 1:50:09  iter: 1279  total_loss: 1.548  loss_cls: 1.007  loss_box_reg: 0.5332  time: 1.0271  data_time: 0.0025  lr: 0.01  max_mem: 21699M
[32m[11/10 06:25:04 d2.utils.events]: [0m eta: 1 day, 1:53:12  iter: 1299  total_loss: 1.423  loss_cls: 0.9075  loss_box_reg: 0.487  time: 1.0280  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:25:25 d2.utils.events]: [0m eta: 1 day, 1:54:20  iter: 1319  total_loss: 1.405  loss_cls: 0.9143  loss_box_reg: 0.502  time: 1.0284  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 06:25:47 d2.utils.events]: [0m eta: 1 day, 1:54:58  iter: 1339  total_loss: 1.418  loss_cls: 0.9417  loss_box_reg: 0.5105  time: 1.0295  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:26:10 d2.utils.events]: [0m eta: 1 day, 1:56:52  iter: 1359  total_loss: 1.347  loss_cls: 0.8757  loss_box_reg: 0.4772  time: 1.0307  data_time: 0.0041  lr: 0.01  max_mem: 21699M
[32m[11/10 06:26:31 d2.utils.events]: [0m eta: 1 day, 1:57:37  iter: 1379  total_loss: 1.453  loss_cls: 0.938  loss_box_reg: 0.5152  time: 1.0314  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 06:26:53 d2.utils.events]: [0m eta: 1 day, 2:02:31  iter: 1399  total_loss: 1.498  loss_cls: 0.9952  loss_box_reg: 0.5098  time: 1.0322  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:27:14 d2.utils.events]: [0m eta: 1 day, 2:03:06  iter: 1419  total_loss: 1.371  loss_cls: 0.8726  loss_box_reg: 0.4799  time: 1.0328  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:27:36 d2.utils.events]: [0m eta: 1 day, 2:05:08  iter: 1439  total_loss: 1.546  loss_cls: 0.9986  loss_box_reg: 0.5423  time: 1.0336  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:27:58 d2.utils.events]: [0m eta: 1 day, 2:08:20  iter: 1459  total_loss: 1.47  loss_cls: 0.9345  loss_box_reg: 0.507  time: 1.0345  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:28:20 d2.utils.events]: [0m eta: 1 day, 2:10:27  iter: 1479  total_loss: 1.436  loss_cls: 0.9129  loss_box_reg: 0.5124  time: 1.0351  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:28:42 d2.utils.events]: [0m eta: 1 day, 2:10:49  iter: 1499  total_loss: 1.505  loss_cls: 0.9988  loss_box_reg: 0.5155  time: 1.0361  data_time: 0.0051  lr: 0.01  max_mem: 21699M
[32m[11/10 06:29:05 d2.utils.events]: [0m eta: 1 day, 2:13:25  iter: 1519  total_loss: 1.451  loss_cls: 0.9392  loss_box_reg: 0.5054  time: 1.0372  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 06:29:28 d2.utils.events]: [0m eta: 1 day, 2:13:52  iter: 1539  total_loss: 1.423  loss_cls: 0.9257  loss_box_reg: 0.5123  time: 1.0386  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:29:50 d2.utils.events]: [0m eta: 1 day, 2:13:25  iter: 1559  total_loss: 1.374  loss_cls: 0.8744  loss_box_reg: 0.5  time: 1.0391  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:30:12 d2.utils.events]: [0m eta: 1 day, 2:13:04  iter: 1579  total_loss: 1.499  loss_cls: 0.9579  loss_box_reg: 0.5444  time: 1.0399  data_time: 0.0057  lr: 0.01  max_mem: 21699M
[32m[11/10 06:30:33 d2.utils.events]: [0m eta: 1 day, 2:12:48  iter: 1599  total_loss: 1.439  loss_cls: 0.9136  loss_box_reg: 0.5123  time: 1.0402  data_time: 0.0046  lr: 0.01  max_mem: 21699M
[32m[11/10 06:30:55 d2.utils.events]: [0m eta: 1 day, 2:12:17  iter: 1619  total_loss: 1.436  loss_cls: 0.9239  loss_box_reg: 0.5073  time: 1.0409  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:31:18 d2.utils.events]: [0m eta: 1 day, 2:12:38  iter: 1639  total_loss: 1.345  loss_cls: 0.889  loss_box_reg: 0.4942  time: 1.0422  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:31:41 d2.utils.events]: [0m eta: 1 day, 2:10:56  iter: 1659  total_loss: 1.5  loss_cls: 0.9679  loss_box_reg: 0.5196  time: 1.0432  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:32:03 d2.utils.events]: [0m eta: 1 day, 2:08:31  iter: 1679  total_loss: 1.404  loss_cls: 0.8978  loss_box_reg: 0.4986  time: 1.0438  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:32:23 d2.utils.events]: [0m eta: 1 day, 2:09:46  iter: 1699  total_loss: 1.408  loss_cls: 0.8999  loss_box_reg: 0.5123  time: 1.0435  data_time: 0.0035  lr: 0.01  max_mem: 21699M
[32m[11/10 06:32:45 d2.utils.events]: [0m eta: 1 day, 2:10:06  iter: 1719  total_loss: 1.426  loss_cls: 0.9165  loss_box_reg: 0.5165  time: 1.0441  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 06:33:07 d2.utils.events]: [0m eta: 1 day, 2:09:44  iter: 1739  total_loss: 1.37  loss_cls: 0.8867  loss_box_reg: 0.4816  time: 1.0444  data_time: 0.0052  lr: 0.01  max_mem: 21699M
[32m[11/10 06:33:30 d2.utils.events]: [0m eta: 1 day, 2:09:23  iter: 1759  total_loss: 1.394  loss_cls: 0.8851  loss_box_reg: 0.5057  time: 1.0455  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 06:33:51 d2.utils.events]: [0m eta: 1 day, 2:09:02  iter: 1779  total_loss: 1.46  loss_cls: 0.9137  loss_box_reg: 0.5276  time: 1.0457  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:34:14 d2.utils.events]: [0m eta: 1 day, 2:09:14  iter: 1799  total_loss: 1.431  loss_cls: 0.927  loss_box_reg: 0.5097  time: 1.0468  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:34:36 d2.utils.events]: [0m eta: 1 day, 2:09:36  iter: 1819  total_loss: 1.383  loss_cls: 0.872  loss_box_reg: 0.4952  time: 1.0476  data_time: 0.0047  lr: 0.01  max_mem: 21699M
[32m[11/10 06:34:58 d2.utils.events]: [0m eta: 1 day, 2:08:31  iter: 1839  total_loss: 1.409  loss_cls: 0.8996  loss_box_reg: 0.5022  time: 1.0481  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:35:21 d2.utils.events]: [0m eta: 1 day, 2:08:10  iter: 1859  total_loss: 1.292  loss_cls: 0.8334  loss_box_reg: 0.4663  time: 1.0490  data_time: 0.0067  lr: 0.01  max_mem: 21699M
[32m[11/10 06:35:43 d2.utils.events]: [0m eta: 1 day, 2:08:31  iter: 1879  total_loss: 1.427  loss_cls: 0.9127  loss_box_reg: 0.5189  time: 1.0494  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:36:03 d2.utils.events]: [0m eta: 1 day, 2:05:35  iter: 1899  total_loss: 1.455  loss_cls: 0.9336  loss_box_reg: 0.5206  time: 1.0487  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:36:26 d2.utils.events]: [0m eta: 1 day, 2:07:06  iter: 1919  total_loss: 1.411  loss_cls: 0.8908  loss_box_reg: 0.525  time: 1.0496  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:36:47 d2.utils.events]: [0m eta: 1 day, 2:05:36  iter: 1939  total_loss: 1.35  loss_cls: 0.8732  loss_box_reg: 0.4893  time: 1.0497  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:37:10 d2.utils.events]: [0m eta: 1 day, 2:04:56  iter: 1959  total_loss: 1.378  loss_cls: 0.8783  loss_box_reg: 0.4995  time: 1.0506  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:37:31 d2.utils.events]: [0m eta: 1 day, 2:04:35  iter: 1979  total_loss: 1.385  loss_cls: 0.8883  loss_box_reg: 0.5079  time: 1.0508  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:37:53 d2.utils.events]: [0m eta: 1 day, 2:03:39  iter: 1999  total_loss: 1.327  loss_cls: 0.8783  loss_box_reg: 0.4488  time: 1.0509  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:38:15 d2.utils.events]: [0m eta: 1 day, 2:04:47  iter: 2019  total_loss: 1.456  loss_cls: 0.9401  loss_box_reg: 0.5136  time: 1.0517  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:38:37 d2.utils.events]: [0m eta: 1 day, 2:03:49  iter: 2039  total_loss: 1.363  loss_cls: 0.8654  loss_box_reg: 0.5094  time: 1.0521  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:39:00 d2.utils.events]: [0m eta: 1 day, 2:04:43  iter: 2059  total_loss: 1.464  loss_cls: 0.9082  loss_box_reg: 0.5347  time: 1.0529  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:39:21 d2.utils.events]: [0m eta: 1 day, 2:04:58  iter: 2079  total_loss: 1.412  loss_cls: 0.8921  loss_box_reg: 0.51  time: 1.0529  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:39:43 d2.utils.events]: [0m eta: 1 day, 2:04:00  iter: 2099  total_loss: 1.39  loss_cls: 0.8768  loss_box_reg: 0.5088  time: 1.0534  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:40:05 d2.utils.events]: [0m eta: 1 day, 2:06:14  iter: 2119  total_loss: 1.422  loss_cls: 0.8791  loss_box_reg: 0.5135  time: 1.0537  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:40:28 d2.utils.events]: [0m eta: 1 day, 2:05:53  iter: 2139  total_loss: 1.349  loss_cls: 0.8763  loss_box_reg: 0.4896  time: 1.0546  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:40:50 d2.utils.events]: [0m eta: 1 day, 2:05:31  iter: 2159  total_loss: 1.408  loss_cls: 0.881  loss_box_reg: 0.5124  time: 1.0547  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:41:12 d2.utils.events]: [0m eta: 1 day, 2:05:10  iter: 2179  total_loss: 1.313  loss_cls: 0.8257  loss_box_reg: 0.4895  time: 1.0550  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:41:32 d2.utils.events]: [0m eta: 1 day, 2:04:49  iter: 2199  total_loss: 1.294  loss_cls: 0.8108  loss_box_reg: 0.483  time: 1.0548  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:41:55 d2.utils.events]: [0m eta: 1 day, 2:04:27  iter: 2219  total_loss: 1.397  loss_cls: 0.9055  loss_box_reg: 0.4915  time: 1.0553  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:42:17 d2.utils.events]: [0m eta: 1 day, 2:08:20  iter: 2239  total_loss: 1.316  loss_cls: 0.8589  loss_box_reg: 0.4822  time: 1.0559  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:42:39 d2.utils.events]: [0m eta: 1 day, 2:03:44  iter: 2259  total_loss: 1.409  loss_cls: 0.9022  loss_box_reg: 0.5035  time: 1.0560  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:43:01 d2.utils.events]: [0m eta: 1 day, 2:07:37  iter: 2279  total_loss: 1.355  loss_cls: 0.8542  loss_box_reg: 0.4948  time: 1.0566  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 06:43:23 d2.utils.events]: [0m eta: 1 day, 2:04:15  iter: 2299  total_loss: 1.339  loss_cls: 0.8554  loss_box_reg: 0.4895  time: 1.0567  data_time: 0.0073  lr: 0.01  max_mem: 21699M
[32m[11/10 06:43:44 d2.utils.events]: [0m eta: 1 day, 2:02:40  iter: 2319  total_loss: 1.342  loss_cls: 0.8476  loss_box_reg: 0.474  time: 1.0566  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:44:06 d2.utils.events]: [0m eta: 1 day, 2:03:33  iter: 2339  total_loss: 1.326  loss_cls: 0.8394  loss_box_reg: 0.509  time: 1.0573  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:44:28 d2.utils.events]: [0m eta: 1 day, 2:01:57  iter: 2359  total_loss: 1.313  loss_cls: 0.8254  loss_box_reg: 0.4859  time: 1.0573  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 06:44:49 d2.utils.events]: [0m eta: 1 day, 2:01:31  iter: 2379  total_loss: 1.335  loss_cls: 0.8465  loss_box_reg: 0.4851  time: 1.0573  data_time: 0.0057  lr: 0.01  max_mem: 21699M
[32m[11/10 06:45:12 d2.utils.events]: [0m eta: 1 day, 2:01:38  iter: 2399  total_loss: 1.291  loss_cls: 0.8177  loss_box_reg: 0.4743  time: 1.0580  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:45:34 d2.utils.events]: [0m eta: 1 day, 2:00:05  iter: 2419  total_loss: 1.316  loss_cls: 0.8419  loss_box_reg: 0.4825  time: 1.0583  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:45:56 d2.utils.events]: [0m eta: 1 day, 1:59:23  iter: 2439  total_loss: 1.417  loss_cls: 0.8901  loss_box_reg: 0.5315  time: 1.0585  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:46:18 d2.utils.events]: [0m eta: 1 day, 1:59:49  iter: 2459  total_loss: 1.231  loss_cls: 0.7682  loss_box_reg: 0.4661  time: 1.0589  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:46:39 d2.utils.events]: [0m eta: 1 day, 1:57:15  iter: 2479  total_loss: 1.348  loss_cls: 0.835  loss_box_reg: 0.5048  time: 1.0588  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:47:01 d2.utils.events]: [0m eta: 1 day, 1:55:38  iter: 2499  total_loss: 1.338  loss_cls: 0.8458  loss_box_reg: 0.4956  time: 1.0592  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:47:23 d2.utils.events]: [0m eta: 1 day, 1:52:05  iter: 2519  total_loss: 1.291  loss_cls: 0.7931  loss_box_reg: 0.472  time: 1.0595  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:47:44 d2.utils.events]: [0m eta: 1 day, 1:49:37  iter: 2539  total_loss: 1.358  loss_cls: 0.8442  loss_box_reg: 0.4944  time: 1.0592  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:48:06 d2.utils.events]: [0m eta: 1 day, 1:48:28  iter: 2559  total_loss: 1.249  loss_cls: 0.7835  loss_box_reg: 0.4764  time: 1.0596  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:48:28 d2.utils.events]: [0m eta: 1 day, 1:47:40  iter: 2579  total_loss: 1.338  loss_cls: 0.8362  loss_box_reg: 0.4983  time: 1.0596  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:48:48 d2.utils.events]: [0m eta: 1 day, 1:47:18  iter: 2599  total_loss: 1.387  loss_cls: 0.8765  loss_box_reg: 0.515  time: 1.0592  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:49:10 d2.utils.events]: [0m eta: 1 day, 1:48:12  iter: 2619  total_loss: 1.349  loss_cls: 0.841  loss_box_reg: 0.492  time: 1.0597  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:49:32 d2.utils.events]: [0m eta: 1 day, 1:47:03  iter: 2639  total_loss: 1.369  loss_cls: 0.8828  loss_box_reg: 0.4962  time: 1.0599  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:49:55 d2.utils.events]: [0m eta: 1 day, 1:48:18  iter: 2659  total_loss: 1.316  loss_cls: 0.8315  loss_box_reg: 0.4785  time: 1.0603  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:50:17 d2.utils.events]: [0m eta: 1 day, 1:51:49  iter: 2679  total_loss: 1.293  loss_cls: 0.8168  loss_box_reg: 0.4778  time: 1.0608  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:50:38 d2.utils.events]: [0m eta: 1 day, 1:48:53  iter: 2699  total_loss: 1.412  loss_cls: 0.8847  loss_box_reg: 0.5068  time: 1.0605  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:51:00 d2.utils.events]: [0m eta: 1 day, 1:49:56  iter: 2719  total_loss: 1.347  loss_cls: 0.8499  loss_box_reg: 0.508  time: 1.0608  data_time: 0.0186  lr: 0.01  max_mem: 21699M
[32m[11/10 06:51:22 d2.utils.events]: [0m eta: 1 day, 1:50:14  iter: 2739  total_loss: 1.421  loss_cls: 0.8778  loss_box_reg: 0.5397  time: 1.0613  data_time: 0.0042  lr: 0.01  max_mem: 21699M
[32m[11/10 06:51:44 d2.utils.events]: [0m eta: 1 day, 1:47:49  iter: 2759  total_loss: 1.261  loss_cls: 0.797  loss_box_reg: 0.4535  time: 1.0613  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:52:06 d2.utils.events]: [0m eta: 1 day, 1:50:36  iter: 2779  total_loss: 1.389  loss_cls: 0.8763  loss_box_reg: 0.5215  time: 1.0616  data_time: 0.0025  lr: 0.01  max_mem: 21699M
[32m[11/10 06:52:29 d2.utils.events]: [0m eta: 1 day, 1:50:15  iter: 2799  total_loss: 1.238  loss_cls: 0.7796  loss_box_reg: 0.4794  time: 1.0622  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:52:51 d2.utils.events]: [0m eta: 1 day, 1:48:49  iter: 2819  total_loss: 1.38  loss_cls: 0.8678  loss_box_reg: 0.4984  time: 1.0626  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:53:12 d2.utils.events]: [0m eta: 1 day, 1:48:28  iter: 2839  total_loss: 1.381  loss_cls: 0.8623  loss_box_reg: 0.4989  time: 1.0624  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:53:34 d2.utils.events]: [0m eta: 1 day, 1:50:35  iter: 2859  total_loss: 1.248  loss_cls: 0.7999  loss_box_reg: 0.465  time: 1.0626  data_time: 0.0041  lr: 0.01  max_mem: 21699M
[32m[11/10 06:53:57 d2.utils.events]: [0m eta: 1 day, 1:49:43  iter: 2879  total_loss: 1.332  loss_cls: 0.8405  loss_box_reg: 0.4916  time: 1.0631  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 06:54:18 d2.utils.events]: [0m eta: 1 day, 1:50:36  iter: 2899  total_loss: 1.302  loss_cls: 0.8317  loss_box_reg: 0.5023  time: 1.0631  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:54:40 d2.utils.events]: [0m eta: 1 day, 1:50:50  iter: 2919  total_loss: 1.282  loss_cls: 0.8125  loss_box_reg: 0.4806  time: 1.0632  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:55:03 d2.utils.events]: [0m eta: 1 day, 1:52:27  iter: 2939  total_loss: 1.275  loss_cls: 0.8177  loss_box_reg: 0.4687  time: 1.0638  data_time: 0.0033  lr: 0.01  max_mem: 21699M
[32m[11/10 06:55:26 d2.utils.events]: [0m eta: 1 day, 1:52:47  iter: 2959  total_loss: 1.384  loss_cls: 0.8631  loss_box_reg: 0.518  time: 1.0642  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:55:48 d2.utils.events]: [0m eta: 1 day, 1:54:14  iter: 2979  total_loss: 1.347  loss_cls: 0.826  loss_box_reg: 0.5073  time: 1.0647  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:56:09 d2.utils.events]: [0m eta: 1 day, 1:52:15  iter: 2999  total_loss: 1.322  loss_cls: 0.8385  loss_box_reg: 0.5059  time: 1.0646  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:56:32 d2.utils.events]: [0m eta: 1 day, 1:51:01  iter: 3019  total_loss: 1.309  loss_cls: 0.8193  loss_box_reg: 0.4896  time: 1.0649  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:56:54 d2.utils.events]: [0m eta: 1 day, 1:52:23  iter: 3039  total_loss: 1.325  loss_cls: 0.8449  loss_box_reg: 0.479  time: 1.0653  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:57:16 d2.utils.events]: [0m eta: 1 day, 1:51:11  iter: 3059  total_loss: 1.305  loss_cls: 0.8308  loss_box_reg: 0.4659  time: 1.0656  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:57:38 d2.utils.events]: [0m eta: 1 day, 1:50:50  iter: 3079  total_loss: 1.326  loss_cls: 0.8444  loss_box_reg: 0.4839  time: 1.0657  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 06:58:01 d2.utils.events]: [0m eta: 1 day, 1:52:05  iter: 3099  total_loss: 1.377  loss_cls: 0.8515  loss_box_reg: 0.5147  time: 1.0662  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:58:23 d2.utils.events]: [0m eta: 1 day, 1:50:07  iter: 3119  total_loss: 1.328  loss_cls: 0.8165  loss_box_reg: 0.4921  time: 1.0663  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 06:58:44 d2.utils.events]: [0m eta: 1 day, 1:49:13  iter: 3139  total_loss: 1.202  loss_cls: 0.745  loss_box_reg: 0.4473  time: 1.0664  data_time: 0.0030  lr: 0.01  max_mem: 21699M
[32m[11/10 06:59:06 d2.utils.events]: [0m eta: 1 day, 1:49:02  iter: 3159  total_loss: 1.205  loss_cls: 0.7547  loss_box_reg: 0.4432  time: 1.0665  data_time: 0.0050  lr: 0.01  max_mem: 21699M
[32m[11/10 06:59:28 d2.utils.events]: [0m eta: 1 day, 1:49:03  iter: 3179  total_loss: 1.322  loss_cls: 0.8124  loss_box_reg: 0.5103  time: 1.0668  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 06:59:50 d2.utils.events]: [0m eta: 1 day, 1:48:09  iter: 3199  total_loss: 1.256  loss_cls: 0.7904  loss_box_reg: 0.4809  time: 1.0668  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:00:12 d2.utils.events]: [0m eta: 1 day, 1:49:10  iter: 3219  total_loss: 1.21  loss_cls: 0.7763  loss_box_reg: 0.448  time: 1.0670  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 07:00:34 d2.utils.events]: [0m eta: 1 day, 1:47:37  iter: 3239  total_loss: 1.292  loss_cls: 0.8036  loss_box_reg: 0.4842  time: 1.0673  data_time: 0.0047  lr: 0.01  max_mem: 21699M
[32m[11/10 07:00:56 d2.utils.events]: [0m eta: 1 day, 1:47:37  iter: 3259  total_loss: 1.258  loss_cls: 0.7835  loss_box_reg: 0.4739  time: 1.0673  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:01:17 d2.utils.events]: [0m eta: 1 day, 1:46:43  iter: 3279  total_loss: 1.329  loss_cls: 0.8226  loss_box_reg: 0.5224  time: 1.0674  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:01:39 d2.utils.events]: [0m eta: 1 day, 1:46:02  iter: 3299  total_loss: 1.217  loss_cls: 0.7744  loss_box_reg: 0.4601  time: 1.0674  data_time: 0.0040  lr: 0.01  max_mem: 21699M
[32m[11/10 07:02:02 d2.utils.events]: [0m eta: 1 day, 1:47:23  iter: 3319  total_loss: 1.3  loss_cls: 0.808  loss_box_reg: 0.4914  time: 1.0679  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:02:24 d2.utils.events]: [0m eta: 1 day, 1:46:12  iter: 3339  total_loss: 1.259  loss_cls: 0.7691  loss_box_reg: 0.492  time: 1.0682  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:02:47 d2.utils.events]: [0m eta: 1 day, 1:47:27  iter: 3359  total_loss: 1.351  loss_cls: 0.8411  loss_box_reg: 0.5125  time: 1.0684  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:03:09 d2.utils.events]: [0m eta: 1 day, 1:50:39  iter: 3379  total_loss: 1.234  loss_cls: 0.7677  loss_box_reg: 0.4561  time: 1.0687  data_time: 0.0038  lr: 0.01  max_mem: 21699M
[32m[11/10 07:03:31 d2.utils.events]: [0m eta: 1 day, 1:50:17  iter: 3399  total_loss: 1.288  loss_cls: 0.8164  loss_box_reg: 0.4875  time: 1.0689  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:03:52 d2.utils.events]: [0m eta: 1 day, 1:50:35  iter: 3419  total_loss: 1.258  loss_cls: 0.7833  loss_box_reg: 0.4871  time: 1.0688  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:04:14 d2.utils.events]: [0m eta: 1 day, 1:49:46  iter: 3439  total_loss: 1.241  loss_cls: 0.7731  loss_box_reg: 0.4739  time: 1.0688  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:04:36 d2.utils.events]: [0m eta: 1 day, 1:48:35  iter: 3459  total_loss: 1.208  loss_cls: 0.752  loss_box_reg: 0.4598  time: 1.0690  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:04:57 d2.utils.events]: [0m eta: 1 day, 1:49:30  iter: 3479  total_loss: 1.333  loss_cls: 0.8321  loss_box_reg: 0.5074  time: 1.0689  data_time: 0.0033  lr: 0.01  max_mem: 21699M
[32m[11/10 07:05:19 d2.utils.events]: [0m eta: 1 day, 1:52:59  iter: 3499  total_loss: 1.245  loss_cls: 0.785  loss_box_reg: 0.4593  time: 1.0692  data_time: 0.0027  lr: 0.01  max_mem: 21699M
[32m[11/10 07:05:42 d2.utils.events]: [0m eta: 1 day, 1:56:47  iter: 3519  total_loss: 1.297  loss_cls: 0.8244  loss_box_reg: 0.4824  time: 1.0694  data_time: 0.0042  lr: 0.01  max_mem: 21699M
[32m[11/10 07:06:04 d2.utils.events]: [0m eta: 1 day, 1:59:28  iter: 3539  total_loss: 1.278  loss_cls: 0.7834  loss_box_reg: 0.4979  time: 1.0696  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:06:26 d2.utils.events]: [0m eta: 1 day, 2:01:49  iter: 3559  total_loss: 1.274  loss_cls: 0.7877  loss_box_reg: 0.482  time: 1.0697  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:06:48 d2.utils.events]: [0m eta: 1 day, 2:01:28  iter: 3579  total_loss: 1.151  loss_cls: 0.7188  loss_box_reg: 0.4471  time: 1.0699  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:07:10 d2.utils.events]: [0m eta: 1 day, 2:01:18  iter: 3599  total_loss: 1.247  loss_cls: 0.7868  loss_box_reg: 0.471  time: 1.0700  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:07:32 d2.utils.events]: [0m eta: 1 day, 2:01:46  iter: 3619  total_loss: 1.313  loss_cls: 0.8052  loss_box_reg: 0.4799  time: 1.0702  data_time: 0.0043  lr: 0.01  max_mem: 21699M
[32m[11/10 07:07:54 d2.utils.events]: [0m eta: 1 day, 2:01:54  iter: 3639  total_loss: 1.229  loss_cls: 0.7748  loss_box_reg: 0.4782  time: 1.0704  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:08:17 d2.utils.events]: [0m eta: 1 day, 2:01:33  iter: 3659  total_loss: 1.331  loss_cls: 0.8198  loss_box_reg: 0.4993  time: 1.0707  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 07:08:39 d2.utils.events]: [0m eta: 1 day, 2:02:23  iter: 3679  total_loss: 1.243  loss_cls: 0.7727  loss_box_reg: 0.4616  time: 1.0710  data_time: 0.0056  lr: 0.01  max_mem: 21699M
[32m[11/10 07:09:01 d2.utils.events]: [0m eta: 1 day, 2:03:32  iter: 3699  total_loss: 1.376  loss_cls: 0.8366  loss_box_reg: 0.5126  time: 1.0711  data_time: 0.0055  lr: 0.01  max_mem: 21699M
[32m[11/10 07:09:25 d2.utils.events]: [0m eta: 1 day, 2:04:14  iter: 3719  total_loss: 1.3  loss_cls: 0.7866  loss_box_reg: 0.4934  time: 1.0718  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:09:48 d2.utils.events]: [0m eta: 1 day, 2:02:49  iter: 3739  total_loss: 1.313  loss_cls: 0.8098  loss_box_reg: 0.4813  time: 1.0722  data_time: 0.0037  lr: 0.01  max_mem: 21699M
[32m[11/10 07:10:10 d2.utils.events]: [0m eta: 1 day, 2:04:24  iter: 3759  total_loss: 1.34  loss_cls: 0.8378  loss_box_reg: 0.5079  time: 1.0723  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:10:32 d2.utils.events]: [0m eta: 1 day, 2:02:05  iter: 3779  total_loss: 1.314  loss_cls: 0.8064  loss_box_reg: 0.4965  time: 1.0725  data_time: 0.0086  lr: 0.01  max_mem: 21699M
[32m[11/10 07:10:53 d2.utils.events]: [0m eta: 1 day, 1:59:53  iter: 3799  total_loss: 1.29  loss_cls: 0.8146  loss_box_reg: 0.4589  time: 1.0722  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:11:14 d2.utils.events]: [0m eta: 1 day, 1:59:51  iter: 3819  total_loss: 1.189  loss_cls: 0.7617  loss_box_reg: 0.4584  time: 1.0722  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:11:36 d2.utils.events]: [0m eta: 1 day, 1:58:29  iter: 3839  total_loss: 1.379  loss_cls: 0.8774  loss_box_reg: 0.5132  time: 1.0723  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:11:58 d2.utils.events]: [0m eta: 1 day, 1:57:46  iter: 3859  total_loss: 1.22  loss_cls: 0.7703  loss_box_reg: 0.4501  time: 1.0725  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:12:20 d2.utils.events]: [0m eta: 1 day, 1:57:45  iter: 3879  total_loss: 1.26  loss_cls: 0.7741  loss_box_reg: 0.4957  time: 1.0727  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:12:41 d2.utils.events]: [0m eta: 1 day, 1:58:05  iter: 3899  total_loss: 1.175  loss_cls: 0.735  loss_box_reg: 0.4529  time: 1.0725  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:13:04 d2.utils.events]: [0m eta: 1 day, 1:56:41  iter: 3919  total_loss: 1.228  loss_cls: 0.7843  loss_box_reg: 0.4627  time: 1.0729  data_time: 0.0032  lr: 0.01  max_mem: 21699M
[32m[11/10 07:13:27 d2.utils.events]: [0m eta: 1 day, 1:54:19  iter: 3939  total_loss: 1.337  loss_cls: 0.8144  loss_box_reg: 0.4894  time: 1.0731  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:13:50 d2.utils.events]: [0m eta: 1 day, 1:53:57  iter: 3959  total_loss: 1.234  loss_cls: 0.7598  loss_box_reg: 0.4746  time: 1.0735  data_time: 0.0047  lr: 0.01  max_mem: 21699M
[32m[11/10 07:14:12 d2.utils.events]: [0m eta: 1 day, 1:54:42  iter: 3979  total_loss: 1.25  loss_cls: 0.7816  loss_box_reg: 0.4792  time: 1.0736  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:14:34 d2.utils.events]: [0m eta: 1 day, 1:56:16  iter: 3999  total_loss: 1.202  loss_cls: 0.7316  loss_box_reg: 0.4731  time: 1.0738  data_time: 0.0037  lr: 0.01  max_mem: 21699M
[32m[11/10 07:14:57 d2.utils.events]: [0m eta: 1 day, 1:58:48  iter: 4019  total_loss: 1.327  loss_cls: 0.8161  loss_box_reg: 0.514  time: 1.0740  data_time: 0.0040  lr: 0.01  max_mem: 21699M
[32m[11/10 07:15:18 d2.utils.events]: [0m eta: 1 day, 1:58:26  iter: 4039  total_loss: 1.317  loss_cls: 0.8035  loss_box_reg: 0.509  time: 1.0741  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:15:41 d2.utils.events]: [0m eta: 1 day, 1:58:32  iter: 4059  total_loss: 1.187  loss_cls: 0.7541  loss_box_reg: 0.4522  time: 1.0743  data_time: 0.0039  lr: 0.01  max_mem: 21699M
[32m[11/10 07:16:03 d2.utils.events]: [0m eta: 1 day, 1:58:18  iter: 4079  total_loss: 1.247  loss_cls: 0.7633  loss_box_reg: 0.4653  time: 1.0744  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:16:23 d2.utils.events]: [0m eta: 1 day, 1:56:28  iter: 4099  total_loss: 1.265  loss_cls: 0.7842  loss_box_reg: 0.4709  time: 1.0740  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:16:45 d2.utils.events]: [0m eta: 1 day, 1:56:06  iter: 4119  total_loss: 1.257  loss_cls: 0.7803  loss_box_reg: 0.4916  time: 1.0741  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:17:06 d2.utils.events]: [0m eta: 1 day, 1:54:45  iter: 4139  total_loss: 1.29  loss_cls: 0.779  loss_box_reg: 0.5142  time: 1.0741  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:17:28 d2.utils.events]: [0m eta: 1 day, 1:53:58  iter: 4159  total_loss: 1.214  loss_cls: 0.7529  loss_box_reg: 0.4549  time: 1.0742  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:17:51 d2.utils.events]: [0m eta: 1 day, 1:55:54  iter: 4179  total_loss: 1.335  loss_cls: 0.837  loss_box_reg: 0.5038  time: 1.0745  data_time: 0.0038  lr: 0.01  max_mem: 21699M
[32m[11/10 07:18:14 d2.utils.events]: [0m eta: 1 day, 1:56:30  iter: 4199  total_loss: 1.271  loss_cls: 0.7808  loss_box_reg: 0.4791  time: 1.0749  data_time: 0.0048  lr: 0.01  max_mem: 21699M
[32m[11/10 07:18:37 d2.utils.events]: [0m eta: 1 day, 1:56:03  iter: 4219  total_loss: 1.234  loss_cls: 0.7535  loss_box_reg: 0.4635  time: 1.0751  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:18:58 d2.utils.events]: [0m eta: 1 day, 1:55:42  iter: 4239  total_loss: 1.24  loss_cls: 0.7609  loss_box_reg: 0.4863  time: 1.0750  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:19:20 d2.utils.events]: [0m eta: 1 day, 1:55:25  iter: 4259  total_loss: 1.308  loss_cls: 0.7823  loss_box_reg: 0.5156  time: 1.0751  data_time: 0.0045  lr: 0.01  max_mem: 21699M
[32m[11/10 07:19:41 d2.utils.events]: [0m eta: 1 day, 1:55:03  iter: 4279  total_loss: 1.289  loss_cls: 0.7936  loss_box_reg: 0.4904  time: 1.0749  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:20:04 d2.utils.events]: [0m eta: 1 day, 1:57:47  iter: 4299  total_loss: 1.211  loss_cls: 0.7516  loss_box_reg: 0.4595  time: 1.0754  data_time: 0.0049  lr: 0.01  max_mem: 21699M
[32m[11/10 07:20:26 d2.utils.events]: [0m eta: 1 day, 1:54:43  iter: 4319  total_loss: 1.204  loss_cls: 0.7478  loss_box_reg: 0.4656  time: 1.0754  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:20:49 d2.utils.events]: [0m eta: 1 day, 1:55:13  iter: 4339  total_loss: 1.214  loss_cls: 0.769  loss_box_reg: 0.45  time: 1.0757  data_time: 0.0037  lr: 0.01  max_mem: 21699M
[32m[11/10 07:21:11 d2.utils.events]: [0m eta: 1 day, 1:53:31  iter: 4359  total_loss: 1.287  loss_cls: 0.8108  loss_box_reg: 0.4844  time: 1.0758  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:21:34 d2.utils.events]: [0m eta: 1 day, 1:52:16  iter: 4379  total_loss: 1.27  loss_cls: 0.7737  loss_box_reg: 0.5025  time: 1.0760  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:21:55 d2.utils.events]: [0m eta: 1 day, 1:49:07  iter: 4399  total_loss: 1.31  loss_cls: 0.7824  loss_box_reg: 0.4891  time: 1.0759  data_time: 0.0039  lr: 0.01  max_mem: 21699M
[32m[11/10 07:22:17 d2.utils.events]: [0m eta: 1 day, 1:48:46  iter: 4419  total_loss: 1.206  loss_cls: 0.7496  loss_box_reg: 0.4595  time: 1.0762  data_time: 0.0025  lr: 0.01  max_mem: 21699M
[32m[11/10 07:22:40 d2.utils.events]: [0m eta: 1 day, 1:49:04  iter: 4439  total_loss: 1.293  loss_cls: 0.7965  loss_box_reg: 0.4981  time: 1.0763  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:23:01 d2.utils.events]: [0m eta: 1 day, 1:48:42  iter: 4459  total_loss: 1.159  loss_cls: 0.7201  loss_box_reg: 0.4392  time: 1.0763  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:23:23 d2.utils.events]: [0m eta: 1 day, 1:48:21  iter: 4479  total_loss: 1.365  loss_cls: 0.8456  loss_box_reg: 0.5201  time: 1.0765  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:23:46 d2.utils.events]: [0m eta: 1 day, 1:46:37  iter: 4499  total_loss: 1.229  loss_cls: 0.7721  loss_box_reg: 0.4505  time: 1.0767  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:24:07 d2.utils.events]: [0m eta: 1 day, 1:45:21  iter: 4519  total_loss: 1.26  loss_cls: 0.7938  loss_box_reg: 0.493  time: 1.0766  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:24:28 d2.utils.events]: [0m eta: 1 day, 1:41:43  iter: 4539  total_loss: 1.187  loss_cls: 0.7272  loss_box_reg: 0.4727  time: 1.0765  data_time: 0.0056  lr: 0.01  max_mem: 21699M
[32m[11/10 07:24:51 d2.utils.events]: [0m eta: 1 day, 1:39:49  iter: 4559  total_loss: 1.266  loss_cls: 0.7727  loss_box_reg: 0.4797  time: 1.0767  data_time: 0.0038  lr: 0.01  max_mem: 21699M
[32m[11/10 07:25:12 d2.utils.events]: [0m eta: 1 day, 1:38:53  iter: 4579  total_loss: 1.179  loss_cls: 0.7262  loss_box_reg: 0.465  time: 1.0766  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:25:34 d2.utils.events]: [0m eta: 1 day, 1:38:12  iter: 4599  total_loss: 1.264  loss_cls: 0.771  loss_box_reg: 0.4972  time: 1.0767  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:25:57 d2.utils.events]: [0m eta: 1 day, 1:36:51  iter: 4619  total_loss: 1.233  loss_cls: 0.7738  loss_box_reg: 0.4864  time: 1.0769  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:26:19 d2.utils.events]: [0m eta: 1 day, 1:36:19  iter: 4639  total_loss: 1.24  loss_cls: 0.7691  loss_box_reg: 0.4703  time: 1.0771  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:26:41 d2.utils.events]: [0m eta: 1 day, 1:35:43  iter: 4659  total_loss: 1.226  loss_cls: 0.7642  loss_box_reg: 0.4768  time: 1.0771  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:27:04 d2.utils.events]: [0m eta: 1 day, 1:34:29  iter: 4679  total_loss: 1.366  loss_cls: 0.8361  loss_box_reg: 0.521  time: 1.0773  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:27:26 d2.utils.events]: [0m eta: 1 day, 1:34:07  iter: 4699  total_loss: 1.219  loss_cls: 0.728  loss_box_reg: 0.4746  time: 1.0774  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:27:47 d2.utils.events]: [0m eta: 1 day, 1:27:45  iter: 4719  total_loss: 1.247  loss_cls: 0.7526  loss_box_reg: 0.481  time: 1.0774  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:28:10 d2.utils.events]: [0m eta: 1 day, 1:27:23  iter: 4739  total_loss: 1.164  loss_cls: 0.7156  loss_box_reg: 0.4611  time: 1.0777  data_time: 0.0026  lr: 0.01  max_mem: 21699M
[32m[11/10 07:28:32 d2.utils.events]: [0m eta: 1 day, 1:24:51  iter: 4759  total_loss: 1.273  loss_cls: 0.7794  loss_box_reg: 0.4896  time: 1.0777  data_time: 0.0030  lr: 0.01  max_mem: 21699M
[32m[11/10 07:28:54 d2.utils.events]: [0m eta: 1 day, 1:29:11  iter: 4779  total_loss: 1.162  loss_cls: 0.7171  loss_box_reg: 0.458  time: 1.0778  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:29:16 d2.utils.events]: [0m eta: 1 day, 1:31:33  iter: 4799  total_loss: 1.226  loss_cls: 0.7588  loss_box_reg: 0.4743  time: 1.0779  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:29:39 d2.utils.events]: [0m eta: 1 day, 1:31:12  iter: 4819  total_loss: 1.269  loss_cls: 0.7613  loss_box_reg: 0.5094  time: 1.0781  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:30:01 d2.utils.events]: [0m eta: 1 day, 1:32:28  iter: 4839  total_loss: 1.248  loss_cls: 0.7668  loss_box_reg: 0.4902  time: 1.0782  data_time: 0.0038  lr: 0.01  max_mem: 21699M
[32m[11/10 07:30:22 d2.utils.events]: [0m eta: 1 day, 1:32:07  iter: 4859  total_loss: 1.309  loss_cls: 0.7739  loss_box_reg: 0.5034  time: 1.0781  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:30:44 d2.utils.events]: [0m eta: 1 day, 1:32:00  iter: 4879  total_loss: 1.244  loss_cls: 0.7316  loss_box_reg: 0.4915  time: 1.0782  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 07:31:07 d2.utils.events]: [0m eta: 1 day, 1:31:23  iter: 4899  total_loss: 1.309  loss_cls: 0.8055  loss_box_reg: 0.5039  time: 1.0784  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:31:28 d2.utils.events]: [0m eta: 1 day, 1:29:58  iter: 4919  total_loss: 1.146  loss_cls: 0.7086  loss_box_reg: 0.4513  time: 1.0782  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:31:50 d2.utils.events]: [0m eta: 1 day, 1:29:20  iter: 4939  total_loss: 1.212  loss_cls: 0.7506  loss_box_reg: 0.4662  time: 1.0783  data_time: 0.0054  lr: 0.01  max_mem: 21699M
[32m[11/10 07:32:12 d2.utils.events]: [0m eta: 1 day, 1:28:59  iter: 4959  total_loss: 1.236  loss_cls: 0.7567  loss_box_reg: 0.4696  time: 1.0784  data_time: 0.0043  lr: 0.01  max_mem: 21699M
[32m[11/10 07:32:34 d2.utils.events]: [0m eta: 1 day, 1:28:37  iter: 4979  total_loss: 1.242  loss_cls: 0.7781  loss_box_reg: 0.4756  time: 1.0786  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:32:55 fvcore.common.checkpoint]: [0mSaving checkpoint to ./results/VG/retinanet/retinanet_r50_fpn_VG_concepts_cat_depth0/model_0004999.pth
[32m[11/10 07:32:57 d2.utils.events]: [0m eta: 1 day, 1:22:44  iter: 4999  total_loss: 1.184  loss_cls: 0.7364  loss_box_reg: 0.4682  time: 1.0784  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 07:33:18 d2.utils.events]: [0m eta: 1 day, 1:19:40  iter: 5019  total_loss: 1.171  loss_cls: 0.7363  loss_box_reg: 0.4496  time: 1.0783  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:33:39 d2.utils.events]: [0m eta: 1 day, 1:19:18  iter: 5039  total_loss: 1.232  loss_cls: 0.7465  loss_box_reg: 0.4927  time: 1.0783  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:34:04 d2.utils.events]: [0m eta: 1 day, 1:20:52  iter: 5059  total_loss: 1.226  loss_cls: 0.7502  loss_box_reg: 0.4437  time: 1.0788  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:34:26 d2.utils.events]: [0m eta: 1 day, 1:23:42  iter: 5079  total_loss: 1.146  loss_cls: 0.7154  loss_box_reg: 0.4371  time: 1.0790  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:34:49 d2.utils.events]: [0m eta: 1 day, 1:26:44  iter: 5099  total_loss: 1.235  loss_cls: 0.7588  loss_box_reg: 0.4833  time: 1.0792  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:35:11 d2.utils.events]: [0m eta: 1 day, 1:27:25  iter: 5119  total_loss: 1.325  loss_cls: 0.816  loss_box_reg: 0.5164  time: 1.0792  data_time: 0.0026  lr: 0.01  max_mem: 21699M
[32m[11/10 07:35:34 d2.utils.events]: [0m eta: 1 day, 1:28:07  iter: 5139  total_loss: 1.191  loss_cls: 0.7214  loss_box_reg: 0.4637  time: 1.0795  data_time: 0.0026  lr: 0.01  max_mem: 21699M
[32m[11/10 07:35:55 d2.utils.events]: [0m eta: 1 day, 1:27:36  iter: 5159  total_loss: 1.216  loss_cls: 0.7364  loss_box_reg: 0.4693  time: 1.0794  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 07:36:15 d2.utils.events]: [0m eta: 1 day, 1:25:01  iter: 5179  total_loss: 1.199  loss_cls: 0.7424  loss_box_reg: 0.4582  time: 1.0790  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 07:36:39 d2.utils.events]: [0m eta: 1 day, 1:25:08  iter: 5199  total_loss: 1.222  loss_cls: 0.7369  loss_box_reg: 0.4863  time: 1.0795  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:37:02 d2.utils.events]: [0m eta: 1 day, 1:26:09  iter: 5219  total_loss: 1.229  loss_cls: 0.7597  loss_box_reg: 0.4649  time: 1.0797  data_time: 0.0070  lr: 0.01  max_mem: 21699M
[32m[11/10 07:37:23 d2.utils.events]: [0m eta: 1 day, 1:20:50  iter: 5239  total_loss: 1.277  loss_cls: 0.7687  loss_box_reg: 0.4996  time: 1.0796  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:37:45 d2.utils.events]: [0m eta: 1 day, 1:23:39  iter: 5259  total_loss: 1.203  loss_cls: 0.7436  loss_box_reg: 0.4546  time: 1.0797  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:38:07 d2.utils.events]: [0m eta: 1 day, 1:23:41  iter: 5279  total_loss: 1.228  loss_cls: 0.7386  loss_box_reg: 0.4882  time: 1.0797  data_time: 0.0053  lr: 0.01  max_mem: 21699M
[32m[11/10 07:38:28 d2.utils.events]: [0m eta: 1 day, 1:20:12  iter: 5299  total_loss: 1.284  loss_cls: 0.798  loss_box_reg: 0.4866  time: 1.0796  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:38:50 d2.utils.events]: [0m eta: 1 day, 1:22:06  iter: 5319  total_loss: 1.2  loss_cls: 0.7405  loss_box_reg: 0.468  time: 1.0797  data_time: 0.0047  lr: 0.01  max_mem: 21699M
[32m[11/10 07:39:11 d2.utils.events]: [0m eta: 1 day, 1:19:29  iter: 5339  total_loss: 1.231  loss_cls: 0.7532  loss_box_reg: 0.459  time: 1.0796  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 07:39:33 d2.utils.events]: [0m eta: 1 day, 1:22:15  iter: 5359  total_loss: 1.176  loss_cls: 0.7187  loss_box_reg: 0.4762  time: 1.0795  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:39:55 d2.utils.events]: [0m eta: 1 day, 1:22:14  iter: 5379  total_loss: 1.233  loss_cls: 0.7566  loss_box_reg: 0.4773  time: 1.0797  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:40:18 d2.utils.events]: [0m eta: 1 day, 1:22:55  iter: 5399  total_loss: 1.206  loss_cls: 0.735  loss_box_reg: 0.4601  time: 1.0799  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:40:39 d2.utils.events]: [0m eta: 1 day, 1:19:27  iter: 5419  total_loss: 1.277  loss_cls: 0.7823  loss_box_reg: 0.4978  time: 1.0798  data_time: 0.0028  lr: 0.01  max_mem: 21699M
[32m[11/10 07:41:02 d2.utils.events]: [0m eta: 1 day, 1:21:09  iter: 5439  total_loss: 1.201  loss_cls: 0.7291  loss_box_reg: 0.4591  time: 1.0801  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 07:41:24 d2.utils.events]: [0m eta: 1 day, 1:19:35  iter: 5459  total_loss: 1.245  loss_cls: 0.7621  loss_box_reg: 0.478  time: 1.0800  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 07:41:44 d2.utils.events]: [0m eta: 1 day, 1:13:38  iter: 5479  total_loss: 1.218  loss_cls: 0.7478  loss_box_reg: 0.4746  time: 1.0798  data_time: 0.0035  lr: 0.01  max_mem: 21699M
[32m[11/10 07:42:06 d2.utils.events]: [0m eta: 1 day, 1:15:36  iter: 5499  total_loss: 1.163  loss_cls: 0.7218  loss_box_reg: 0.4408  time: 1.0799  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:42:29 d2.utils.events]: [0m eta: 1 day, 1:17:40  iter: 5519  total_loss: 1.18  loss_cls: 0.7069  loss_box_reg: 0.4786  time: 1.0801  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:42:51 d2.utils.events]: [0m eta: 1 day, 1:17:41  iter: 5539  total_loss: 1.222  loss_cls: 0.7295  loss_box_reg: 0.4962  time: 1.0801  data_time: 0.0037  lr: 0.01  max_mem: 21699M
[32m[11/10 07:43:12 d2.utils.events]: [0m eta: 1 day, 1:17:54  iter: 5559  total_loss: 1.146  loss_cls: 0.7233  loss_box_reg: 0.4583  time: 1.0801  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 07:43:33 d2.utils.events]: [0m eta: 1 day, 1:18:09  iter: 5579  total_loss: 1.17  loss_cls: 0.6976  loss_box_reg: 0.4468  time: 1.0800  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:43:56 d2.utils.events]: [0m eta: 1 day, 1:20:02  iter: 5599  total_loss: 1.18  loss_cls: 0.7349  loss_box_reg: 0.4563  time: 1.0801  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:44:18 d2.utils.events]: [0m eta: 1 day, 1:19:40  iter: 5619  total_loss: 1.303  loss_cls: 0.7921  loss_box_reg: 0.4966  time: 1.0802  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:44:40 d2.utils.events]: [0m eta: 1 day, 1:17:46  iter: 5639  total_loss: 1.245  loss_cls: 0.7646  loss_box_reg: 0.4879  time: 1.0803  data_time: 0.0039  lr: 0.01  max_mem: 21699M
[32m[11/10 07:45:03 d2.utils.events]: [0m eta: 1 day, 1:16:43  iter: 5659  total_loss: 1.19  loss_cls: 0.731  loss_box_reg: 0.453  time: 1.0805  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:45:25 d2.utils.events]: [0m eta: 1 day, 1:15:33  iter: 5679  total_loss: 1.242  loss_cls: 0.7661  loss_box_reg: 0.494  time: 1.0805  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:45:47 d2.utils.events]: [0m eta: 1 day, 1:14:49  iter: 5699  total_loss: 1.282  loss_cls: 0.7736  loss_box_reg: 0.501  time: 1.0807  data_time: 0.0050  lr: 0.01  max_mem: 21699M
[32m[11/10 07:46:08 d2.utils.events]: [0m eta: 1 day, 1:14:27  iter: 5719  total_loss: 1.084  loss_cls: 0.6513  loss_box_reg: 0.4245  time: 1.0805  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:46:31 d2.utils.events]: [0m eta: 1 day, 1:15:58  iter: 5739  total_loss: 1.285  loss_cls: 0.7907  loss_box_reg: 0.4862  time: 1.0808  data_time: 0.0059  lr: 0.01  max_mem: 21699M
[32m[11/10 07:46:54 d2.utils.events]: [0m eta: 1 day, 1:17:57  iter: 5759  total_loss: 1.247  loss_cls: 0.7658  loss_box_reg: 0.4856  time: 1.0809  data_time: 0.0025  lr: 0.01  max_mem: 21699M
[32m[11/10 07:47:15 d2.utils.events]: [0m eta: 1 day, 1:15:47  iter: 5779  total_loss: 1.173  loss_cls: 0.7129  loss_box_reg: 0.4429  time: 1.0808  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:47:37 d2.utils.events]: [0m eta: 1 day, 1:14:54  iter: 5799  total_loss: 1.193  loss_cls: 0.7261  loss_box_reg: 0.4652  time: 1.0809  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:47:59 d2.utils.events]: [0m eta: 1 day, 1:14:32  iter: 5819  total_loss: 1.192  loss_cls: 0.7289  loss_box_reg: 0.4635  time: 1.0809  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:48:21 d2.utils.events]: [0m eta: 1 day, 1:12:18  iter: 5839  total_loss: 1.206  loss_cls: 0.728  loss_box_reg: 0.4911  time: 1.0809  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:48:42 d2.utils.events]: [0m eta: 1 day, 1:11:56  iter: 5859  total_loss: 1.17  loss_cls: 0.727  loss_box_reg: 0.4428  time: 1.0809  data_time: 0.0043  lr: 0.01  max_mem: 21699M
[32m[11/10 07:49:05 d2.utils.events]: [0m eta: 1 day, 1:11:35  iter: 5879  total_loss: 1.206  loss_cls: 0.7362  loss_box_reg: 0.463  time: 1.0811  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:49:26 d2.utils.events]: [0m eta: 1 day, 1:10:17  iter: 5899  total_loss: 1.209  loss_cls: 0.7366  loss_box_reg: 0.4944  time: 1.0810  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:49:48 d2.utils.events]: [0m eta: 1 day, 1:10:51  iter: 5919  total_loss: 1.215  loss_cls: 0.7387  loss_box_reg: 0.4662  time: 1.0811  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 07:50:09 d2.utils.events]: [0m eta: 1 day, 1:10:30  iter: 5939  total_loss: 1.158  loss_cls: 0.7138  loss_box_reg: 0.4624  time: 1.0809  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:50:31 d2.utils.events]: [0m eta: 1 day, 1:08:19  iter: 5959  total_loss: 1.247  loss_cls: 0.7576  loss_box_reg: 0.4903  time: 1.0811  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:50:54 d2.utils.events]: [0m eta: 1 day, 1:07:00  iter: 5979  total_loss: 1.218  loss_cls: 0.7352  loss_box_reg: 0.4827  time: 1.0812  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:51:16 d2.utils.events]: [0m eta: 1 day, 1:06:38  iter: 5999  total_loss: 1.24  loss_cls: 0.7532  loss_box_reg: 0.4899  time: 1.0812  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:51:37 d2.utils.events]: [0m eta: 1 day, 1:03:25  iter: 6019  total_loss: 1.119  loss_cls: 0.6865  loss_box_reg: 0.4462  time: 1.0812  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:51:59 d2.utils.events]: [0m eta: 1 day, 1:01:35  iter: 6039  total_loss: 1.252  loss_cls: 0.775  loss_box_reg: 0.4771  time: 1.0813  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:52:23 d2.utils.events]: [0m eta: 1 day, 0:59:25  iter: 6059  total_loss: 1.172  loss_cls: 0.6984  loss_box_reg: 0.4561  time: 1.0816  data_time: 0.0025  lr: 0.01  max_mem: 21699M
[32m[11/10 07:52:45 d2.utils.events]: [0m eta: 1 day, 0:56:07  iter: 6079  total_loss: 1.199  loss_cls: 0.7498  loss_box_reg: 0.4641  time: 1.0817  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:53:08 d2.utils.events]: [0m eta: 1 day, 0:55:04  iter: 6099  total_loss: 1.223  loss_cls: 0.7211  loss_box_reg: 0.4934  time: 1.0818  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:53:31 d2.utils.events]: [0m eta: 1 day, 0:55:24  iter: 6119  total_loss: 1.26  loss_cls: 0.7608  loss_box_reg: 0.5106  time: 1.0820  data_time: 0.0043  lr: 0.01  max_mem: 21699M
[32m[11/10 07:53:53 d2.utils.events]: [0m eta: 1 day, 0:54:21  iter: 6139  total_loss: 1.156  loss_cls: 0.7106  loss_box_reg: 0.4499  time: 1.0821  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:54:14 d2.utils.events]: [0m eta: 1 day, 0:51:30  iter: 6159  total_loss: 1.128  loss_cls: 0.7003  loss_box_reg: 0.4333  time: 1.0819  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:54:35 d2.utils.events]: [0m eta: 1 day, 0:53:30  iter: 6179  total_loss: 1.216  loss_cls: 0.7532  loss_box_reg: 0.464  time: 1.0819  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:54:57 d2.utils.events]: [0m eta: 1 day, 0:50:09  iter: 6199  total_loss: 1.168  loss_cls: 0.7077  loss_box_reg: 0.4528  time: 1.0820  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:55:19 d2.utils.events]: [0m eta: 1 day, 0:48:40  iter: 6219  total_loss: 1.207  loss_cls: 0.7312  loss_box_reg: 0.4757  time: 1.0820  data_time: 0.0040  lr: 0.01  max_mem: 21699M
[32m[11/10 07:55:42 d2.utils.events]: [0m eta: 1 day, 0:52:34  iter: 6239  total_loss: 1.181  loss_cls: 0.7164  loss_box_reg: 0.4717  time: 1.0822  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:56:03 d2.utils.events]: [0m eta: 1 day, 0:49:12  iter: 6259  total_loss: 1.24  loss_cls: 0.7539  loss_box_reg: 0.4896  time: 1.0820  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:56:25 d2.utils.events]: [0m eta: 1 day, 0:47:36  iter: 6279  total_loss: 1.174  loss_cls: 0.7095  loss_box_reg: 0.4438  time: 1.0822  data_time: 0.0028  lr: 0.01  max_mem: 21699M
[32m[11/10 07:56:48 d2.utils.events]: [0m eta: 1 day, 0:48:31  iter: 6299  total_loss: 1.227  loss_cls: 0.7519  loss_box_reg: 0.4723  time: 1.0823  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 07:57:10 d2.utils.events]: [0m eta: 1 day, 0:48:39  iter: 6319  total_loss: 1.168  loss_cls: 0.7232  loss_box_reg: 0.4597  time: 1.0823  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:57:32 d2.utils.events]: [0m eta: 1 day, 0:45:45  iter: 6339  total_loss: 1.235  loss_cls: 0.7567  loss_box_reg: 0.4695  time: 1.0824  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:57:54 d2.utils.events]: [0m eta: 1 day, 0:41:36  iter: 6359  total_loss: 1.216  loss_cls: 0.7256  loss_box_reg: 0.4741  time: 1.0825  data_time: 0.0042  lr: 0.01  max_mem: 21699M
[32m[11/10 07:58:16 d2.utils.events]: [0m eta: 1 day, 0:40:44  iter: 6379  total_loss: 1.301  loss_cls: 0.7407  loss_box_reg: 0.5191  time: 1.0826  data_time: 0.0045  lr: 0.01  max_mem: 21699M
[32m[11/10 07:58:38 d2.utils.events]: [0m eta: 1 day, 0:38:15  iter: 6399  total_loss: 1.23  loss_cls: 0.7412  loss_box_reg: 0.4698  time: 1.0826  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:59:00 d2.utils.events]: [0m eta: 1 day, 0:40:32  iter: 6419  total_loss: 1.144  loss_cls: 0.6853  loss_box_reg: 0.4504  time: 1.0827  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 07:59:23 d2.utils.events]: [0m eta: 1 day, 0:39:49  iter: 6439  total_loss: 1.225  loss_cls: 0.7426  loss_box_reg: 0.4852  time: 1.0828  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 07:59:44 d2.utils.events]: [0m eta: 1 day, 0:39:38  iter: 6459  total_loss: 1.152  loss_cls: 0.6988  loss_box_reg: 0.4536  time: 1.0826  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:00:07 d2.utils.events]: [0m eta: 1 day, 0:45:48  iter: 6479  total_loss: 1.215  loss_cls: 0.7263  loss_box_reg: 0.4855  time: 1.0829  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:00:28 d2.utils.events]: [0m eta: 1 day, 0:40:52  iter: 6499  total_loss: 1.231  loss_cls: 0.7364  loss_box_reg: 0.4606  time: 1.0828  data_time: 0.0114  lr: 0.01  max_mem: 21699M
[32m[11/10 08:00:51 d2.utils.events]: [0m eta: 1 day, 0:40:06  iter: 6519  total_loss: 1.182  loss_cls: 0.707  loss_box_reg: 0.4736  time: 1.0829  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:01:13 d2.utils.events]: [0m eta: 1 day, 0:42:11  iter: 6539  total_loss: 1.195  loss_cls: 0.7018  loss_box_reg: 0.4931  time: 1.0830  data_time: 0.0048  lr: 0.01  max_mem: 21699M
[32m[11/10 08:01:36 d2.utils.events]: [0m eta: 1 day, 0:41:50  iter: 6559  total_loss: 1.149  loss_cls: 0.6835  loss_box_reg: 0.4548  time: 1.0832  data_time: 0.0035  lr: 0.01  max_mem: 21699M
[32m[11/10 08:01:57 d2.utils.events]: [0m eta: 1 day, 0:41:29  iter: 6579  total_loss: 1.197  loss_cls: 0.7254  loss_box_reg: 0.4772  time: 1.0831  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:02:18 d2.utils.events]: [0m eta: 1 day, 0:41:07  iter: 6599  total_loss: 1.197  loss_cls: 0.7357  loss_box_reg: 0.4634  time: 1.0830  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 08:02:41 d2.utils.events]: [0m eta: 1 day, 0:38:45  iter: 6619  total_loss: 1.115  loss_cls: 0.6859  loss_box_reg: 0.4544  time: 1.0831  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:03:03 d2.utils.events]: [0m eta: 1 day, 0:45:06  iter: 6639  total_loss: 1.15  loss_cls: 0.6788  loss_box_reg: 0.4588  time: 1.0832  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:03:25 d2.utils.events]: [0m eta: 1 day, 0:42:28  iter: 6659  total_loss: 1.16  loss_cls: 0.7106  loss_box_reg: 0.4468  time: 1.0833  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:03:46 d2.utils.events]: [0m eta: 1 day, 0:40:42  iter: 6679  total_loss: 1.269  loss_cls: 0.7631  loss_box_reg: 0.494  time: 1.0831  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:04:09 d2.utils.events]: [0m eta: 1 day, 0:41:10  iter: 6699  total_loss: 1.221  loss_cls: 0.7397  loss_box_reg: 0.467  time: 1.0833  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:04:30 d2.utils.events]: [0m eta: 1 day, 0:40:49  iter: 6719  total_loss: 1.148  loss_cls: 0.699  loss_box_reg: 0.4485  time: 1.0833  data_time: 0.0074  lr: 0.01  max_mem: 21699M
[32m[11/10 08:04:53 d2.utils.events]: [0m eta: 1 day, 0:39:38  iter: 6739  total_loss: 1.212  loss_cls: 0.7264  loss_box_reg: 0.4665  time: 1.0834  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:05:15 d2.utils.events]: [0m eta: 1 day, 0:36:30  iter: 6759  total_loss: 1.168  loss_cls: 0.7112  loss_box_reg: 0.4653  time: 1.0835  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 08:05:36 d2.utils.events]: [0m eta: 1 day, 0:35:29  iter: 6779  total_loss: 1.248  loss_cls: 0.7767  loss_box_reg: 0.4872  time: 1.0834  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:05:58 d2.utils.events]: [0m eta: 1 day, 0:34:44  iter: 6799  total_loss: 1.22  loss_cls: 0.7447  loss_box_reg: 0.4815  time: 1.0834  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:06:19 d2.utils.events]: [0m eta: 1 day, 0:33:27  iter: 6819  total_loss: 1.238  loss_cls: 0.754  loss_box_reg: 0.4871  time: 1.0834  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:06:41 d2.utils.events]: [0m eta: 1 day, 0:33:29  iter: 6839  total_loss: 1.229  loss_cls: 0.7576  loss_box_reg: 0.4743  time: 1.0833  data_time: 0.0071  lr: 0.01  max_mem: 21699M
[32m[11/10 08:07:03 d2.utils.events]: [0m eta: 1 day, 0:33:40  iter: 6859  total_loss: 1.184  loss_cls: 0.732  loss_box_reg: 0.4664  time: 1.0834  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 08:07:24 d2.utils.events]: [0m eta: 1 day, 0:31:18  iter: 6879  total_loss: 1.114  loss_cls: 0.6903  loss_box_reg: 0.451  time: 1.0832  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:07:46 d2.utils.events]: [0m eta: 1 day, 0:30:56  iter: 6899  total_loss: 1.123  loss_cls: 0.6921  loss_box_reg: 0.4351  time: 1.0833  data_time: 0.0028  lr: 0.01  max_mem: 21699M
[32m[11/10 08:08:08 d2.utils.events]: [0m eta: 1 day, 0:29:53  iter: 6919  total_loss: 1.202  loss_cls: 0.724  loss_box_reg: 0.4722  time: 1.0833  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:08:29 d2.utils.events]: [0m eta: 1 day, 0:30:14  iter: 6939  total_loss: 1.182  loss_cls: 0.7089  loss_box_reg: 0.4627  time: 1.0833  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:08:52 d2.utils.events]: [0m eta: 1 day, 0:32:18  iter: 6959  total_loss: 1.244  loss_cls: 0.7647  loss_box_reg: 0.4799  time: 1.0835  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 08:09:14 d2.utils.events]: [0m eta: 1 day, 0:31:31  iter: 6979  total_loss: 1.194  loss_cls: 0.7536  loss_box_reg: 0.4681  time: 1.0834  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:09:36 d2.utils.events]: [0m eta: 1 day, 0:31:57  iter: 6999  total_loss: 1.208  loss_cls: 0.7401  loss_box_reg: 0.4607  time: 1.0835  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:09:57 d2.utils.events]: [0m eta: 1 day, 0:33:19  iter: 7019  total_loss: 1.247  loss_cls: 0.7597  loss_box_reg: 0.4933  time: 1.0834  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:10:20 d2.utils.events]: [0m eta: 1 day, 0:32:58  iter: 7039  total_loss: 1.226  loss_cls: 0.7599  loss_box_reg: 0.4681  time: 1.0836  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 08:10:41 d2.utils.events]: [0m eta: 1 day, 0:29:35  iter: 7059  total_loss: 1.117  loss_cls: 0.6974  loss_box_reg: 0.4325  time: 1.0835  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:11:04 d2.utils.events]: [0m eta: 1 day, 0:30:01  iter: 7079  total_loss: 1.249  loss_cls: 0.7601  loss_box_reg: 0.4987  time: 1.0837  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:11:26 d2.utils.events]: [0m eta: 1 day, 0:29:37  iter: 7099  total_loss: 1.187  loss_cls: 0.6917  loss_box_reg: 0.4756  time: 1.0838  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:11:48 d2.utils.events]: [0m eta: 1 day, 0:28:31  iter: 7119  total_loss: 1.185  loss_cls: 0.71  loss_box_reg: 0.4686  time: 1.0838  data_time: 0.0045  lr: 0.01  max_mem: 21699M
[32m[11/10 08:12:11 d2.utils.events]: [0m eta: 1 day, 0:28:54  iter: 7139  total_loss: 1.232  loss_cls: 0.7197  loss_box_reg: 0.4857  time: 1.0839  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:12:32 d2.utils.events]: [0m eta: 1 day, 0:29:17  iter: 7159  total_loss: 1.174  loss_cls: 0.7197  loss_box_reg: 0.4633  time: 1.0839  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:12:55 d2.utils.events]: [0m eta: 1 day, 0:28:56  iter: 7179  total_loss: 1.321  loss_cls: 0.8135  loss_box_reg: 0.5127  time: 1.0840  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:13:17 d2.utils.events]: [0m eta: 1 day, 0:28:20  iter: 7199  total_loss: 1.172  loss_cls: 0.7048  loss_box_reg: 0.4622  time: 1.0840  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:13:39 d2.utils.events]: [0m eta: 1 day, 0:27:41  iter: 7219  total_loss: 1.184  loss_cls: 0.7157  loss_box_reg: 0.4548  time: 1.0841  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:14:01 d2.utils.events]: [0m eta: 1 day, 0:27:11  iter: 7239  total_loss: 1.26  loss_cls: 0.7429  loss_box_reg: 0.4969  time: 1.0841  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:14:23 d2.utils.events]: [0m eta: 1 day, 0:26:49  iter: 7259  total_loss: 1.165  loss_cls: 0.702  loss_box_reg: 0.4619  time: 1.0842  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:14:45 d2.utils.events]: [0m eta: 1 day, 0:26:28  iter: 7279  total_loss: 1.201  loss_cls: 0.7268  loss_box_reg: 0.4799  time: 1.0842  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 08:15:07 d2.utils.events]: [0m eta: 1 day, 0:26:16  iter: 7299  total_loss: 1.165  loss_cls: 0.7118  loss_box_reg: 0.4684  time: 1.0842  data_time: 0.0070  lr: 0.01  max_mem: 21699M
[32m[11/10 08:15:29 d2.utils.events]: [0m eta: 1 day, 0:25:28  iter: 7319  total_loss: 1.212  loss_cls: 0.7352  loss_box_reg: 0.4772  time: 1.0842  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:15:51 d2.utils.events]: [0m eta: 1 day, 0:25:22  iter: 7339  total_loss: 1.248  loss_cls: 0.7568  loss_box_reg: 0.4788  time: 1.0843  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 08:16:13 d2.utils.events]: [0m eta: 1 day, 0:25:10  iter: 7359  total_loss: 1.16  loss_cls: 0.7168  loss_box_reg: 0.468  time: 1.0843  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:16:34 d2.utils.events]: [0m eta: 1 day, 0:24:49  iter: 7379  total_loss: 1.197  loss_cls: 0.73  loss_box_reg: 0.4679  time: 1.0843  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:16:55 d2.utils.events]: [0m eta: 1 day, 0:24:47  iter: 7399  total_loss: 1.157  loss_cls: 0.7035  loss_box_reg: 0.4626  time: 1.0841  data_time: 0.0037  lr: 0.01  max_mem: 21699M
[32m[11/10 08:17:18 d2.utils.events]: [0m eta: 1 day, 0:24:26  iter: 7419  total_loss: 1.198  loss_cls: 0.7082  loss_box_reg: 0.4609  time: 1.0843  data_time: 0.0044  lr: 0.01  max_mem: 21699M
[32m[11/10 08:17:40 d2.utils.events]: [0m eta: 1 day, 0:24:24  iter: 7439  total_loss: 1.191  loss_cls: 0.6924  loss_box_reg: 0.4804  time: 1.0843  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 08:18:02 d2.utils.events]: [0m eta: 1 day, 0:24:03  iter: 7459  total_loss: 1.187  loss_cls: 0.7126  loss_box_reg: 0.4666  time: 1.0843  data_time: 0.0033  lr: 0.01  max_mem: 21699M
[32m[11/10 08:18:23 d2.utils.events]: [0m eta: 1 day, 0:21:36  iter: 7479  total_loss: 1.177  loss_cls: 0.7063  loss_box_reg: 0.4832  time: 1.0843  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:18:45 d2.utils.events]: [0m eta: 1 day, 0:21:15  iter: 7499  total_loss: 1.184  loss_cls: 0.7152  loss_box_reg: 0.4229  time: 1.0843  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 08:19:07 d2.utils.events]: [0m eta: 1 day, 0:21:56  iter: 7519  total_loss: 1.189  loss_cls: 0.7233  loss_box_reg: 0.4673  time: 1.0844  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:19:30 d2.utils.events]: [0m eta: 1 day, 0:21:34  iter: 7539  total_loss: 1.11  loss_cls: 0.6822  loss_box_reg: 0.4424  time: 1.0845  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 08:19:52 d2.utils.events]: [0m eta: 1 day, 0:20:47  iter: 7559  total_loss: 1.198  loss_cls: 0.7287  loss_box_reg: 0.4585  time: 1.0845  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:20:14 d2.utils.events]: [0m eta: 1 day, 0:20:52  iter: 7579  total_loss: 1.117  loss_cls: 0.6646  loss_box_reg: 0.4376  time: 1.0846  data_time: 0.0039  lr: 0.01  max_mem: 21699M
[32m[11/10 08:20:36 d2.utils.events]: [0m eta: 1 day, 0:20:31  iter: 7599  total_loss: 1.129  loss_cls: 0.6954  loss_box_reg: 0.4368  time: 1.0846  data_time: 0.0027  lr: 0.01  max_mem: 21699M
[32m[11/10 08:20:57 d2.utils.events]: [0m eta: 1 day, 0:19:43  iter: 7619  total_loss: 1.205  loss_cls: 0.7395  loss_box_reg: 0.4714  time: 1.0845  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:21:20 d2.utils.events]: [0m eta: 1 day, 0:17:50  iter: 7639  total_loss: 1.119  loss_cls: 0.6733  loss_box_reg: 0.4527  time: 1.0847  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:21:41 d2.utils.events]: [0m eta: 1 day, 0:17:20  iter: 7659  total_loss: 1.244  loss_cls: 0.7651  loss_box_reg: 0.4896  time: 1.0846  data_time: 0.0046  lr: 0.01  max_mem: 21699M
[32m[11/10 08:22:03 d2.utils.events]: [0m eta: 1 day, 0:18:39  iter: 7679  total_loss: 1.205  loss_cls: 0.73  loss_box_reg: 0.4721  time: 1.0846  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:22:25 d2.utils.events]: [0m eta: 1 day, 0:18:18  iter: 7699  total_loss: 1.16  loss_cls: 0.6917  loss_box_reg: 0.461  time: 1.0847  data_time: 0.0025  lr: 0.01  max_mem: 21699M
[32m[11/10 08:22:47 d2.utils.events]: [0m eta: 1 day, 0:18:29  iter: 7719  total_loss: 1.221  loss_cls: 0.7181  loss_box_reg: 0.4757  time: 1.0847  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:23:09 d2.utils.events]: [0m eta: 1 day, 0:17:36  iter: 7739  total_loss: 1.194  loss_cls: 0.7354  loss_box_reg: 0.472  time: 1.0848  data_time: 0.0051  lr: 0.01  max_mem: 21699M
[32m[11/10 08:23:31 d2.utils.events]: [0m eta: 1 day, 0:15:42  iter: 7759  total_loss: 1.103  loss_cls: 0.6718  loss_box_reg: 0.4473  time: 1.0847  data_time: 0.0049  lr: 0.01  max_mem: 21699M
[32m[11/10 08:23:54 d2.utils.events]: [0m eta: 1 day, 0:17:25  iter: 7779  total_loss: 1.195  loss_cls: 0.7283  loss_box_reg: 0.474  time: 1.0849  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:24:16 d2.utils.events]: [0m eta: 1 day, 0:17:57  iter: 7799  total_loss: 1.146  loss_cls: 0.6862  loss_box_reg: 0.4723  time: 1.0849  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:24:38 d2.utils.events]: [0m eta: 1 day, 0:18:38  iter: 7819  total_loss: 1.188  loss_cls: 0.7185  loss_box_reg: 0.4731  time: 1.0849  data_time: 0.0035  lr: 0.01  max_mem: 21699M
[32m[11/10 08:24:59 d2.utils.events]: [0m eta: 1 day, 0:17:53  iter: 7839  total_loss: 1.177  loss_cls: 0.7284  loss_box_reg: 0.4734  time: 1.0849  data_time: 0.0055  lr: 0.01  max_mem: 21699M
[32m[11/10 08:25:22 d2.utils.events]: [0m eta: 1 day, 0:18:09  iter: 7859  total_loss: 1.18  loss_cls: 0.7147  loss_box_reg: 0.465  time: 1.0850  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:25:44 d2.utils.events]: [0m eta: 1 day, 0:20:39  iter: 7879  total_loss: 1.235  loss_cls: 0.7671  loss_box_reg: 0.472  time: 1.0850  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 08:26:06 d2.utils.events]: [0m eta: 1 day, 0:23:37  iter: 7899  total_loss: 1.217  loss_cls: 0.7255  loss_box_reg: 0.4812  time: 1.0851  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:26:27 d2.utils.events]: [0m eta: 1 day, 0:23:16  iter: 7919  total_loss: 1.164  loss_cls: 0.7022  loss_box_reg: 0.4554  time: 1.0850  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:26:49 d2.utils.events]: [0m eta: 1 day, 0:21:58  iter: 7939  total_loss: 1.235  loss_cls: 0.7201  loss_box_reg: 0.493  time: 1.0850  data_time: 0.0043  lr: 0.01  max_mem: 21699M
[32m[11/10 08:27:11 d2.utils.events]: [0m eta: 1 day, 0:17:18  iter: 7959  total_loss: 1.178  loss_cls: 0.7176  loss_box_reg: 0.4454  time: 1.0850  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:27:32 d2.utils.events]: [0m eta: 1 day, 0:15:07  iter: 7979  total_loss: 1.15  loss_cls: 0.6979  loss_box_reg: 0.4585  time: 1.0850  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:27:56 d2.utils.events]: [0m eta: 1 day, 0:16:36  iter: 7999  total_loss: 1.142  loss_cls: 0.7048  loss_box_reg: 0.4451  time: 1.0852  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:28:16 d2.utils.events]: [0m eta: 1 day, 0:14:25  iter: 8019  total_loss: 1.201  loss_cls: 0.7226  loss_box_reg: 0.4707  time: 1.0850  data_time: 0.0050  lr: 0.01  max_mem: 21699M
[32m[11/10 08:28:37 d2.utils.events]: [0m eta: 1 day, 0:13:55  iter: 8039  total_loss: 1.129  loss_cls: 0.6842  loss_box_reg: 0.4477  time: 1.0850  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:28:59 d2.utils.events]: [0m eta: 1 day, 0:15:32  iter: 8059  total_loss: 1.179  loss_cls: 0.7161  loss_box_reg: 0.4719  time: 1.0849  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:29:21 d2.utils.events]: [0m eta: 1 day, 0:13:51  iter: 8079  total_loss: 1.238  loss_cls: 0.739  loss_box_reg: 0.5015  time: 1.0850  data_time: 0.0035  lr: 0.01  max_mem: 21699M
[32m[11/10 08:29:43 d2.utils.events]: [0m eta: 1 day, 0:14:31  iter: 8099  total_loss: 1.238  loss_cls: 0.7235  loss_box_reg: 0.4872  time: 1.0850  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:30:04 d2.utils.events]: [0m eta: 1 day, 0:12:38  iter: 8119  total_loss: 1.206  loss_cls: 0.7132  loss_box_reg: 0.4812  time: 1.0850  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:30:25 d2.utils.events]: [0m eta: 1 day, 0:08:03  iter: 8139  total_loss: 1.175  loss_cls: 0.7063  loss_box_reg: 0.4669  time: 1.0849  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:30:48 d2.utils.events]: [0m eta: 1 day, 0:10:15  iter: 8159  total_loss: 1.19  loss_cls: 0.7079  loss_box_reg: 0.4721  time: 1.0850  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:31:10 d2.utils.events]: [0m eta: 1 day, 0:11:26  iter: 8179  total_loss: 1.171  loss_cls: 0.7057  loss_box_reg: 0.4656  time: 1.0851  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:31:33 d2.utils.events]: [0m eta: 1 day, 0:12:44  iter: 8199  total_loss: 1.181  loss_cls: 0.6985  loss_box_reg: 0.4677  time: 1.0852  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:31:55 d2.utils.events]: [0m eta: 1 day, 0:13:27  iter: 8219  total_loss: 1.168  loss_cls: 0.7111  loss_box_reg: 0.4647  time: 1.0852  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:32:17 d2.utils.events]: [0m eta: 1 day, 0:13:12  iter: 8239  total_loss: 1.171  loss_cls: 0.6788  loss_box_reg: 0.4878  time: 1.0852  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:32:38 d2.utils.events]: [0m eta: 1 day, 0:15:54  iter: 8259  total_loss: 1.163  loss_cls: 0.699  loss_box_reg: 0.4555  time: 1.0852  data_time: 0.0039  lr: 0.01  max_mem: 21699M
[32m[11/10 08:33:01 d2.utils.events]: [0m eta: 1 day, 0:15:43  iter: 8279  total_loss: 1.233  loss_cls: 0.7261  loss_box_reg: 0.4945  time: 1.0853  data_time: 0.0068  lr: 0.01  max_mem: 21699M
[32m[11/10 08:33:23 d2.utils.events]: [0m eta: 1 day, 0:13:22  iter: 8299  total_loss: 1.238  loss_cls: 0.7616  loss_box_reg: 0.4931  time: 1.0853  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:33:45 d2.utils.events]: [0m eta: 1 day, 0:14:30  iter: 8319  total_loss: 1.139  loss_cls: 0.6874  loss_box_reg: 0.452  time: 1.0853  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:34:06 d2.utils.events]: [0m eta: 1 day, 0:15:01  iter: 8339  total_loss: 1.199  loss_cls: 0.7056  loss_box_reg: 0.4821  time: 1.0853  data_time: 0.0038  lr: 0.01  max_mem: 21699M
[32m[11/10 08:34:29 d2.utils.events]: [0m eta: 1 day, 0:14:11  iter: 8359  total_loss: 1.203  loss_cls: 0.731  loss_box_reg: 0.4831  time: 1.0854  data_time: 0.0027  lr: 0.01  max_mem: 21699M
[32m[11/10 08:34:50 d2.utils.events]: [0m eta: 1 day, 0:13:50  iter: 8379  total_loss: 1.135  loss_cls: 0.6853  loss_box_reg: 0.4559  time: 1.0853  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:35:12 d2.utils.events]: [0m eta: 1 day, 0:13:57  iter: 8399  total_loss: 1.136  loss_cls: 0.6938  loss_box_reg: 0.4558  time: 1.0853  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:35:33 d2.utils.events]: [0m eta: 1 day, 0:12:43  iter: 8419  total_loss: 1.147  loss_cls: 0.6795  loss_box_reg: 0.4744  time: 1.0853  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:35:57 d2.utils.events]: [0m eta: 1 day, 0:09:14  iter: 8439  total_loss: 1.115  loss_cls: 0.6674  loss_box_reg: 0.4617  time: 1.0855  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:36:20 d2.utils.events]: [0m eta: 1 day, 0:16:10  iter: 8459  total_loss: 1.181  loss_cls: 0.726  loss_box_reg: 0.4585  time: 1.0856  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:36:41 d2.utils.events]: [0m eta: 1 day, 0:18:52  iter: 8479  total_loss: 1.206  loss_cls: 0.7271  loss_box_reg: 0.4753  time: 1.0856  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:37:04 d2.utils.events]: [0m eta: 1 day, 0:19:34  iter: 8499  total_loss: 1.19  loss_cls: 0.7156  loss_box_reg: 0.4618  time: 1.0856  data_time: 0.0054  lr: 0.01  max_mem: 21699M
[32m[11/10 08:37:27 d2.utils.events]: [0m eta: 1 day, 0:18:58  iter: 8519  total_loss: 1.169  loss_cls: 0.7297  loss_box_reg: 0.4426  time: 1.0858  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:37:49 d2.utils.events]: [0m eta: 1 day, 0:18:51  iter: 8539  total_loss: 1.174  loss_cls: 0.7167  loss_box_reg: 0.4593  time: 1.0859  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:38:12 d2.utils.events]: [0m eta: 1 day, 0:18:29  iter: 8559  total_loss: 1.244  loss_cls: 0.7367  loss_box_reg: 0.5067  time: 1.0860  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:38:33 d2.utils.events]: [0m eta: 1 day, 0:18:08  iter: 8579  total_loss: 1.162  loss_cls: 0.6727  loss_box_reg: 0.4649  time: 1.0859  data_time: 0.0027  lr: 0.01  max_mem: 21699M
[32m[11/10 08:38:55 d2.utils.events]: [0m eta: 1 day, 0:17:15  iter: 8599  total_loss: 1.131  loss_cls: 0.6816  loss_box_reg: 0.4491  time: 1.0859  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:39:17 d2.utils.events]: [0m eta: 1 day, 0:18:16  iter: 8619  total_loss: 1.126  loss_cls: 0.6828  loss_box_reg: 0.4569  time: 1.0860  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 08:39:40 d2.utils.events]: [0m eta: 1 day, 0:17:16  iter: 8639  total_loss: 1.124  loss_cls: 0.6758  loss_box_reg: 0.4494  time: 1.0861  data_time: 0.0043  lr: 0.01  max_mem: 21699M
[32m[11/10 08:40:02 d2.utils.events]: [0m eta: 1 day, 0:18:34  iter: 8659  total_loss: 1.195  loss_cls: 0.7179  loss_box_reg: 0.4814  time: 1.0862  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:40:24 d2.utils.events]: [0m eta: 1 day, 0:16:46  iter: 8679  total_loss: 1.113  loss_cls: 0.6782  loss_box_reg: 0.4363  time: 1.0862  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 08:40:47 d2.utils.events]: [0m eta: 1 day, 0:17:51  iter: 8699  total_loss: 1.192  loss_cls: 0.7087  loss_box_reg: 0.4902  time: 1.0863  data_time: 0.0045  lr: 0.01  max_mem: 21699M
[32m[11/10 08:41:09 d2.utils.events]: [0m eta: 1 day, 0:18:39  iter: 8719  total_loss: 1.189  loss_cls: 0.7113  loss_box_reg: 0.4615  time: 1.0863  data_time: 0.0040  lr: 0.01  max_mem: 21699M
[32m[11/10 08:41:32 d2.utils.events]: [0m eta: 1 day, 0:21:09  iter: 8739  total_loss: 1.178  loss_cls: 0.6988  loss_box_reg: 0.4716  time: 1.0864  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 08:41:55 d2.utils.events]: [0m eta: 1 day, 0:21:40  iter: 8759  total_loss: 1.167  loss_cls: 0.7087  loss_box_reg: 0.4618  time: 1.0866  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:42:17 d2.utils.events]: [0m eta: 1 day, 0:22:50  iter: 8779  total_loss: 1.152  loss_cls: 0.6897  loss_box_reg: 0.4592  time: 1.0866  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:42:39 d2.utils.events]: [0m eta: 1 day, 0:20:57  iter: 8799  total_loss: 1.147  loss_cls: 0.693  loss_box_reg: 0.4499  time: 1.0866  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 08:43:01 d2.utils.events]: [0m eta: 1 day, 0:21:13  iter: 8819  total_loss: 1.161  loss_cls: 0.6851  loss_box_reg: 0.4892  time: 1.0867  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 08:43:24 d2.utils.events]: [0m eta: 1 day, 0:21:45  iter: 8839  total_loss: 1.094  loss_cls: 0.6534  loss_box_reg: 0.4247  time: 1.0868  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:43:46 d2.utils.events]: [0m eta: 1 day, 0:19:52  iter: 8859  total_loss: 1.163  loss_cls: 0.6979  loss_box_reg: 0.4704  time: 1.0869  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:44:08 d2.utils.events]: [0m eta: 1 day, 0:19:04  iter: 8879  total_loss: 1.206  loss_cls: 0.6919  loss_box_reg: 0.4761  time: 1.0868  data_time: 0.0068  lr: 0.01  max_mem: 21699M
[32m[11/10 08:44:31 d2.utils.events]: [0m eta: 1 day, 0:20:00  iter: 8899  total_loss: 1.15  loss_cls: 0.7081  loss_box_reg: 0.4405  time: 1.0870  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:44:52 d2.utils.events]: [0m eta: 1 day, 0:18:07  iter: 8919  total_loss: 1.193  loss_cls: 0.7059  loss_box_reg: 0.4966  time: 1.0869  data_time: 0.0026  lr: 0.01  max_mem: 21699M
[32m[11/10 08:45:14 d2.utils.events]: [0m eta: 1 day, 0:20:31  iter: 8939  total_loss: 1.139  loss_cls: 0.6761  loss_box_reg: 0.4703  time: 1.0869  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:45:37 d2.utils.events]: [0m eta: 1 day, 0:20:42  iter: 8959  total_loss: 1.125  loss_cls: 0.6768  loss_box_reg: 0.4518  time: 1.0870  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:45:59 d2.utils.events]: [0m eta: 1 day, 0:24:14  iter: 8979  total_loss: 1.155  loss_cls: 0.6811  loss_box_reg: 0.4682  time: 1.0871  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:46:21 d2.utils.events]: [0m eta: 1 day, 0:19:59  iter: 8999  total_loss: 1.153  loss_cls: 0.6984  loss_box_reg: 0.4737  time: 1.0871  data_time: 0.0033  lr: 0.01  max_mem: 21699M
[32m[11/10 08:46:43 d2.utils.events]: [0m eta: 1 day, 0:23:55  iter: 9019  total_loss: 1.102  loss_cls: 0.6671  loss_box_reg: 0.4403  time: 1.0871  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:47:04 d2.utils.events]: [0m eta: 1 day, 0:22:36  iter: 9039  total_loss: 1.227  loss_cls: 0.7351  loss_box_reg: 0.5031  time: 1.0870  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:47:27 d2.utils.events]: [0m eta: 1 day, 0:22:47  iter: 9059  total_loss: 1.085  loss_cls: 0.6484  loss_box_reg: 0.4368  time: 1.0872  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:47:50 d2.utils.events]: [0m eta: 1 day, 0:23:06  iter: 9079  total_loss: 1.171  loss_cls: 0.7136  loss_box_reg: 0.4666  time: 1.0873  data_time: 0.0116  lr: 0.01  max_mem: 21699M
[32m[11/10 08:48:12 d2.utils.events]: [0m eta: 1 day, 0:24:00  iter: 9099  total_loss: 1.13  loss_cls: 0.6745  loss_box_reg: 0.4541  time: 1.0873  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 08:48:35 d2.utils.events]: [0m eta: 1 day, 0:28:16  iter: 9119  total_loss: 1.184  loss_cls: 0.7107  loss_box_reg: 0.4636  time: 1.0874  data_time: 0.0050  lr: 0.01  max_mem: 21699M
[32m[11/10 08:48:56 d2.utils.events]: [0m eta: 1 day, 0:28:35  iter: 9139  total_loss: 1.091  loss_cls: 0.6457  loss_box_reg: 0.4417  time: 1.0873  data_time: 0.0038  lr: 0.01  max_mem: 21699M
[32m[11/10 08:49:19 d2.utils.events]: [0m eta: 1 day, 0:28:13  iter: 9159  total_loss: 1.167  loss_cls: 0.7009  loss_box_reg: 0.472  time: 1.0875  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:49:41 d2.utils.events]: [0m eta: 1 day, 0:27:26  iter: 9179  total_loss: 1.205  loss_cls: 0.7283  loss_box_reg: 0.4753  time: 1.0875  data_time: 0.0051  lr: 0.01  max_mem: 21699M
[32m[11/10 08:50:03 d2.utils.events]: [0m eta: 1 day, 0:26:49  iter: 9199  total_loss: 1.194  loss_cls: 0.7127  loss_box_reg: 0.4837  time: 1.0876  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:50:25 d2.utils.events]: [0m eta: 1 day, 0:24:34  iter: 9219  total_loss: 1.14  loss_cls: 0.6681  loss_box_reg: 0.4776  time: 1.0876  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:50:47 d2.utils.events]: [0m eta: 1 day, 0:22:14  iter: 9239  total_loss: 1.112  loss_cls: 0.6708  loss_box_reg: 0.446  time: 1.0876  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:51:10 d2.utils.events]: [0m eta: 1 day, 0:23:50  iter: 9259  total_loss: 1.222  loss_cls: 0.7459  loss_box_reg: 0.4822  time: 1.0877  data_time: 0.0052  lr: 0.01  max_mem: 21699M
[32m[11/10 08:51:33 d2.utils.events]: [0m eta: 1 day, 0:24:00  iter: 9279  total_loss: 1.206  loss_cls: 0.7408  loss_box_reg: 0.4925  time: 1.0879  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:51:55 d2.utils.events]: [0m eta: 1 day, 0:24:27  iter: 9299  total_loss: 1.093  loss_cls: 0.6581  loss_box_reg: 0.4249  time: 1.0878  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:52:17 d2.utils.events]: [0m eta: 1 day, 0:24:38  iter: 9319  total_loss: 1.15  loss_cls: 0.6657  loss_box_reg: 0.4845  time: 1.0879  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 08:52:39 d2.utils.events]: [0m eta: 1 day, 0:24:16  iter: 9339  total_loss: 1.156  loss_cls: 0.6897  loss_box_reg: 0.4739  time: 1.0879  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:53:01 d2.utils.events]: [0m eta: 1 day, 0:23:59  iter: 9359  total_loss: 1.108  loss_cls: 0.6634  loss_box_reg: 0.4466  time: 1.0879  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:53:24 d2.utils.events]: [0m eta: 1 day, 0:27:23  iter: 9379  total_loss: 1.268  loss_cls: 0.7566  loss_box_reg: 0.5173  time: 1.0880  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:53:46 d2.utils.events]: [0m eta: 1 day, 0:30:36  iter: 9399  total_loss: 1.15  loss_cls: 0.6834  loss_box_reg: 0.4704  time: 1.0880  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:54:10 d2.utils.events]: [0m eta: 1 day, 0:33:36  iter: 9419  total_loss: 1.162  loss_cls: 0.6977  loss_box_reg: 0.464  time: 1.0882  data_time: 0.0040  lr: 0.01  max_mem: 21699M
[32m[11/10 08:54:32 d2.utils.events]: [0m eta: 1 day, 0:30:58  iter: 9439  total_loss: 1.146  loss_cls: 0.6867  loss_box_reg: 0.4658  time: 1.0882  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:54:54 d2.utils.events]: [0m eta: 1 day, 0:27:38  iter: 9459  total_loss: 1.18  loss_cls: 0.7162  loss_box_reg: 0.4594  time: 1.0883  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:55:16 d2.utils.events]: [0m eta: 1 day, 0:23:39  iter: 9479  total_loss: 1.156  loss_cls: 0.6853  loss_box_reg: 0.4533  time: 1.0883  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:55:38 d2.utils.events]: [0m eta: 1 day, 0:24:28  iter: 9499  total_loss: 1.186  loss_cls: 0.7243  loss_box_reg: 0.4551  time: 1.0884  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:56:00 d2.utils.events]: [0m eta: 1 day, 0:22:55  iter: 9519  total_loss: 1.097  loss_cls: 0.6699  loss_box_reg: 0.4465  time: 1.0884  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:56:24 d2.utils.events]: [0m eta: 1 day, 0:22:33  iter: 9539  total_loss: 1.189  loss_cls: 0.7184  loss_box_reg: 0.472  time: 1.0886  data_time: 0.0035  lr: 0.01  max_mem: 21699M
[32m[11/10 08:56:46 d2.utils.events]: [0m eta: 1 day, 0:21:08  iter: 9559  total_loss: 1.14  loss_cls: 0.6902  loss_box_reg: 0.4554  time: 1.0885  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 08:57:08 d2.utils.events]: [0m eta: 1 day, 0:20:46  iter: 9579  total_loss: 1.127  loss_cls: 0.6774  loss_box_reg: 0.4538  time: 1.0886  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 08:57:30 d2.utils.events]: [0m eta: 1 day, 0:21:28  iter: 9599  total_loss: 1.133  loss_cls: 0.682  loss_box_reg: 0.4699  time: 1.0886  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 08:57:51 d2.utils.events]: [0m eta: 1 day, 0:19:12  iter: 9619  total_loss: 1.115  loss_cls: 0.6714  loss_box_reg: 0.4457  time: 1.0886  data_time: 0.0026  lr: 0.01  max_mem: 21699M
[32m[11/10 08:58:14 d2.utils.events]: [0m eta: 1 day, 0:18:54  iter: 9639  total_loss: 1.147  loss_cls: 0.6919  loss_box_reg: 0.4609  time: 1.0887  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:58:37 d2.utils.events]: [0m eta: 1 day, 0:18:32  iter: 9659  total_loss: 1.169  loss_cls: 0.7001  loss_box_reg: 0.4791  time: 1.0888  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 08:58:59 d2.utils.events]: [0m eta: 1 day, 0:23:37  iter: 9679  total_loss: 1.195  loss_cls: 0.707  loss_box_reg: 0.4709  time: 1.0888  data_time: 0.0046  lr: 0.01  max_mem: 21699M
[32m[11/10 08:59:21 d2.utils.events]: [0m eta: 1 day, 0:19:39  iter: 9699  total_loss: 1.05  loss_cls: 0.6255  loss_box_reg: 0.4124  time: 1.0888  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 08:59:43 d2.utils.events]: [0m eta: 1 day, 0:18:07  iter: 9719  total_loss: 1.251  loss_cls: 0.7374  loss_box_reg: 0.5148  time: 1.0888  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:00:04 d2.utils.events]: [0m eta: 1 day, 0:16:31  iter: 9739  total_loss: 1.151  loss_cls: 0.6798  loss_box_reg: 0.4597  time: 1.0887  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:00:26 d2.utils.events]: [0m eta: 1 day, 0:17:01  iter: 9759  total_loss: 1.178  loss_cls: 0.7015  loss_box_reg: 0.4709  time: 1.0888  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:00:48 d2.utils.events]: [0m eta: 1 day, 0:13:55  iter: 9779  total_loss: 1.121  loss_cls: 0.6757  loss_box_reg: 0.4629  time: 1.0888  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:01:10 d2.utils.events]: [0m eta: 1 day, 0:14:03  iter: 9799  total_loss: 1.15  loss_cls: 0.6829  loss_box_reg: 0.4715  time: 1.0888  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:01:32 d2.utils.events]: [0m eta: 1 day, 0:12:55  iter: 9819  total_loss: 1.137  loss_cls: 0.6833  loss_box_reg: 0.4475  time: 1.0888  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:01:54 d2.utils.events]: [0m eta: 1 day, 0:09:20  iter: 9839  total_loss: 1.168  loss_cls: 0.695  loss_box_reg: 0.4615  time: 1.0888  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:02:16 d2.utils.events]: [0m eta: 1 day, 0:09:59  iter: 9859  total_loss: 1.097  loss_cls: 0.6707  loss_box_reg: 0.436  time: 1.0889  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:02:38 d2.utils.events]: [0m eta: 1 day, 0:11:50  iter: 9879  total_loss: 1.118  loss_cls: 0.6635  loss_box_reg: 0.4475  time: 1.0889  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:03:00 d2.utils.events]: [0m eta: 1 day, 0:05:53  iter: 9899  total_loss: 1.166  loss_cls: 0.6983  loss_box_reg: 0.4742  time: 1.0888  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:03:22 d2.utils.events]: [0m eta: 1 day, 0:06:58  iter: 9919  total_loss: 1.193  loss_cls: 0.7133  loss_box_reg: 0.4656  time: 1.0889  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:03:44 d2.utils.events]: [0m eta: 1 day, 0:07:59  iter: 9939  total_loss: 1.122  loss_cls: 0.6678  loss_box_reg: 0.4594  time: 1.0889  data_time: 0.0049  lr: 0.01  max_mem: 21699M
[32m[11/10 09:04:07 d2.utils.events]: [0m eta: 1 day, 0:10:23  iter: 9959  total_loss: 1.124  loss_cls: 0.6847  loss_box_reg: 0.445  time: 1.0890  data_time: 0.0028  lr: 0.01  max_mem: 21699M
[32m[11/10 09:04:27 d2.utils.events]: [0m eta: 1 day, 0:07:16  iter: 9979  total_loss: 1.15  loss_cls: 0.6919  loss_box_reg: 0.4677  time: 1.0889  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:04:49 fvcore.common.checkpoint]: [0mSaving checkpoint to ./results/VG/retinanet/retinanet_r50_fpn_VG_concepts_cat_depth0/model_0009999.pth
[32m[11/10 09:04:51 d2.utils.events]: [0m eta: 1 day, 0:05:12  iter: 9999  total_loss: 1.245  loss_cls: 0.7448  loss_box_reg: 0.5071  time: 1.0889  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:05:12 d2.utils.events]: [0m eta: 1 day, 0:03:09  iter: 10019  total_loss: 1.162  loss_cls: 0.6842  loss_box_reg: 0.4806  time: 1.0889  data_time: 0.0047  lr: 0.01  max_mem: 21699M
[32m[11/10 09:05:34 d2.utils.events]: [0m eta: 1 day, 0:03:50  iter: 10039  total_loss: 1.182  loss_cls: 0.7077  loss_box_reg: 0.4756  time: 1.0888  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:05:57 d2.utils.events]: [0m eta: 1 day, 0:02:43  iter: 10059  total_loss: 1.109  loss_cls: 0.6714  loss_box_reg: 0.4496  time: 1.0889  data_time: 0.0025  lr: 0.01  max_mem: 21699M
[32m[11/10 09:06:19 d2.utils.events]: [0m eta: 1 day, 0:00:29  iter: 10079  total_loss: 1.174  loss_cls: 0.6873  loss_box_reg: 0.4629  time: 1.0890  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:06:41 d2.utils.events]: [0m eta: 1 day, 0:00:07  iter: 10099  total_loss: 1.17  loss_cls: 0.6832  loss_box_reg: 0.4832  time: 1.0890  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:07:04 d2.utils.events]: [0m eta: 1 day, 0:00:40  iter: 10119  total_loss: 1.126  loss_cls: 0.653  loss_box_reg: 0.471  time: 1.0891  data_time: 0.0040  lr: 0.01  max_mem: 21699M
[32m[11/10 09:07:25 d2.utils.events]: [0m eta: 23:59:24  iter: 10139  total_loss: 1.173  loss_cls: 0.694  loss_box_reg: 0.4675  time: 1.0891  data_time: 0.0043  lr: 0.01  max_mem: 21699M
[32m[11/10 09:07:47 d2.utils.events]: [0m eta: 23:56:55  iter: 10159  total_loss: 1.062  loss_cls: 0.6493  loss_box_reg: 0.4466  time: 1.0890  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:08:10 d2.utils.events]: [0m eta: 23:56:58  iter: 10179  total_loss: 1.1  loss_cls: 0.6605  loss_box_reg: 0.4419  time: 1.0891  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:08:31 d2.utils.events]: [0m eta: 23:55:45  iter: 10199  total_loss: 1.195  loss_cls: 0.7207  loss_box_reg: 0.4909  time: 1.0891  data_time: 0.0059  lr: 0.01  max_mem: 21699M
[32m[11/10 09:08:53 d2.utils.events]: [0m eta: 23:55:23  iter: 10219  total_loss: 1.12  loss_cls: 0.6721  loss_box_reg: 0.4504  time: 1.0891  data_time: 0.0042  lr: 0.01  max_mem: 21699M
[32m[11/10 09:09:16 d2.utils.events]: [0m eta: 23:59:11  iter: 10239  total_loss: 1.142  loss_cls: 0.6753  loss_box_reg: 0.4559  time: 1.0891  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:09:37 d2.utils.events]: [0m eta: 23:54:40  iter: 10259  total_loss: 1.153  loss_cls: 0.6912  loss_box_reg: 0.4592  time: 1.0891  data_time: 0.0045  lr: 0.01  max_mem: 21699M
[32m[11/10 09:09:59 d2.utils.events]: [0m eta: 23:54:02  iter: 10279  total_loss: 1.181  loss_cls: 0.6946  loss_box_reg: 0.4776  time: 1.0891  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 09:10:21 d2.utils.events]: [0m eta: 23:50:40  iter: 10299  total_loss: 1.173  loss_cls: 0.7048  loss_box_reg: 0.4693  time: 1.0891  data_time: 0.0025  lr: 0.01  max_mem: 21699M
[32m[11/10 09:10:42 d2.utils.events]: [0m eta: 23:50:00  iter: 10319  total_loss: 1.182  loss_cls: 0.6994  loss_box_reg: 0.4831  time: 1.0891  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:11:05 d2.utils.events]: [0m eta: 23:49:10  iter: 10339  total_loss: 1.15  loss_cls: 0.6909  loss_box_reg: 0.4502  time: 1.0891  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:11:26 d2.utils.events]: [0m eta: 23:46:04  iter: 10359  total_loss: 1.17  loss_cls: 0.7044  loss_box_reg: 0.4912  time: 1.0890  data_time: 0.0028  lr: 0.01  max_mem: 21699M
[32m[11/10 09:11:48 d2.utils.events]: [0m eta: 23:44:16  iter: 10379  total_loss: 1.153  loss_cls: 0.6841  loss_box_reg: 0.4731  time: 1.0891  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:12:09 d2.utils.events]: [0m eta: 23:39:02  iter: 10399  total_loss: 1.068  loss_cls: 0.6357  loss_box_reg: 0.443  time: 1.0890  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:12:32 d2.utils.events]: [0m eta: 23:40:48  iter: 10419  total_loss: 1.193  loss_cls: 0.7034  loss_box_reg: 0.4818  time: 1.0890  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:12:53 d2.utils.events]: [0m eta: 23:46:48  iter: 10439  total_loss: 1.09  loss_cls: 0.6516  loss_box_reg: 0.4306  time: 1.0890  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:13:17 d2.utils.events]: [0m eta: 23:46:26  iter: 10459  total_loss: 1.13  loss_cls: 0.6744  loss_box_reg: 0.4491  time: 1.0892  data_time: 0.0042  lr: 0.01  max_mem: 21699M
[32m[11/10 09:13:40 d2.utils.events]: [0m eta: 23:47:23  iter: 10479  total_loss: 1.158  loss_cls: 0.7017  loss_box_reg: 0.4693  time: 1.0893  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:14:02 d2.utils.events]: [0m eta: 23:46:43  iter: 10499  total_loss: 1.141  loss_cls: 0.6732  loss_box_reg: 0.4529  time: 1.0893  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:14:24 d2.utils.events]: [0m eta: 23:45:56  iter: 10519  total_loss: 1.221  loss_cls: 0.6981  loss_box_reg: 0.4942  time: 1.0893  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:14:45 d2.utils.events]: [0m eta: 23:41:34  iter: 10539  total_loss: 1.191  loss_cls: 0.6911  loss_box_reg: 0.4954  time: 1.0893  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:15:07 d2.utils.events]: [0m eta: 23:45:21  iter: 10559  total_loss: 1.11  loss_cls: 0.6683  loss_box_reg: 0.4447  time: 1.0893  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:15:30 d2.utils.events]: [0m eta: 23:45:35  iter: 10579  total_loss: 1.091  loss_cls: 0.6561  loss_box_reg: 0.4434  time: 1.0893  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:15:53 d2.utils.events]: [0m eta: 23:45:13  iter: 10599  total_loss: 1.246  loss_cls: 0.7375  loss_box_reg: 0.5171  time: 1.0894  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:16:13 d2.utils.events]: [0m eta: 23:44:31  iter: 10619  total_loss: 1.155  loss_cls: 0.6922  loss_box_reg: 0.4603  time: 1.0893  data_time: 0.0038  lr: 0.01  max_mem: 21699M
[32m[11/10 09:16:36 d2.utils.events]: [0m eta: 23:44:34  iter: 10639  total_loss: 1.117  loss_cls: 0.6561  loss_box_reg: 0.4512  time: 1.0894  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:16:58 d2.utils.events]: [0m eta: 23:44:09  iter: 10659  total_loss: 1.255  loss_cls: 0.7335  loss_box_reg: 0.5097  time: 1.0894  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 09:17:22 d2.utils.events]: [0m eta: 23:43:47  iter: 10679  total_loss: 1.101  loss_cls: 0.6685  loss_box_reg: 0.4554  time: 1.0896  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:17:43 d2.utils.events]: [0m eta: 23:40:37  iter: 10699  total_loss: 1.183  loss_cls: 0.7069  loss_box_reg: 0.4655  time: 1.0896  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:18:05 d2.utils.events]: [0m eta: 23:40:15  iter: 10719  total_loss: 1.157  loss_cls: 0.6935  loss_box_reg: 0.4567  time: 1.0895  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:18:26 d2.utils.events]: [0m eta: 23:37:59  iter: 10739  total_loss: 1.118  loss_cls: 0.6665  loss_box_reg: 0.4578  time: 1.0894  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:18:49 d2.utils.events]: [0m eta: 23:41:04  iter: 10759  total_loss: 1.077  loss_cls: 0.6322  loss_box_reg: 0.436  time: 1.0895  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:19:11 d2.utils.events]: [0m eta: 23:42:00  iter: 10779  total_loss: 1.157  loss_cls: 0.6619  loss_box_reg: 0.4879  time: 1.0895  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 09:19:33 d2.utils.events]: [0m eta: 23:41:49  iter: 10799  total_loss: 1.129  loss_cls: 0.6896  loss_box_reg: 0.4379  time: 1.0896  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:19:55 d2.utils.events]: [0m eta: 23:41:20  iter: 10819  total_loss: 1.118  loss_cls: 0.6781  loss_box_reg: 0.4344  time: 1.0896  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 09:20:17 d2.utils.events]: [0m eta: 23:42:09  iter: 10839  total_loss: 1.172  loss_cls: 0.6816  loss_box_reg: 0.4742  time: 1.0896  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:20:38 d2.utils.events]: [0m eta: 23:40:59  iter: 10859  total_loss: 1.142  loss_cls: 0.6756  loss_box_reg: 0.4589  time: 1.0895  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:21:02 d2.utils.events]: [0m eta: 23:41:25  iter: 10879  total_loss: 1.152  loss_cls: 0.6791  loss_box_reg: 0.4627  time: 1.0896  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 09:21:24 d2.utils.events]: [0m eta: 23:42:06  iter: 10899  total_loss: 1.193  loss_cls: 0.689  loss_box_reg: 0.4638  time: 1.0897  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:21:47 d2.utils.events]: [0m eta: 23:42:31  iter: 10919  total_loss: 1.103  loss_cls: 0.6512  loss_box_reg: 0.4255  time: 1.0898  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:22:09 d2.utils.events]: [0m eta: 23:40:19  iter: 10939  total_loss: 1.242  loss_cls: 0.747  loss_box_reg: 0.4966  time: 1.0898  data_time: 0.0045  lr: 0.01  max_mem: 21699M
[32m[11/10 09:22:32 d2.utils.events]: [0m eta: 23:39:11  iter: 10959  total_loss: 1.175  loss_cls: 0.714  loss_box_reg: 0.4594  time: 1.0899  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:22:54 d2.utils.events]: [0m eta: 23:39:36  iter: 10979  total_loss: 1.203  loss_cls: 0.7262  loss_box_reg: 0.4844  time: 1.0899  data_time: 0.0027  lr: 0.01  max_mem: 21699M
[32m[11/10 09:23:16 d2.utils.events]: [0m eta: 23:40:34  iter: 10999  total_loss: 1.175  loss_cls: 0.6937  loss_box_reg: 0.4573  time: 1.0899  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:23:38 d2.utils.events]: [0m eta: 23:41:50  iter: 11019  total_loss: 1.109  loss_cls: 0.6646  loss_box_reg: 0.4436  time: 1.0899  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:23:58 d2.utils.events]: [0m eta: 23:38:02  iter: 11039  total_loss: 1.205  loss_cls: 0.7218  loss_box_reg: 0.4705  time: 1.0897  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:24:21 d2.utils.events]: [0m eta: 23:41:07  iter: 11059  total_loss: 1.158  loss_cls: 0.6908  loss_box_reg: 0.4551  time: 1.0898  data_time: 0.0048  lr: 0.01  max_mem: 21699M
[32m[11/10 09:24:42 d2.utils.events]: [0m eta: 23:41:40  iter: 11079  total_loss: 1.117  loss_cls: 0.651  loss_box_reg: 0.4495  time: 1.0898  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:25:05 d2.utils.events]: [0m eta: 23:41:50  iter: 11099  total_loss: 1.134  loss_cls: 0.6813  loss_box_reg: 0.4499  time: 1.0898  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:25:26 d2.utils.events]: [0m eta: 23:37:05  iter: 11119  total_loss: 1.106  loss_cls: 0.6703  loss_box_reg: 0.4411  time: 1.0897  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:25:47 d2.utils.events]: [0m eta: 23:36:15  iter: 11139  total_loss: 1.2  loss_cls: 0.7098  loss_box_reg: 0.4924  time: 1.0897  data_time: 0.0042  lr: 0.01  max_mem: 21699M
[32m[11/10 09:26:09 d2.utils.events]: [0m eta: 23:39:11  iter: 11159  total_loss: 1.109  loss_cls: 0.6517  loss_box_reg: 0.4548  time: 1.0897  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:26:32 d2.utils.events]: [0m eta: 23:39:52  iter: 11179  total_loss: 1.114  loss_cls: 0.657  loss_box_reg: 0.4568  time: 1.0898  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:26:54 d2.utils.events]: [0m eta: 23:39:06  iter: 11199  total_loss: 1.148  loss_cls: 0.6911  loss_box_reg: 0.4639  time: 1.0898  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:27:16 d2.utils.events]: [0m eta: 23:40:07  iter: 11219  total_loss: 1.163  loss_cls: 0.6962  loss_box_reg: 0.4709  time: 1.0899  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:27:39 d2.utils.events]: [0m eta: 23:38:04  iter: 11239  total_loss: 1.123  loss_cls: 0.6756  loss_box_reg: 0.4467  time: 1.0899  data_time: 0.0032  lr: 0.01  max_mem: 21699M
[32m[11/10 09:28:02 d2.utils.events]: [0m eta: 23:39:24  iter: 11259  total_loss: 1.117  loss_cls: 0.6637  loss_box_reg: 0.4606  time: 1.0900  data_time: 0.0050  lr: 0.01  max_mem: 21699M
[32m[11/10 09:28:23 d2.utils.events]: [0m eta: 23:38:35  iter: 11279  total_loss: 1.143  loss_cls: 0.688  loss_box_reg: 0.4536  time: 1.0899  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:28:46 d2.utils.events]: [0m eta: 23:41:36  iter: 11299  total_loss: 1.159  loss_cls: 0.6904  loss_box_reg: 0.4573  time: 1.0900  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:29:06 d2.utils.events]: [0m eta: 23:41:15  iter: 11319  total_loss: 1.16  loss_cls: 0.6952  loss_box_reg: 0.4662  time: 1.0898  data_time: 0.0029  lr: 0.01  max_mem: 21699M
[32m[11/10 09:29:29 d2.utils.events]: [0m eta: 23:43:08  iter: 11339  total_loss: 1.1  loss_cls: 0.669  loss_box_reg: 0.4309  time: 1.0900  data_time: 0.0040  lr: 0.01  max_mem: 21699M
[32m[11/10 09:29:53 d2.utils.events]: [0m eta: 23:47:24  iter: 11359  total_loss: 1.131  loss_cls: 0.6712  loss_box_reg: 0.4611  time: 1.0901  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:30:14 d2.utils.events]: [0m eta: 23:46:40  iter: 11379  total_loss: 1.187  loss_cls: 0.715  loss_box_reg: 0.467  time: 1.0900  data_time: 0.0064  lr: 0.01  max_mem: 21699M
[32m[11/10 09:30:36 d2.utils.events]: [0m eta: 23:46:18  iter: 11399  total_loss: 1.133  loss_cls: 0.6829  loss_box_reg: 0.4434  time: 1.0900  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:30:57 d2.utils.events]: [0m eta: 23:41:41  iter: 11419  total_loss: 1.181  loss_cls: 0.7034  loss_box_reg: 0.4685  time: 1.0900  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:31:19 d2.utils.events]: [0m eta: 23:40:00  iter: 11439  total_loss: 1.165  loss_cls: 0.6849  loss_box_reg: 0.48  time: 1.0899  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:31:42 d2.utils.events]: [0m eta: 23:38:34  iter: 11459  total_loss: 1.207  loss_cls: 0.7177  loss_box_reg: 0.4759  time: 1.0900  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:32:04 d2.utils.events]: [0m eta: 23:38:12  iter: 11479  total_loss: 1.119  loss_cls: 0.6522  loss_box_reg: 0.4471  time: 1.0901  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:32:25 d2.utils.events]: [0m eta: 23:35:04  iter: 11499  total_loss: 1.1  loss_cls: 0.653  loss_box_reg: 0.4361  time: 1.0900  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:32:47 d2.utils.events]: [0m eta: 23:37:11  iter: 11519  total_loss: 1.11  loss_cls: 0.6571  loss_box_reg: 0.4553  time: 1.0900  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 09:33:09 d2.utils.events]: [0m eta: 23:34:48  iter: 11539  total_loss: 1.11  loss_cls: 0.6707  loss_box_reg: 0.439  time: 1.0900  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:33:31 d2.utils.events]: [0m eta: 23:33:01  iter: 11559  total_loss: 1.205  loss_cls: 0.7345  loss_box_reg: 0.4738  time: 1.0900  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:33:53 d2.utils.events]: [0m eta: 23:28:53  iter: 11579  total_loss: 1.139  loss_cls: 0.6741  loss_box_reg: 0.461  time: 1.0900  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:34:16 d2.utils.events]: [0m eta: 23:29:47  iter: 11599  total_loss: 1.15  loss_cls: 0.7007  loss_box_reg: 0.4681  time: 1.0901  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:34:38 d2.utils.events]: [0m eta: 23:31:56  iter: 11619  total_loss: 1.15  loss_cls: 0.6876  loss_box_reg: 0.471  time: 1.0901  data_time: 0.0065  lr: 0.01  max_mem: 21699M
[32m[11/10 09:34:59 d2.utils.events]: [0m eta: 23:27:48  iter: 11639  total_loss: 1.105  loss_cls: 0.6668  loss_box_reg: 0.4526  time: 1.0900  data_time: 0.0033  lr: 0.01  max_mem: 21699M
[32m[11/10 09:35:20 d2.utils.events]: [0m eta: 23:25:18  iter: 11659  total_loss: 1.177  loss_cls: 0.7099  loss_box_reg: 0.473  time: 1.0900  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:35:43 d2.utils.events]: [0m eta: 23:21:02  iter: 11679  total_loss: 1.179  loss_cls: 0.7025  loss_box_reg: 0.47  time: 1.0901  data_time: 0.0048  lr: 0.01  max_mem: 21699M
[32m[11/10 09:36:04 d2.utils.events]: [0m eta: 23:21:58  iter: 11699  total_loss: 1.144  loss_cls: 0.6782  loss_box_reg: 0.4597  time: 1.0900  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:36:28 d2.utils.events]: [0m eta: 23:27:38  iter: 11719  total_loss: 1.083  loss_cls: 0.6403  loss_box_reg: 0.4442  time: 1.0902  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:36:50 d2.utils.events]: [0m eta: 23:29:46  iter: 11739  total_loss: 1.083  loss_cls: 0.6453  loss_box_reg: 0.4533  time: 1.0902  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:37:12 d2.utils.events]: [0m eta: 23:29:00  iter: 11759  total_loss: 1.131  loss_cls: 0.6701  loss_box_reg: 0.4705  time: 1.0902  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:37:34 d2.utils.events]: [0m eta: 23:27:52  iter: 11779  total_loss: 1.171  loss_cls: 0.6834  loss_box_reg: 0.4903  time: 1.0901  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:37:56 d2.utils.events]: [0m eta: 23:25:27  iter: 11799  total_loss: 1.11  loss_cls: 0.6601  loss_box_reg: 0.4437  time: 1.0902  data_time: 0.0029  lr: 0.01  max_mem: 21699M
[32m[11/10 09:38:19 d2.utils.events]: [0m eta: 23:25:21  iter: 11819  total_loss: 1.161  loss_cls: 0.696  loss_box_reg: 0.4729  time: 1.0903  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:38:42 d2.utils.events]: [0m eta: 23:25:00  iter: 11839  total_loss: 1.091  loss_cls: 0.6409  loss_box_reg: 0.452  time: 1.0903  data_time: 0.0049  lr: 0.01  max_mem: 21699M
[32m[11/10 09:39:04 d2.utils.events]: [0m eta: 23:25:24  iter: 11859  total_loss: 1.106  loss_cls: 0.6683  loss_box_reg: 0.4492  time: 1.0904  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:39:26 d2.utils.events]: [0m eta: 23:24:17  iter: 11879  total_loss: 1.084  loss_cls: 0.6409  loss_box_reg: 0.4452  time: 1.0904  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:39:48 d2.utils.events]: [0m eta: 23:24:40  iter: 11899  total_loss: 1.113  loss_cls: 0.6596  loss_box_reg: 0.4354  time: 1.0904  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:40:10 d2.utils.events]: [0m eta: 23:25:20  iter: 11919  total_loss: 1.142  loss_cls: 0.6806  loss_box_reg: 0.4612  time: 1.0904  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:40:33 d2.utils.events]: [0m eta: 23:24:59  iter: 11939  total_loss: 1.094  loss_cls: 0.665  loss_box_reg: 0.4235  time: 1.0905  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:40:56 d2.utils.events]: [0m eta: 23:23:36  iter: 11959  total_loss: 1.103  loss_cls: 0.662  loss_box_reg: 0.452  time: 1.0906  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:41:17 d2.utils.events]: [0m eta: 23:24:16  iter: 11979  total_loss: 1.192  loss_cls: 0.7066  loss_box_reg: 0.4709  time: 1.0906  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:41:40 d2.utils.events]: [0m eta: 23:22:53  iter: 11999  total_loss: 1.132  loss_cls: 0.671  loss_box_reg: 0.4572  time: 1.0906  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:42:02 d2.utils.events]: [0m eta: 23:22:31  iter: 12019  total_loss: 1.162  loss_cls: 0.6989  loss_box_reg: 0.4693  time: 1.0906  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:42:24 d2.utils.events]: [0m eta: 23:23:39  iter: 12039  total_loss: 1.119  loss_cls: 0.667  loss_box_reg: 0.4549  time: 1.0906  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:42:45 d2.utils.events]: [0m eta: 23:20:07  iter: 12059  total_loss: 1.16  loss_cls: 0.6809  loss_box_reg: 0.471  time: 1.0906  data_time: 0.0035  lr: 0.01  max_mem: 21699M
[32m[11/10 09:43:07 d2.utils.events]: [0m eta: 23:18:31  iter: 12079  total_loss: 1.115  loss_cls: 0.6721  loss_box_reg: 0.4454  time: 1.0906  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:43:30 d2.utils.events]: [0m eta: 23:16:42  iter: 12099  total_loss: 1.159  loss_cls: 0.6959  loss_box_reg: 0.4545  time: 1.0906  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:43:52 d2.utils.events]: [0m eta: 23:18:00  iter: 12119  total_loss: 1.131  loss_cls: 0.6623  loss_box_reg: 0.462  time: 1.0907  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 09:44:14 d2.utils.events]: [0m eta: 23:15:59  iter: 12139  total_loss: 1.119  loss_cls: 0.6751  loss_box_reg: 0.4478  time: 1.0907  data_time: 0.0029  lr: 0.01  max_mem: 21699M
[32m[11/10 09:44:36 d2.utils.events]: [0m eta: 23:14:47  iter: 12159  total_loss: 1.119  loss_cls: 0.6564  loss_box_reg: 0.4583  time: 1.0907  data_time: 0.0033  lr: 0.01  max_mem: 21699M
[32m[11/10 09:44:57 d2.utils.events]: [0m eta: 23:08:58  iter: 12179  total_loss: 1.158  loss_cls: 0.6844  loss_box_reg: 0.4748  time: 1.0906  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:45:19 d2.utils.events]: [0m eta: 23:10:17  iter: 12199  total_loss: 1.035  loss_cls: 0.6156  loss_box_reg: 0.3992  time: 1.0906  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:45:41 d2.utils.events]: [0m eta: 23:08:16  iter: 12219  total_loss: 1.173  loss_cls: 0.6953  loss_box_reg: 0.4731  time: 1.0906  data_time: 0.0040  lr: 0.01  max_mem: 21699M
[32m[11/10 09:46:03 d2.utils.events]: [0m eta: 23:11:22  iter: 12239  total_loss: 1.117  loss_cls: 0.6776  loss_box_reg: 0.4685  time: 1.0907  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 09:46:25 d2.utils.events]: [0m eta: 23:08:07  iter: 12259  total_loss: 1.126  loss_cls: 0.6676  loss_box_reg: 0.4648  time: 1.0906  data_time: 0.0039  lr: 0.01  max_mem: 21699M
[32m[11/10 09:46:47 d2.utils.events]: [0m eta: 23:07:11  iter: 12279  total_loss: 1.127  loss_cls: 0.6601  loss_box_reg: 0.4715  time: 1.0906  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:47:08 d2.utils.events]: [0m eta: 23:05:38  iter: 12299  total_loss: 1.083  loss_cls: 0.65  loss_box_reg: 0.4416  time: 1.0906  data_time: 0.0090  lr: 0.01  max_mem: 21699M
[32m[11/10 09:47:31 d2.utils.events]: [0m eta: 23:08:08  iter: 12319  total_loss: 1.165  loss_cls: 0.7086  loss_box_reg: 0.4672  time: 1.0906  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:47:53 d2.utils.events]: [0m eta: 23:05:22  iter: 12339  total_loss: 1.119  loss_cls: 0.6694  loss_box_reg: 0.4446  time: 1.0906  data_time: 0.0059  lr: 0.01  max_mem: 21699M
[32m[11/10 09:48:16 d2.utils.events]: [0m eta: 23:05:00  iter: 12359  total_loss: 1.045  loss_cls: 0.6297  loss_box_reg: 0.4186  time: 1.0907  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 09:48:39 d2.utils.events]: [0m eta: 23:08:52  iter: 12379  total_loss: 1.129  loss_cls: 0.6644  loss_box_reg: 0.4582  time: 1.0909  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:49:00 d2.utils.events]: [0m eta: 23:06:38  iter: 12399  total_loss: 1.122  loss_cls: 0.6379  loss_box_reg: 0.4782  time: 1.0908  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:49:24 d2.utils.events]: [0m eta: 23:09:54  iter: 12419  total_loss: 1.099  loss_cls: 0.6468  loss_box_reg: 0.4474  time: 1.0909  data_time: 0.0052  lr: 0.01  max_mem: 21699M
[32m[11/10 09:49:47 d2.utils.events]: [0m eta: 23:13:17  iter: 12439  total_loss: 1.101  loss_cls: 0.6591  loss_box_reg: 0.4468  time: 1.0910  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:50:09 d2.utils.events]: [0m eta: 23:11:48  iter: 12459  total_loss: 1.18  loss_cls: 0.6923  loss_box_reg: 0.4812  time: 1.0910  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:50:31 d2.utils.events]: [0m eta: 23:11:27  iter: 12479  total_loss: 1.057  loss_cls: 0.6173  loss_box_reg: 0.4276  time: 1.0911  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:50:54 d2.utils.events]: [0m eta: 23:12:48  iter: 12499  total_loss: 1.148  loss_cls: 0.6904  loss_box_reg: 0.4679  time: 1.0911  data_time: 0.0044  lr: 0.01  max_mem: 21699M
[32m[11/10 09:51:16 d2.utils.events]: [0m eta: 23:12:48  iter: 12519  total_loss: 1.079  loss_cls: 0.6278  loss_box_reg: 0.449  time: 1.0911  data_time: 0.0024  lr: 0.01  max_mem: 21699M
[32m[11/10 09:51:37 d2.utils.events]: [0m eta: 23:14:00  iter: 12539  total_loss: 1.19  loss_cls: 0.7066  loss_box_reg: 0.4817  time: 1.0911  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 09:52:02 d2.utils.events]: [0m eta: 23:16:41  iter: 12559  total_loss: 1.145  loss_cls: 0.6685  loss_box_reg: 0.4615  time: 1.0913  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 09:52:24 d2.utils.events]: [0m eta: 23:21:21  iter: 12579  total_loss: 1.193  loss_cls: 0.7154  loss_box_reg: 0.4774  time: 1.0913  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:52:47 d2.utils.events]: [0m eta: 23:21:01  iter: 12599  total_loss: 1.15  loss_cls: 0.6565  loss_box_reg: 0.4756  time: 1.0914  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:53:11 d2.utils.events]: [0m eta: 23:20:38  iter: 12619  total_loss: 1.101  loss_cls: 0.6318  loss_box_reg: 0.4537  time: 1.0915  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:53:33 d2.utils.events]: [0m eta: 23:21:17  iter: 12639  total_loss: 1.146  loss_cls: 0.6735  loss_box_reg: 0.4705  time: 1.0915  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:53:57 d2.utils.events]: [0m eta: 23:25:09  iter: 12659  total_loss: 1.128  loss_cls: 0.6773  loss_box_reg: 0.4484  time: 1.0917  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:54:18 d2.utils.events]: [0m eta: 23:23:56  iter: 12679  total_loss: 1.069  loss_cls: 0.6471  loss_box_reg: 0.4354  time: 1.0916  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:54:41 d2.utils.events]: [0m eta: 23:24:46  iter: 12699  total_loss: 1.157  loss_cls: 0.6765  loss_box_reg: 0.4752  time: 1.0917  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:55:04 d2.utils.events]: [0m eta: 23:24:04  iter: 12719  total_loss: 1.09  loss_cls: 0.6451  loss_box_reg: 0.4569  time: 1.0918  data_time: 0.0053  lr: 0.01  max_mem: 21699M
[32m[11/10 09:55:29 d2.utils.events]: [0m eta: 23:24:16  iter: 12739  total_loss: 1.135  loss_cls: 0.6771  loss_box_reg: 0.4576  time: 1.0920  data_time: 0.0074  lr: 0.01  max_mem: 21699M
[32m[11/10 09:55:52 d2.utils.events]: [0m eta: 23:23:54  iter: 12759  total_loss: 1.051  loss_cls: 0.6292  loss_box_reg: 0.4295  time: 1.0921  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:56:15 d2.utils.events]: [0m eta: 23:27:35  iter: 12779  total_loss: 1.136  loss_cls: 0.6726  loss_box_reg: 0.4829  time: 1.0922  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:56:38 d2.utils.events]: [0m eta: 23:28:56  iter: 12799  total_loss: 1.118  loss_cls: 0.6623  loss_box_reg: 0.4656  time: 1.0923  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:57:02 d2.utils.events]: [0m eta: 23:30:56  iter: 12819  total_loss: 1.069  loss_cls: 0.6411  loss_box_reg: 0.437  time: 1.0924  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:57:25 d2.utils.events]: [0m eta: 23:28:31  iter: 12839  total_loss: 1.121  loss_cls: 0.6564  loss_box_reg: 0.4685  time: 1.0925  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 09:57:46 d2.utils.events]: [0m eta: 23:26:07  iter: 12859  total_loss: 1.093  loss_cls: 0.6629  loss_box_reg: 0.4362  time: 1.0925  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:58:09 d2.utils.events]: [0m eta: 23:25:02  iter: 12879  total_loss: 1.141  loss_cls: 0.6724  loss_box_reg: 0.4682  time: 1.0926  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 09:58:31 d2.utils.events]: [0m eta: 23:24:00  iter: 12899  total_loss: 1.182  loss_cls: 0.6936  loss_box_reg: 0.4825  time: 1.0925  data_time: 0.0043  lr: 0.01  max_mem: 21699M
[32m[11/10 09:58:54 d2.utils.events]: [0m eta: 23:23:38  iter: 12919  total_loss: 1.014  loss_cls: 0.5996  loss_box_reg: 0.4124  time: 1.0926  data_time: 0.0034  lr: 0.01  max_mem: 21699M
[32m[11/10 09:59:17 d2.utils.events]: [0m eta: 23:23:57  iter: 12939  total_loss: 1.143  loss_cls: 0.6752  loss_box_reg: 0.4647  time: 1.0927  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 09:59:40 d2.utils.events]: [0m eta: 23:25:22  iter: 12959  total_loss: 1.107  loss_cls: 0.6587  loss_box_reg: 0.4537  time: 1.0927  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:00:02 d2.utils.events]: [0m eta: 23:23:13  iter: 12979  total_loss: 1.156  loss_cls: 0.6792  loss_box_reg: 0.4498  time: 1.0928  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:00:25 d2.utils.events]: [0m eta: 23:25:36  iter: 12999  total_loss: 1.177  loss_cls: 0.6923  loss_box_reg: 0.4796  time: 1.0929  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:00:47 d2.utils.events]: [0m eta: 23:25:01  iter: 13019  total_loss: 1.126  loss_cls: 0.6589  loss_box_reg: 0.4522  time: 1.0929  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:01:10 d2.utils.events]: [0m eta: 23:24:39  iter: 13039  total_loss: 1.153  loss_cls: 0.6613  loss_box_reg: 0.4801  time: 1.0929  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 10:01:33 d2.utils.events]: [0m eta: 23:27:37  iter: 13059  total_loss: 1.178  loss_cls: 0.6784  loss_box_reg: 0.4984  time: 1.0930  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:01:56 d2.utils.events]: [0m eta: 23:28:54  iter: 13079  total_loss: 1.147  loss_cls: 0.6652  loss_box_reg: 0.4838  time: 1.0931  data_time: 0.0042  lr: 0.01  max_mem: 21699M
[32m[11/10 10:02:17 d2.utils.events]: [0m eta: 23:25:49  iter: 13099  total_loss: 1.131  loss_cls: 0.6896  loss_box_reg: 0.4537  time: 1.0930  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 10:02:39 d2.utils.events]: [0m eta: 23:26:31  iter: 13119  total_loss: 1.109  loss_cls: 0.66  loss_box_reg: 0.4625  time: 1.0931  data_time: 0.0025  lr: 0.01  max_mem: 21699M
[32m[11/10 10:03:03 d2.utils.events]: [0m eta: 23:31:36  iter: 13139  total_loss: 1.117  loss_cls: 0.635  loss_box_reg: 0.4555  time: 1.0932  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 10:03:26 d2.utils.events]: [0m eta: 23:36:09  iter: 13159  total_loss: 1.097  loss_cls: 0.6471  loss_box_reg: 0.4541  time: 1.0933  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:03:49 d2.utils.events]: [0m eta: 23:36:28  iter: 13179  total_loss: 1.108  loss_cls: 0.6745  loss_box_reg: 0.4448  time: 1.0934  data_time: 0.0020  lr: 0.01  max_mem: 21699M
[32m[11/10 10:04:11 d2.utils.events]: [0m eta: 23:39:31  iter: 13199  total_loss: 1.098  loss_cls: 0.6684  loss_box_reg: 0.4356  time: 1.0934  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:04:34 d2.utils.events]: [0m eta: 23:39:20  iter: 13219  total_loss: 1.172  loss_cls: 0.6831  loss_box_reg: 0.4745  time: 1.0934  data_time: 0.0023  lr: 0.01  max_mem: 21699M
[32m[11/10 10:04:56 d2.utils.events]: [0m eta: 23:39:17  iter: 13239  total_loss: 1.121  loss_cls: 0.6624  loss_box_reg: 0.4668  time: 1.0934  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 10:05:18 d2.utils.events]: [0m eta: 23:42:49  iter: 13259  total_loss: 1.131  loss_cls: 0.6864  loss_box_reg: 0.4449  time: 1.0934  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:05:40 d2.utils.events]: [0m eta: 23:44:21  iter: 13279  total_loss: 1.134  loss_cls: 0.6597  loss_box_reg: 0.4683  time: 1.0935  data_time: 0.0032  lr: 0.01  max_mem: 21699M
[32m[11/10 10:06:03 d2.utils.events]: [0m eta: 23:47:21  iter: 13299  total_loss: 1.114  loss_cls: 0.6392  loss_box_reg: 0.436  time: 1.0935  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 10:06:25 d2.utils.events]: [0m eta: 23:42:50  iter: 13319  total_loss: 1.134  loss_cls: 0.6591  loss_box_reg: 0.4613  time: 1.0935  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:06:48 d2.utils.events]: [0m eta: 23:46:46  iter: 13339  total_loss: 1.059  loss_cls: 0.6224  loss_box_reg: 0.428  time: 1.0936  data_time: 0.0047  lr: 0.01  max_mem: 21699M
[32m[11/10 10:07:11 d2.utils.events]: [0m eta: 23:42:52  iter: 13359  total_loss: 1.098  loss_cls: 0.6409  loss_box_reg: 0.4622  time: 1.0937  data_time: 0.0033  lr: 0.01  max_mem: 21699M
[32m[11/10 10:07:34 d2.utils.events]: [0m eta: 23:40:36  iter: 13379  total_loss: 1.117  loss_cls: 0.6564  loss_box_reg: 0.4575  time: 1.0937  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:07:57 d2.utils.events]: [0m eta: 23:41:48  iter: 13399  total_loss: 1.17  loss_cls: 0.7015  loss_box_reg: 0.4719  time: 1.0938  data_time: 0.0036  lr: 0.01  max_mem: 21699M
[32m[11/10 10:08:20 d2.utils.events]: [0m eta: 23:41:45  iter: 13419  total_loss: 1.064  loss_cls: 0.6371  loss_box_reg: 0.427  time: 1.0939  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 10:08:44 d2.utils.events]: [0m eta: 23:40:00  iter: 13439  total_loss: 1.113  loss_cls: 0.6482  loss_box_reg: 0.4711  time: 1.0940  data_time: 0.0022  lr: 0.01  max_mem: 21699M
[32m[11/10 10:09:05 d2.utils.events]: [0m eta: 23:40:15  iter: 13459  total_loss: 1.109  loss_cls: 0.6625  loss_box_reg: 0.4481  time: 1.0940  data_time: 0.0021  lr: 0.01  max_mem: 21699M
[32m[11/10 10:09:28 d2.utils.events]: [0m eta: 23:39:52  iter: 13479  total_loss: 1.172  loss_cls: 0.6839  loss_box_reg: 0.4911  time: 1.0941  data_time: 0.0020  lr: 0.01  max_mem: 21699M
slurmstepd: error: *** JOB 49601820 ON gn22 CANCELLED AT 2022-11-10T10:09:41 ***
slurmstepd: error: *** STEP 49601820.0 ON gn22 CANCELLED AT 2022-11-10T10:09:41 ***
