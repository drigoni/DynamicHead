
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 
Date:  mer 19 ott 2022, 17.26.39, CEST
Directory:  /ceph/hpc/home/eudavider/repository/DynamicHead
Nodelist:  gn23
Number of nodes:  1
Ntasks per node:  1
NGPUs per node:  4
CUDA_VISIBLE_DEVICES:  0,1,2,3
TORCH_DEVICE_COUNT:  4
SLURM_MASTER_PORT:  16488
SLURM_MASTER_NODE:  gn23
SLURM_MASTER_ADDR:  gn23
SLURM_MASTER_URL:  tcp://gn23:16488
--------------------------------------------- 
MODEL_NUM_GPUS:  4
MODEL_NUM_MACHINES:  1
MODEL_BATCH_SIZE:  16
MODEL_MAX_ITER:  90000
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 

srun: error: WARNING: Multiple leaf switches contain nodes: gn[01-60]
[nltk_data] Downloading package omw-1.4 to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /ceph/hpc/home/eudavider/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Command Line Args: Namespace(config_file='./configs/OID/retinanet/r50/retinanet_drigoni_r50_fpn_OID_train.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://gn23:16488', opts=['SOLVER.IMS_PER_BATCH', '16', 'SOLVER.MAX_ITER', '90000'])
Loading config ./configs/OID/retinanet/r50/../base_retinanet_OID.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/OID/retinanet/r50/../base_retinanet_OID.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/OID/retinanet/r50/../base_retinanet_OID.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Loading config ./configs/OID/retinanet/r50/../base_retinanet_OID.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
[32m[10/19 17:27:21 detectron2]: [0mRank of current process: 0. World size: 4
[32m[10/19 17:27:26 detectron2]: [0mEnvironment info:
----------------------  ------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) [GCC 10.3.0]
numpy                   1.23.1
detectron2              0.6 @/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.3
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.0 @/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA A100-SXM4-40GB (arch=8.0)
Driver version          510.47.03
CUDA_HOME               /usr/local/cuda
Pillow                  9.2.0
torchvision             0.11.0 @/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     Not found
----------------------  ------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[32m[10/19 17:27:26 detectron2]: [0mCommand line arguments: Namespace(config_file='./configs/OID/retinanet/r50/retinanet_drigoni_r50_fpn_OID_train.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://gn23:16488', opts=['SOLVER.IMS_PER_BATCH', '16', 'SOLVER.MAX_ITER', '90000'])
[32m[10/19 17:27:26 detectron2]: [0mContents of args.config_file=./configs/OID/retinanet/r50/retinanet_drigoni_r50_fpn_OID_train.yaml:
_BASE_: "../base_retinanet_OID.yaml"
MODEL:
  META_ARCHITECTURE: "drigoniRetinaNet"
  WEIGHTS: "detectron2://ImageNetPretrained/MSRA/R-50.pkl"
  MASK_ON: False
  RESNETS:
    DEPTH: 50
OUTPUT_DIR: "./results/OID/retinanet/retinanet_drigoni_r50_fpn_OID/"
[32m[10/19 17:27:26 detectron2]: [0mRunning with full config:
CONCEPT:
  APPLY_CONDITION: true
  APPLY_CONDITION_FROM_FILE: false
  ACTIVATE_CONCEPT_GENERATOR: true
  CONCEPT_FUSION: cat
  DEPTH: 3
  EXTERNAL_CONCEPTS_FOLDER: ./datasets/ewiser_concepts_COCO_valid/
  FILE: ./concept/coco_to_synset.json
  ONLY_NAME: true
  UNIQUE: true
  VOCAB: ./concept/vocab.json
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 32
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - oid_v4_val
  TRAIN:
  - oid_v4_train
DEEPSETS:
  AGGREGATE: sum
  EMB: wordnet
  EMB_DIM: 150
  FILE: ./concept/wn30_holE_500_150_0.1_0.2_embeddings.pickle
  FREEZE: true
  MLP1_LAYERS:
  - 150
  - 150
  MLP1_OUTPUT_DIM: 150
  MLP2_LAYERS:
  - 150
  OUTPUT_DIM: 256
EVALUATOR_TYPE: default
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 40.31747359663594
      - 50.79683366298238
    - - 64
      - 80.63494719327188
      - 101.59366732596476
    - - 128
      - 161.26989438654377
      - 203.18733465192952
    - - 256
      - 322.53978877308754
      - 406.37466930385904
    - - 512
      - 645.0795775461751
      - 812.7493386077181
  ATSS:
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CHANNELS: 256
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    INFERENCE_TH: 0.05
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_TH: 0.6
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    REG_LOSS_WEIGHT: 2.0
    TOPK: 9
    USE_GN: true
  BACKBONE:
    FREEZE_AT: -1
    NAME: build_retinanet_resnet_fpn_backbone
  DEVICE: cuda
  DYHEAD:
    CHANNELS: 256
    NUM_CONVS: 6
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: drigoniRetinaNet
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 103.53
  - 116.28
  - 123.675
  PIXEL_STD:
  - 1.0
  - 1.0
  - 1.0
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 601
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.0
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 601
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWINT:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 96
    MLP_RATIO: 4
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - stage2
    - stage3
    - stage4
    - stage5
    OUT_NORM: true
    VERSION: 1
    WINDOW_SIZE: 7
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./results/OID/retinanet/retinanet_drigoni_r50_fpn_OID/
SEED: 2022
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: false
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 90000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: SGD
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 60000
  - 80000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[32m[10/19 17:27:26 detectron2]: [0mFull config saved to ./results/OID/retinanet/retinanet_drigoni_r50_fpn_OID/config.yaml
wandb: Currently logged in as: drigoni. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /ceph/hpc/scratch/user/eudavider/repository/DynamicHead/wandb/run-20221019_172729-2mctb4gm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sun-353
wandb: ⭐️ View project at https://wandb.ai/drigoni/CATSS
wandb: 🚀 View run at https://wandb.ai/drigoni/CATSS/runs/2mctb4gm
Load concept for each category. 
[32m[10/19 17:27:37 d2.engine.defaults]: [0mModel:
drigoniRetinaNet(
  (backbone): FPN(
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelP6P7(
      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (head): RetinaNetHead(
    (cls_subnet): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): ReLU()
    )
    (bbox_subnet): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): ReLU()
    )
    (cls_score): Conv2d(256, 5409, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (anchor_generator): DefaultAnchorGenerator(
    (cell_anchors): BufferList()
  )
)
[32m[10/19 17:29:05 d2.data.datasets.coco]: [0mLoading datasets/OpenImagesDataset/annotations/openimages_v4_train_bbox.json takes 88.34 seconds.
[32m[10/19 17:29:15 d2.data.datasets.coco]: [0mLoaded 1743042 images in COCO format from datasets/OpenImagesDataset/annotations/openimages_v4_train_bbox.json
[32m[10/19 17:30:22 d2.data.build]: [0mRemoved 0 images with no usable annotations. 1743042 images left.
[32m[10/19 17:31:31 d2.data.build]: [0mDistribution of instances among all 601 categories:
[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |
|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|
|   Tortoise    | 1998         |   Container   | 0            |    Magpie     | 145          |
|  Sea turtle   | 1132         |   Football    | 5097         |   Ambulance   | 447          |
|    Ladder     | 994          |  Toothbrush   | 219          |    Syringe    | 127          |
|     Sink      | 1648         |      Toy      | 70963        |     Organ     | 398          |
| Cassette deck | 74           |     Apple     | 3898         |   Human eye   | 77233        |
|   Cosmetics   | 2394         |    Paddle     | 6951         |    Snowman    | 770          |
|     Beer      | 9565         |  Chopsticks   | 617          |  Human beard  | 3157         |
|     Bird      | 47921        | Parking meter | 209          | Traffic light | 7426         |
|   Croissant   | 447          |   Cucumber    | 1194         |    Radish     | 688          |
|     Towel     | 338          |     Doll      | 6442         |     Skull     | 2661         |
| Washing mac.. | 655          |     Glove     | 1198         |     Tick      | 143          |
|     Belt      | 422          |  Sunglasses   | 23996        |     Banjo     | 264          |
|     Cart      | 2755         |     Ball      | 6845         |   Backpack    | 1216         |
|    Bicycle    | 40161        | Home applia.. | 2086         |   Centipede   | 280          |
|     Boat      | 79113        |   Surfboard   | 2594         |     Boot      | 3132         |
|  Headphones   | 1255         |    Hot dog    | 482          |    Shorts     | 16981        |
|   Fast food   | 24991        |      Bus      | 11927        |      Boy      | 87555        |
|  Screwdriver  | 85           | Bicycle wheel | 59521        |     Barge     | 983          |
|    Laptop     | 9327         |   Miniskirt   | 954          |     Drill     | 203          |
|     Dress     | 52999        |     Bear      | 427          |    Waffle     | 710          |
|    Pancake    | 775          |  Brown bear   | 647          |  Woodpecker   | 515          |
|   Blue jay    | 259          |    Pretzel    | 294          |     Bagel     | 640          |
|     Tower     | 67945        |    Teapot     | 632          |    Person     | 1034721      |
| Bow and arrow | 594          |   Swimwear    | 10079        |    Beehive    | 511          |
|   Brassiere   | 1694         |      Bee      | 11401        |      Bat      | 655          |
|   Starfish    | 639          |    Popcorn    | 362          |    Burrito    | 262          |
|   Chainsaw    | 130          |    Balloon    | 13505        |    Wrench     | 204          |
|     Tent      | 6907         | Vehicle reg.. | 7852         |    Lantern    | 5429         |
|    Toaster    | 73           |  Flashlight   | 88           |   Billboard   | 9823         |
|     Tiara     | 411          |   Limousine   | 366          |   Necklace    | 2735         |
|   Carnivore   | 3501         |   Scissors    | 399          |    Stairs     | 5981         |
| Computer ke.. | 4542         |    Printer    | 263          | Traffic sign  | 6112         |
|     Chair     | 132483       |     Shirt     | 7465         |    Poster     | 23566        |
|    Cheese     | 1560         |     Sock      | 1425         | Fire hydrant  | 432          |
| Land vehicle  | 81108        |   Earrings    | 1446         |      Tie      | 10545        |
|  Watercraft   | 5202         |   Cabinetry   | 9191         |   Suitcase    | 630          |
|    Muffin     | 4608         |     Bidet     | 440          |     Snack     | 37374        |
|  Snowmobile   | 366          |     Clock     | 1222         | Medical equ.. | 2060         |
|    Cattle     | 11603        |     Cello     | 2004         |    Jet ski    | 543          |
|     Camel     | 1340         |     Coat      | 6523         |     Suit      | 110848       |
|     Desk      | 11693        |      Cat      | 15183        | Bronze scul.. | 2748         |
|     Juice     | 2838         |    Gondola    | 1868         |    Beetle     | 3523         |
|    Cannon     | 1087         | Computer mo.. | 733          |    Cookie     | 4158         |
| Office buil.. | 4986         |   Fountain    | 3691         |     Coin      | 3042         |
|  Calculator   | 210          |   Cocktail    | 4458         | Computer mo.. | 6112         |
|      Box      | 5364         |    Stapler    | 59           | Christmas t.. | 4243         |
|  Cowboy hat   | 3068         | Hiking equi.. | 10505        | Studio couch  | 1889         |
|     Drum      | 24818        |    Dessert    | 27407        |   Wine rack   | 254          |
|     Drink     | 40323        |   Zucchini    | 1098         |     Ladle     | 54           |
|  Human mouth  | 44197        |     Dairy     | 8146         |     Dice      | 714          |
|     Oven      | 637          |   Dinosaur    | 1721         |    Ratchet    | 327          |
|     Couch     | 4259         | Cricket ball  | 132          | Winter melon  | 43           |
|    Spatula    | 64           |  Whiteboard   | 1003         | Pencil shar.. | 21           |
|     Door      | 19256        |      Hat      | 13245        |    Shower     | 235          |
|    Eraser     | 53           |    Fedora     | 3660         |   Guacamole   | 224          |
|    Dagger     | 370          |     Scarf     | 2303         |    Dolphin    | 1532         |
|   Sombrero    | 651          |    Tin can    | 2988         |      Mug      | 2272         |
|      Tap      | 1695         |  Harbor seal  | 2084         |   Stretcher   | 174          |
|  Can opener   | 7            |    Goggles    | 9636         |  Human body   | 175244       |
| Roller skates | 5476         |  Coffee cup   | 5327         | Cutting board | 213          |
|    Blender    | 235          | Plumbing fi.. | 481          |   Stop sign   | 394          |
| Office supp.. | 6198         |  Volleyball   | 661          |     Vase      | 2468         |
|  Slow cooker  | 125          |   Wardrobe    | 238          |    Coffee     | 2384         |
|     Whisk     | 180          |  Paper towel  | 210          | Personal care | 409          |
|     Food      | 88422        |    Sun hat    | 6979         |  Tree house   | 110          |
|  Flying disc  | 249          |     Skirt     | 1259         |   Gas stove   | 526          |
| Salt and pe.. | 180          | Mechanical .. | 681          |  Face powder  | 80           |
|      Fax      | 28           |     Fruit     | 26236        | French fries  | 2114         |
|  Nightstand   | 1125         |    Barrel     | 2086         |     Kite      | 1766         |
|     Tart      | 973          |   Treadmill   | 247          |      Fox      | 565          |
|     Flag      | 29246        |     Horn      | 1239         | Window blind  | 570          |
|  Human foot   | 2237         |   Golf cart   | 595          |    Jacket     | 25957        |
|      Egg      | 1865         | Street light  | 44697        |    Guitar     | 25896        |
|    Pillow     | 3508         |   Human leg   | 71479        |    Isopod     | 154          |
|     Grape     | 2787         |   Human ear   | 17774        | Power plugs.. | 290          |
|     Panda     | 882          |    Giraffe    | 1431         |     Woman     | 767337       |
|  Door handle  | 751          |  Rhinoceros   | 724          |    Bathtub    | 545          |
|   Goldfish    | 2204         |  Houseplant   | 22834        |     Goat      | 2075         |
| Baseball bat  | 1228         | Baseball gl.. | 2529         |  Mixing bowl  | 1005         |
| Marine inve.. | 9112         | Kitchen ute.. | 549          | Light switch  | 97           |
|     House     | 136152       |     Horse     | 13368        | Stationary .. | 338          |
|    Hammer     | 139          |  Ceiling fan  | 478          |   Sofa bed    | 1501         |
| Adhesive tape | 255          |     Harp      | 231          |    Sandal     | 2938         |
| Bicycle hel.. | 15952        |    Saucer     | 2819         |  Harpsichord  | 212          |
|  Human hair   | 234057       |    Heater     | 35           |   Harmonica   | 38           |
|    Hamster    | 546          |    Curtain    | 4872         |      Bed      | 3563         |
|    Kettle     | 657          |   Fireplace   | 711          |     Scale     | 139          |
| Drinking st.. | 292          |    Insect     | 8981         |  Hair dryer   | 27           |
|  Kitchenware  | 0            | Indoor rower  | 35           | Invertebrate  | 1568         |
| Food proces.. | 192          |   Bookcase    | 5307         | Refrigerator  | 592          |
| Wood-burnin.. | 300          | Punching bag  | 336          |  Common fig   | 317          |
| Cocktail sh.. | 27           |    Jaguar     | 586          |   Golf ball   | 434          |
| Fashion acc.. | 91024        |  Alarm clock  | 169          | Filing cabi.. | 374          |
|   Artichoke   | 376          |     Table     | 85691        |   Tableware   | 41086        |
|   Kangaroo    | 778          |     Koala     | 418          |     Knife     | 850          |
|    Bottle     | 40188        | Bottle opener | 21           |     Lynx      | 237          |
|   Lavender    | 6472         |  Lighthouse   | 1518         |   Dumbbell    | 413          |
|  Human head   | 201633       |     Bowl      | 4507         |  Humidifier   | 11           |
|     Porch     | 3854         |    Lizard     | 2120         | Billiard ta.. | 912          |
|    Mammal     | 156154       |     Mouse     | 857          |  Motorcycle   | 13382        |
| Musical ins.. | 16503        |   Swim cap    | 615          |  Frying pan   | 377          |
|   Snowplow    | 300          | Bathroom ca.. | 358          |    Missile    | 603          |
|     Bust      | 1060         |      Man      | 1418594      |  Waffle iron  | 31           |
|     Milk      | 214          |  Ring binder  | 84           |     Plate     | 5416         |
| Mobile phone  | 6365         |  Baked goods  | 23010        |   Mushroom    | 4497         |
|    Crutch     | 150          |    Pitcher    | 347          |    Mirror     | 1572         |
|  Lifejacket   | 3678         | Table tenni.. | 849          |  Pencil case  | 132          |
| Musical key.. | 1771         |  Scoreboard   | 517          |   Briefcase   | 162          |
| Kitchen knife | 350          |     Nail      | 491          |  Tennis ball  | 502          |
|  Plastic bag  | 986          |     Oboe      | 162          | Chest of dr.. | 1526         |
|    Ostrich    | 640          |     Piano     | 1374         |     Girl      | 197155       |
|     Plant     | 267913       |    Potato     | 599          |  Hair spray   | 10           |
| Sports equi.. | 44900        |     Pasta     | 769          |    Penguin    | 4197         |
|    Pumpkin    | 6150         |     Pear      | 923          |  Infant bed   | 462          |
|  Polar bear   | 664          |     Mixer     | 216          |   Cupboard    | 1353         |
|    Jacuzzi    | 103          |     Pizza     | 2008         | Digital clock | 199          |
|      Pig      | 1613         |    Reptile    | 578          |     Rifle     | 2540         |
|   Lipstick    | 1343         |  Skateboard   | 1810         |     Raven     | 567          |
|  High heels   | 3124         |   Red panda   | 423          |     Rose      | 12053        |
|    Rabbit     | 1641         |   Sculpture   | 34533        |   Saxophone   | 1208         |
|    Shotgun    | 580          |    Seafood    | 3063         | Submarine s.. | 273          |
|   Snowboard   | 944          |     Sword     | 567          | Picture frame | 11957        |
|     Sushi     | 2088         |   Loveseat    | 631          |      Ski      | 3505         |
|   Squirrel    | 1940         |    Tripod     | 1446         |  Stethoscope  | 78           |
|   Submarine   | 81           |   Scorpion    | 204          |    Segway     | 565          |
| Training be.. | 194          |     Snake     | 1378         | Coffee table  | 5314         |
|  Skyscraper   | 81261        |     Sheep     | 3438         |  Television   | 3789         |
|   Trombone    | 953          |      Tea      | 1342         |     Tank      | 1716         |
|     Taco      | 677          |   Telephone   | 274          |     Torch     | 20           |
|     Tiger     | 1260         |  Strawberry   | 7944         |    Trumpet    | 1546         |
|     Tree      | 1051344      |    Tomato     | 6254         |     Train     | 13050        |
|     Tool      | 2075         | Picnic basket | 425          | Cooking spray | 45           |
|   Trousers    | 8481         | Bowling equ.. | 3846         | Football he.. | 11705        |
|     Truck     | 12135        | Measuring cup | 74           |  Coffeemaker  | 323          |
|    Violin     | 2828         |    Vehicle    | 50959        |    Handbag    | 2495         |
| Paper cutter  | 4            |     Wine      | 15400        |    Weapon     | 2960         |
|     Wheel     | 340639       |     Worm      | 270          |      Wok      | 730          |
|     Whale     | 1014         |     Zebra     | 1120         |   Auto part   | 13586        |
|      Jug      | 590          | Pizza cutter  | 20           |     Cream     | 123          |
|    Monkey     | 3026         |     Lion      | 1653         |     Bread     | 3846         |
|    Platter    | 3462         |    Chicken    | 3059         |     Eagle     | 1704         |
|  Helicopter   | 3023         |      Owl      | 1663         |     Duck      | 15451        |
|    Turtle     | 205          | Hippopotamus  | 685          |   Crocodile   | 1069         |
|    Toilet     | 1099         | Toilet paper  | 377          |     Squid     | 221          |
|   Clothing    | 1438128      |   Footwear    | 744474       |     Lemon     | 1756         |
|    Spider     | 2033         |     Deer      | 3867         |     Frog      | 1608         |
|    Banana     | 1612         |    Rocket     | 918          |  Wine glass   | 12934        |
|  Countertop   | 3113         | Tablet comp.. | 975          | Waste conta.. | 1807         |
| Swimming pool | 3881         |      Dog      | 28675        |     Book      | 41280        |
|   Elephant    | 3272         |     Shark     | 625          |    Candle     | 5886         |
|    Leopard    | 811          |      Axe      | 148          |  Hand dryer   | 70           |
| Soap dispen.. | 78           |   Porcupine   | 229          |    Flower     | 345296       |
|    Canary     | 387          |    Cheetah    | 715          |   Palm tree   | 42026        |
|   Hamburger   | 1486         |     Maple     | 4923         |   Building    | 178634       |
|     Fish      | 23195        |    Lobster    | 597          |   Asparagus   | 387          |
|   Furniture   | 38527        |   Hedgehog    | 261          |   Airplane    | 21285        |
|     Spoon     | 1709         |     Otter     | 752          |     Bull      | 1736         |
|    Oyster     | 1038         | Horizontal .. | 75           | Convenience.. | 1817         |
|     Bomb      | 8            |     Bench     | 7229         |   Ice cream   | 2834         |
|  Caterpillar  | 884          |   Butterfly   | 10127        |   Parachute   | 2672         |
|    Orange     | 6195         |   Antelope    | 1568         |    Beaker     | 168          |
| Moths and b.. | 1857         |    Window     | 503467       |    Closet     | 661          |
|    Castle     | 4310         |   Jellyfish   | 2064         |     Goose     | 8436         |
|     Mule      | 1117         |     Swan      | 4523         |     Peach     | 756          |
|    Coconut    | 874          |   Seat belt   | 461          |    Raccoon    | 381          |
|    Chisel     | 33           |     Fork      | 1687         |     Lamp      | 3663         |
|    Camera     | 6404         |    Squash     | 375          |    Racket     | 281          |
|  Human face   | 1037710      |   Human arm   | 208982       |   Vegetable   | 18621        |
|    Diaper     | 152          |   Unicycle    | 194          |    Falcon     | 1717         |
|     Chime     | 41           |     Snail     | 943          |   Shellfish   | 1287         |
|    Cabbage    | 435          |    Carrot     | 1456         |     Mango     | 429          |
|     Jeans     | 78473        |   Flowerpot   | 22760        |   Pineapple   | 660          |
|    Drawer     | 4414         |     Stool     | 1254         |   Envelope    | 177          |
|     Cake      | 5784         |   Dragonfly   | 1490         |   Sunflower   | 6815         |
| Microwave o.. | 485          |   Honeycomb   | 383          | Marine mammal | 1746         |
|   Sea lion    | 1823         |    Ladybug    | 734          |     Shelf     | 22899        |
|     Watch     | 1903         |     Candy     | 2261         |     Salad     | 3088         |
|    Parrot     | 2388         |    Handgun    | 727          |    Sparrow    | 1651         |
|      Van      | 7720         |    Grinder    | 14           |  Spice rack   | 131          |
|  Light bulb   | 1816         | Corded phone  | 433          | Sports unif.. | 19396        |
| Tennis racket | 985          |  Wall clock   | 1067         | Serving tray  | 118          |
| Kitchen & d.. | 2127         |    Dog bed    | 266          |  Cake stand   | 337          |
| Cat furniture | 208          | Bathroom ac.. | 2678         | Facial tiss.. | 20           |
| Pressure co.. | 14           | Kitchen app.. | 4662         |     Tire      | 122615       |
|     Ruler     | 253          | Luggage and.. | 2220         |  Microphone   | 27272        |
|   Broccoli    | 1128         |   Umbrella    | 7204         |    Pastry     | 5852         |
|  Grapefruit   | 1283         |   Band-aid    | 36           |    Animal     | 17442        |
|  Bell pepper  | 802          |    Turkey     | 734          |     Lily      | 2252         |
|  Pomegranate  | 677          |   Doughnut    | 930          |    Glasses    | 57946        |
|  Human nose   | 60142        |      Pen      | 1705         |      Ant      | 925          |
|      Car      | 248075       |   Aircraft    | 1898         |  Human hand   | 75307        |
|     Skunk     | 56           |  Teddy bear   | 1587         |  Watermelon   | 844          |
|  Cantaloupe   | 166          |  Dishwasher   | 92           |     Flute     | 362          |
| Balance beam  | 326          |   Sandwich    | 1157         |    Shrimp     | 1856         |
| Sewing mach.. | 453          |  Binoculars   | 123          | Rays and sk.. | 485          |
|     Ipod      | 595          |   Accordion   | 955          |    Willow     | 735          |
|     Crab      | 1041         |     Crown     | 687          |   Seahorse    | 314          |
|    Perfume    | 363          |    Alpaca     | 829          |     Taxi      | 4199         |
|     Canoe     | 4543         | Remote cont.. | 236          |  Wheelchair   | 1464         |
|  Rugby ball   | 294          |   Armadillo   | 56           |    Maracas    | 10           |
|    Helmet     | 16502        |               |              |               |              |
|     total     | 14610229     |               |              |               |              |[0m
[32m[10/19 17:31:31 d2.data.common]: [0mSerializing 1743042 elements to byte tensors and concatenating them all ...
[32m[10/19 17:31:47 d2.data.common]: [0mSerialized dataset takes 1195.87 MiB
[32m[10/19 17:31:53 fvcore.common.checkpoint]: [0m[Checkpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[32m[10/19 17:31:53 d2.checkpoint.c2_model_loading]: [0mRenaming Caffe2 weights ......
[32m[10/19 17:31:53 d2.checkpoint.c2_model_loading]: [0mFollowing weights matched with submodule backbone.bottom_up:
| Names in Model    | Names in Checkpoint      | Shapes                                          |
|:------------------|:-------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*           | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                  | (64, 3, 7, 7)                                   |
Load concept for each category. 
[5m[31mWARNING[0m [32m[10/19 17:31:53 fvcore.common.checkpoint]: [0mSome model parameters or buffers are not found in the checkpoint:
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mbackbone.top_block.p6.{bias, weight}[0m
[34mbackbone.top_block.p7.{bias, weight}[0m
[34mhead.bbox_pred.{bias, weight}[0m
[34mhead.bbox_subnet.0.{bias, weight}[0m
[34mhead.bbox_subnet.2.{bias, weight}[0m
[34mhead.bbox_subnet.4.{bias, weight}[0m
[34mhead.bbox_subnet.6.{bias, weight}[0m
[34mhead.cls_score.{bias, weight}[0m
[34mhead.cls_subnet.0.{bias, weight}[0m
[34mhead.cls_subnet.2.{bias, weight}[0m
[34mhead.cls_subnet.4.{bias, weight}[0m
[34mhead.cls_subnet.6.{bias, weight}[0m
[5m[31mWARNING[0m [32m[10/19 17:31:53 fvcore.common.checkpoint]: [0mThe checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
  [35mstem.conv1.bias[0m
[32m[10/19 17:31:53 d2.engine.train_loop]: [0mStarting training from iteration 0
Load concept for each category. 
Load concept for each category. 
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  max_size = (max_size + (stride - 1)) // stride * stride
/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272164809/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[32m[10/19 17:34:51 d2.utils.events]: [0m eta: 1 day, 7:01:37  iter: 19  total_loss: 3.075  loss_cls: 2.175  loss_box_reg: 0.9417  time: 1.2983  data_time: 7.5680  lr: 0.00019981  max_mem: 32629M
[32m[10/19 17:35:09 d2.utils.events]: [0m eta: 1 day, 0:12:39  iter: 39  total_loss: 2.34  loss_cls: 1.639  loss_box_reg: 0.7004  time: 1.0825  data_time: 0.0160  lr: 0.00039961  max_mem: 32629M
[32m[10/19 17:35:24 d2.utils.events]: [0m eta: 20:42:18  iter: 59  total_loss: 1.998  loss_cls: 1.456  loss_box_reg: 0.5415  time: 0.9788  data_time: 0.0138  lr: 0.00059941  max_mem: 32629M
[32m[10/19 17:35:41 d2.utils.events]: [0m eta: 20:43:00  iter: 79  total_loss: 2.308  loss_cls: 1.643  loss_box_reg: 0.6601  time: 0.9437  data_time: 0.0131  lr: 0.00079921  max_mem: 32642M
[32m[10/19 17:35:58 d2.utils.events]: [0m eta: 20:27:20  iter: 99  total_loss: 2.174  loss_cls: 1.522  loss_box_reg: 0.621  time: 0.9151  data_time: 0.0105  lr: 0.00099901  max_mem: 32642M
[32m[10/19 17:36:14 d2.utils.events]: [0m eta: 20:15:58  iter: 119  total_loss: 2.393  loss_cls: 1.696  loss_box_reg: 0.6973  time: 0.8945  data_time: 0.0174  lr: 0.0011988  max_mem: 32642M
[32m[10/19 17:36:30 d2.utils.events]: [0m eta: 20:15:00  iter: 139  total_loss: 2.102  loss_cls: 1.494  loss_box_reg: 0.6084  time: 0.8815  data_time: 0.0186  lr: 0.0013986  max_mem: 32642M
[32m[10/19 17:36:46 d2.utils.events]: [0m eta: 20:05:31  iter: 159  total_loss: 2.055  loss_cls: 1.474  loss_box_reg: 0.5984  time: 0.8691  data_time: 0.0125  lr: 0.0015984  max_mem: 32645M
[32m[10/19 17:37:02 d2.utils.events]: [0m eta: 20:04:46  iter: 179  total_loss: 2.147  loss_cls: 1.535  loss_box_reg: 0.6214  time: 0.8622  data_time: 0.0191  lr: 0.0017982  max_mem: 32645M
[32m[10/19 17:37:18 d2.utils.events]: [0m eta: 20:02:04  iter: 199  total_loss: 2.096  loss_cls: 1.508  loss_box_reg: 0.6174  time: 0.8555  data_time: 0.0177  lr: 0.001998  max_mem: 32645M
[32m[10/19 17:37:34 d2.utils.events]: [0m eta: 19:59:30  iter: 219  total_loss: 2.163  loss_cls: 1.552  loss_box_reg: 0.6125  time: 0.8502  data_time: 0.0129  lr: 0.0021978  max_mem: 32645M
[32m[10/19 17:37:51 d2.utils.events]: [0m eta: 19:59:17  iter: 239  total_loss: 2.059  loss_cls: 1.456  loss_box_reg: 0.6017  time: 0.8478  data_time: 0.0151  lr: 0.0023976  max_mem: 32645M
[32m[10/19 17:38:07 d2.utils.events]: [0m eta: 19:58:35  iter: 259  total_loss: 2.065  loss_cls: 1.445  loss_box_reg: 0.6035  time: 0.8432  data_time: 0.0141  lr: 0.0025974  max_mem: 32645M
[32m[10/19 17:38:23 d2.utils.events]: [0m eta: 19:58:15  iter: 279  total_loss: 1.903  loss_cls: 1.34  loss_box_reg: 0.5754  time: 0.8415  data_time: 0.0228  lr: 0.0027972  max_mem: 32645M
[32m[10/19 17:38:39 d2.utils.events]: [0m eta: 19:56:20  iter: 299  total_loss: 1.741  loss_cls: 1.224  loss_box_reg: 0.5348  time: 0.8372  data_time: 0.0157  lr: 0.002997  max_mem: 32645M
[32m[10/19 17:38:55 d2.utils.events]: [0m eta: 19:56:36  iter: 319  total_loss: 1.894  loss_cls: 1.316  loss_box_reg: 0.5968  time: 0.8352  data_time: 0.0201  lr: 0.0031968  max_mem: 32645M
[32m[10/19 17:39:11 d2.utils.events]: [0m eta: 19:55:43  iter: 339  total_loss: 1.749  loss_cls: 1.183  loss_box_reg: 0.5812  time: 0.8323  data_time: 0.0130  lr: 0.0033966  max_mem: 32645M
[32m[10/19 17:39:27 d2.utils.events]: [0m eta: 19:56:04  iter: 359  total_loss: 1.855  loss_cls: 1.179  loss_box_reg: 0.6557  time: 0.8309  data_time: 0.0146  lr: 0.0035964  max_mem: 32645M
[32m[10/19 17:39:43 d2.utils.events]: [0m eta: 19:55:11  iter: 379  total_loss: 1.782  loss_cls: 1.129  loss_box_reg: 0.6398  time: 0.8288  data_time: 0.0175  lr: 0.0037962  max_mem: 32645M
[32m[10/19 17:39:59 d2.utils.events]: [0m eta: 19:54:50  iter: 399  total_loss: 1.716  loss_cls: 1.113  loss_box_reg: 0.5769  time: 0.8267  data_time: 0.0133  lr: 0.003996  max_mem: 32645M
[32m[10/19 17:40:15 d2.utils.events]: [0m eta: 19:53:15  iter: 419  total_loss: 1.655  loss_cls: 1.061  loss_box_reg: 0.6011  time: 0.8244  data_time: 0.0157  lr: 0.0041958  max_mem: 32645M
[32m[10/19 17:40:31 d2.utils.events]: [0m eta: 19:52:59  iter: 439  total_loss: 1.438  loss_cls: 0.8588  loss_box_reg: 0.5447  time: 0.8231  data_time: 0.0161  lr: 0.0043956  max_mem: 32645M
[32m[10/19 17:40:47 d2.utils.events]: [0m eta: 19:51:59  iter: 459  total_loss: 1.498  loss_cls: 0.9774  loss_box_reg: 0.5648  time: 0.8220  data_time: 0.0084  lr: 0.0045954  max_mem: 32647M
[32m[10/19 17:41:03 d2.utils.events]: [0m eta: 19:53:14  iter: 479  total_loss: 1.547  loss_cls: 0.9526  loss_box_reg: 0.5941  time: 0.8214  data_time: 0.0181  lr: 0.0047952  max_mem: 32647M
[32m[10/19 17:41:19 d2.utils.events]: [0m eta: 19:52:52  iter: 499  total_loss: 1.488  loss_cls: 0.8914  loss_box_reg: 0.5707  time: 0.8206  data_time: 0.0129  lr: 0.004995  max_mem: 32647M
[32m[10/19 17:41:35 d2.utils.events]: [0m eta: 19:52:36  iter: 519  total_loss: 1.401  loss_cls: 0.8742  loss_box_reg: 0.5267  time: 0.8195  data_time: 0.0124  lr: 0.0051948  max_mem: 32647M
[32m[10/19 17:41:51 d2.utils.events]: [0m eta: 19:52:26  iter: 539  total_loss: 1.359  loss_cls: 0.8704  loss_box_reg: 0.4975  time: 0.8189  data_time: 0.0216  lr: 0.0053946  max_mem: 32647M
[32m[10/19 17:42:07 d2.utils.events]: [0m eta: 19:52:19  iter: 559  total_loss: 1.353  loss_cls: 0.8211  loss_box_reg: 0.5293  time: 0.8183  data_time: 0.0111  lr: 0.0055944  max_mem: 32647M
[32m[10/19 17:42:23 d2.utils.events]: [0m eta: 19:51:11  iter: 579  total_loss: 1.462  loss_cls: 0.8862  loss_box_reg: 0.5649  time: 0.8173  data_time: 0.0105  lr: 0.0057942  max_mem: 32647M
[32m[10/19 17:42:39 d2.utils.events]: [0m eta: 19:49:18  iter: 599  total_loss: 1.21  loss_cls: 0.7589  loss_box_reg: 0.4582  time: 0.8161  data_time: 0.0213  lr: 0.005994  max_mem: 32647M
[32m[10/19 17:42:55 d2.utils.events]: [0m eta: 19:47:56  iter: 619  total_loss: 1.462  loss_cls: 0.8991  loss_box_reg: 0.552  time: 0.8153  data_time: 0.0172  lr: 0.0061938  max_mem: 32647M
[32m[10/19 17:43:11 d2.utils.events]: [0m eta: 19:48:46  iter: 639  total_loss: 1.24  loss_cls: 0.7622  loss_box_reg: 0.4778  time: 0.8152  data_time: 0.0128  lr: 0.0063936  max_mem: 32647M
[32m[10/19 17:43:28 d2.utils.events]: [0m eta: 19:46:46  iter: 659  total_loss: 1.229  loss_cls: 0.7712  loss_box_reg: 0.4715  time: 0.8150  data_time: 0.0101  lr: 0.0065934  max_mem: 32647M
[32m[10/19 17:43:44 d2.utils.events]: [0m eta: 19:46:30  iter: 679  total_loss: 1.453  loss_cls: 0.8665  loss_box_reg: 0.5594  time: 0.8143  data_time: 0.0160  lr: 0.0067932  max_mem: 32647M
[32m[10/19 17:44:00 d2.utils.events]: [0m eta: 19:47:59  iter: 699  total_loss: 1.401  loss_cls: 0.8619  loss_box_reg: 0.5234  time: 0.8141  data_time: 0.0171  lr: 0.006993  max_mem: 32647M
[32m[10/19 17:44:16 d2.utils.events]: [0m eta: 19:48:54  iter: 719  total_loss: 1.517  loss_cls: 1.034  loss_box_reg: 0.5027  time: 0.8141  data_time: 0.0148  lr: 0.0071928  max_mem: 32647M
[32m[10/19 17:44:32 d2.utils.events]: [0m eta: 19:49:03  iter: 739  total_loss: 1.525  loss_cls: 1.023  loss_box_reg: 0.514  time: 0.8136  data_time: 0.0132  lr: 0.0073926  max_mem: 32647M
[32m[10/19 17:44:48 d2.utils.events]: [0m eta: 19:49:08  iter: 759  total_loss: 1.513  loss_cls: 1.003  loss_box_reg: 0.5242  time: 0.8133  data_time: 0.0244  lr: 0.0075924  max_mem: 32647M
[32m[10/19 17:45:05 d2.utils.events]: [0m eta: 19:48:23  iter: 779  total_loss: 1.44  loss_cls: 0.9075  loss_box_reg: 0.5187  time: 0.8130  data_time: 0.0123  lr: 0.0077922  max_mem: 32647M
[32m[10/19 17:45:21 d2.utils.events]: [0m eta: 19:47:50  iter: 799  total_loss: 1.252  loss_cls: 0.7861  loss_box_reg: 0.4913  time: 0.8127  data_time: 0.0159  lr: 0.007992  max_mem: 32647M
[32m[10/19 17:45:37 d2.utils.events]: [0m eta: 19:47:51  iter: 819  total_loss: 1.239  loss_cls: 0.7799  loss_box_reg: 0.4439  time: 0.8123  data_time: 0.0233  lr: 0.0081918  max_mem: 32647M
[32m[10/19 17:45:53 d2.utils.events]: [0m eta: 19:47:35  iter: 839  total_loss: 1.193  loss_cls: 0.7732  loss_box_reg: 0.4375  time: 0.8119  data_time: 0.0206  lr: 0.0083916  max_mem: 32647M
[32m[10/19 17:46:09 d2.utils.events]: [0m eta: 19:47:08  iter: 859  total_loss: 1.429  loss_cls: 0.9498  loss_box_reg: 0.4669  time: 0.8114  data_time: 0.0152  lr: 0.0085914  max_mem: 32647M
[32m[10/19 17:46:25 d2.utils.events]: [0m eta: 19:46:39  iter: 879  total_loss: 1.763  loss_cls: 1.201  loss_box_reg: 0.5612  time: 0.8111  data_time: 0.0158  lr: 0.0087912  max_mem: 32647M
[32m[10/19 17:46:41 d2.utils.events]: [0m eta: 19:46:47  iter: 899  total_loss: 1.491  loss_cls: 0.962  loss_box_reg: 0.5044  time: 0.8111  data_time: 0.0240  lr: 0.008991  max_mem: 32647M
[32m[10/19 17:46:57 d2.utils.events]: [0m eta: 19:46:31  iter: 919  total_loss: 1.695  loss_cls: 1.164  loss_box_reg: 0.5429  time: 0.8108  data_time: 0.0179  lr: 0.0091908  max_mem: 32647M
[32m[10/19 17:47:14 d2.utils.events]: [0m eta: 19:46:44  iter: 939  total_loss: 1.311  loss_cls: 0.8705  loss_box_reg: 0.4325  time: 0.8108  data_time: 0.0133  lr: 0.0093906  max_mem: 32652M
[32m[10/19 17:47:30 d2.utils.events]: [0m eta: 19:46:46  iter: 959  total_loss: 1.475  loss_cls: 0.9148  loss_box_reg: 0.5159  time: 0.8107  data_time: 0.0142  lr: 0.0095904  max_mem: 32652M
[32m[10/19 17:47:47 d2.utils.events]: [0m eta: 19:46:59  iter: 979  total_loss: 1.273  loss_cls: 0.8158  loss_box_reg: 0.4673  time: 0.8111  data_time: 0.0140  lr: 0.0097902  max_mem: 32652M
[32m[10/19 17:48:03 d2.utils.events]: [0m eta: 19:46:25  iter: 999  total_loss: 1.444  loss_cls: 0.9484  loss_box_reg: 0.5036  time: 0.8108  data_time: 0.0200  lr: 0.00999  max_mem: 32652M
[32m[10/19 17:48:19 d2.utils.events]: [0m eta: 19:45:11  iter: 1019  total_loss: 1.677  loss_cls: 1.155  loss_box_reg: 0.5235  time: 0.8105  data_time: 0.0094  lr: 0.01  max_mem: 32652M
[32m[10/19 17:48:35 d2.utils.events]: [0m eta: 19:44:45  iter: 1039  total_loss: 1.332  loss_cls: 0.9146  loss_box_reg: 0.4308  time: 0.8102  data_time: 0.0117  lr: 0.01  max_mem: 32652M
[32m[10/19 17:48:51 d2.utils.events]: [0m eta: 19:45:08  iter: 1059  total_loss: 1.297  loss_cls: 0.8734  loss_box_reg: 0.4361  time: 0.8098  data_time: 0.0146  lr: 0.01  max_mem: 32652M
[32m[10/19 17:49:07 d2.utils.events]: [0m eta: 19:44:06  iter: 1079  total_loss: 1.466  loss_cls: 0.9211  loss_box_reg: 0.5238  time: 0.8095  data_time: 0.0185  lr: 0.01  max_mem: 32652M
[32m[10/19 17:49:23 d2.utils.events]: [0m eta: 19:43:20  iter: 1099  total_loss: 1.309  loss_cls: 0.8479  loss_box_reg: 0.4603  time: 0.8094  data_time: 0.0162  lr: 0.01  max_mem: 32652M
[32m[10/19 17:49:39 d2.utils.events]: [0m eta: 19:42:57  iter: 1119  total_loss: 1.44  loss_cls: 0.9402  loss_box_reg: 0.4856  time: 0.8093  data_time: 0.0138  lr: 0.01  max_mem: 32652M
[32m[10/19 17:49:55 d2.utils.events]: [0m eta: 19:42:49  iter: 1139  total_loss: 1.305  loss_cls: 0.8512  loss_box_reg: 0.4369  time: 0.8093  data_time: 0.0152  lr: 0.01  max_mem: 32652M
[32m[10/19 17:50:12 d2.utils.events]: [0m eta: 19:43:02  iter: 1159  total_loss: 1.189  loss_cls: 0.7668  loss_box_reg: 0.4317  time: 0.8092  data_time: 0.0173  lr: 0.01  max_mem: 32652M
[32m[10/19 17:50:27 d2.utils.events]: [0m eta: 19:42:32  iter: 1179  total_loss: 1.387  loss_cls: 0.8893  loss_box_reg: 0.4927  time: 0.8088  data_time: 0.0133  lr: 0.01  max_mem: 32652M
[32m[10/19 17:50:44 d2.utils.events]: [0m eta: 19:42:44  iter: 1199  total_loss: 1.197  loss_cls: 0.7928  loss_box_reg: 0.4124  time: 0.8088  data_time: 0.0174  lr: 0.01  max_mem: 32652M
[32m[10/19 17:51:00 d2.utils.events]: [0m eta: 19:42:35  iter: 1219  total_loss: 1.335  loss_cls: 0.8648  loss_box_reg: 0.4654  time: 0.8086  data_time: 0.0169  lr: 0.01  max_mem: 32652M
[32m[10/19 17:51:16 d2.utils.events]: [0m eta: 19:42:45  iter: 1239  total_loss: 1.235  loss_cls: 0.7823  loss_box_reg: 0.4369  time: 0.8088  data_time: 0.0246  lr: 0.01  max_mem: 32652M
[32m[10/19 17:51:32 d2.utils.events]: [0m eta: 19:42:51  iter: 1259  total_loss: 1.575  loss_cls: 0.9828  loss_box_reg: 0.5327  time: 0.8087  data_time: 0.0128  lr: 0.01  max_mem: 32652M
[32m[10/19 17:51:48 d2.utils.events]: [0m eta: 19:42:24  iter: 1279  total_loss: 1.324  loss_cls: 0.811  loss_box_reg: 0.4995  time: 0.8084  data_time: 0.0130  lr: 0.01  max_mem: 32652M
[32m[10/19 17:52:04 d2.utils.events]: [0m eta: 19:42:25  iter: 1299  total_loss: 1.223  loss_cls: 0.7482  loss_box_reg: 0.4611  time: 0.8083  data_time: 0.0187  lr: 0.01  max_mem: 32652M
[32m[10/19 17:52:20 d2.utils.events]: [0m eta: 19:42:00  iter: 1319  total_loss: 1.291  loss_cls: 0.8024  loss_box_reg: 0.4232  time: 0.8081  data_time: 0.0207  lr: 0.01  max_mem: 32652M
[32m[10/19 17:52:37 d2.utils.events]: [0m eta: 19:41:51  iter: 1339  total_loss: 1.533  loss_cls: 1.026  loss_box_reg: 0.4641  time: 0.8081  data_time: 0.0164  lr: 0.01  max_mem: 32652M
[32m[10/19 17:52:53 d2.utils.events]: [0m eta: 19:41:35  iter: 1359  total_loss: 1.478  loss_cls: 0.9399  loss_box_reg: 0.5302  time: 0.8080  data_time: 0.0145  lr: 0.01  max_mem: 32652M
[32m[10/19 17:53:09 d2.utils.events]: [0m eta: 19:41:53  iter: 1379  total_loss: 1.273  loss_cls: 0.815  loss_box_reg: 0.4249  time: 0.8080  data_time: 0.0200  lr: 0.01  max_mem: 32652M
[32m[10/19 17:53:25 d2.utils.events]: [0m eta: 19:42:05  iter: 1399  total_loss: 1.342  loss_cls: 0.8672  loss_box_reg: 0.4359  time: 0.8081  data_time: 0.0120  lr: 0.01  max_mem: 32652M
[32m[10/19 17:53:42 d2.utils.events]: [0m eta: 19:42:51  iter: 1419  total_loss: 1.358  loss_cls: 0.8695  loss_box_reg: 0.4882  time: 0.8080  data_time: 0.0130  lr: 0.01  max_mem: 32652M
[32m[10/19 17:53:58 d2.utils.events]: [0m eta: 19:42:26  iter: 1439  total_loss: 1.285  loss_cls: 0.8451  loss_box_reg: 0.4554  time: 0.8079  data_time: 0.0137  lr: 0.01  max_mem: 32652M
[32m[10/19 17:54:14 d2.utils.events]: [0m eta: 19:42:33  iter: 1459  total_loss: 1.251  loss_cls: 0.8009  loss_box_reg: 0.4503  time: 0.8078  data_time: 0.0110  lr: 0.01  max_mem: 32652M
[32m[10/19 17:54:30 d2.utils.events]: [0m eta: 19:42:17  iter: 1479  total_loss: 1.253  loss_cls: 0.819  loss_box_reg: 0.4366  time: 0.8077  data_time: 0.0122  lr: 0.01  max_mem: 32652M
[32m[10/19 17:54:46 d2.utils.events]: [0m eta: 19:42:59  iter: 1499  total_loss: 1.242  loss_cls: 0.7898  loss_box_reg: 0.4399  time: 0.8078  data_time: 0.0171  lr: 0.01  max_mem: 32652M
[32m[10/19 17:55:03 d2.utils.events]: [0m eta: 19:43:15  iter: 1519  total_loss: 1.223  loss_cls: 0.7707  loss_box_reg: 0.4311  time: 0.8079  data_time: 0.0159  lr: 0.01  max_mem: 32652M
[32m[10/19 17:55:19 d2.utils.events]: [0m eta: 19:43:30  iter: 1539  total_loss: 1.243  loss_cls: 0.8061  loss_box_reg: 0.4295  time: 0.8080  data_time: 0.0102  lr: 0.01  max_mem: 32652M
[32m[10/19 17:55:35 d2.utils.events]: [0m eta: 19:42:52  iter: 1559  total_loss: 1.401  loss_cls: 0.897  loss_box_reg: 0.4433  time: 0.8077  data_time: 0.0153  lr: 0.01  max_mem: 32652M
[32m[10/19 17:55:51 d2.utils.events]: [0m eta: 19:42:39  iter: 1579  total_loss: 1.408  loss_cls: 0.9162  loss_box_reg: 0.499  time: 0.8076  data_time: 0.0176  lr: 0.01  max_mem: 32652M
[32m[10/19 17:56:07 d2.utils.events]: [0m eta: 19:42:26  iter: 1599  total_loss: 1.3  loss_cls: 0.8104  loss_box_reg: 0.4494  time: 0.8073  data_time: 0.0089  lr: 0.01  max_mem: 32652M
[32m[10/19 17:56:23 d2.utils.events]: [0m eta: 19:42:03  iter: 1619  total_loss: 1.189  loss_cls: 0.7764  loss_box_reg: 0.4074  time: 0.8069  data_time: 0.0102  lr: 0.01  max_mem: 32652M
[32m[10/19 17:56:39 d2.utils.events]: [0m eta: 19:41:25  iter: 1639  total_loss: 1.228  loss_cls: 0.7436  loss_box_reg: 0.4534  time: 0.8067  data_time: 0.0183  lr: 0.01  max_mem: 32652M
[32m[10/19 17:56:54 d2.utils.events]: [0m eta: 19:40:32  iter: 1659  total_loss: 1.274  loss_cls: 0.8547  loss_box_reg: 0.453  time: 0.8064  data_time: 0.0117  lr: 0.01  max_mem: 32652M
[32m[10/19 17:57:11 d2.utils.events]: [0m eta: 19:40:35  iter: 1679  total_loss: 1.354  loss_cls: 0.8604  loss_box_reg: 0.4787  time: 0.8063  data_time: 0.0178  lr: 0.01  max_mem: 32652M
[32m[10/19 17:57:26 d2.utils.events]: [0m eta: 19:39:52  iter: 1699  total_loss: 1.103  loss_cls: 0.7194  loss_box_reg: 0.3997  time: 0.8061  data_time: 0.0134  lr: 0.01  max_mem: 32652M
[32m[10/19 17:57:43 d2.utils.events]: [0m eta: 19:39:05  iter: 1719  total_loss: 1.173  loss_cls: 0.769  loss_box_reg: 0.4613  time: 0.8061  data_time: 0.0203  lr: 0.01  max_mem: 32652M
[32m[10/19 17:57:59 d2.utils.events]: [0m eta: 19:39:15  iter: 1739  total_loss: 1.211  loss_cls: 0.7864  loss_box_reg: 0.4332  time: 0.8061  data_time: 0.0160  lr: 0.01  max_mem: 32652M
[32m[10/19 17:58:15 d2.utils.events]: [0m eta: 19:38:18  iter: 1759  total_loss: 1.28  loss_cls: 0.8329  loss_box_reg: 0.4701  time: 0.8060  data_time: 0.0182  lr: 0.01  max_mem: 32652M
[4m[5m[31mERROR[0m [32m[10/19 17:58:29 d2.engine.train_loop]: [0mException during training:
Traceback (most recent call last):
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 287, in run_step
    self._write_metrics(loss_dict, data_time)
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 302, in _write_metrics
    SimpleTrainer.write_metrics(loss_dict, data_time, prefix)
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 338, in write_metrics
    raise FloatingPointError(
FloatingPointError: Loss became infinite or NaN at iteration=1776!
loss_dict = {'loss_cls': nan, 'loss_box_reg': nan}
[32m[10/19 17:58:29 d2.engine.hooks]: [0mOverall training speed: 1774 iterations in 0:23:50 (0.8065 s / it)
[32m[10/19 17:58:29 d2.engine.hooks]: [0mTotal training time: 0:24:01 (0:00:10 on hooks)
[32m[10/19 17:58:29 d2.utils.events]: [0m eta: 19:38:35  iter: 1776  total_loss: 1.518  loss_cls: 1.036  loss_box_reg: 0.5036  time: 0.8060  data_time: 0.0261  lr: 0.01  max_mem: 32652M
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:       data_time ▁▆▅▆▇▄▇▆▄▅▄▅▆▆▆▅▆▅▇▅▇▅▆▅▇▆▅▅▇▇▅▆▅▆▆▅▆▆▇█
wandb:     eta_seconds █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:    loss_box_reg █▃▄▄▄▃▃▃▄▃▃▃▂▁▂▃▂▂▁▂▃▂▂▁▂▁▂▂▂▁▂▁▁▁▁▂▁▂▂▂
wandb:        loss_cls █▄▅▅▅▄▄▄▃▃▂▂▁▁▁▂▂▂▁▂▃▂▂▂▂▂▂▂▁▁▂▂▁▁▁▂▁▂▁▂
wandb:              lr ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇███████████████████
wandb: num_pos_anchors ▆▂▂▃▂▂▇▅▂▂▆▄▄▄▆▆▇▁▅▂▅▂▂█▃▄▆▅▂▂▃▅▂▂▃▆▇▄▃▆
wandb:            time █▁▂▂▁▂▁▂▁▁▁▁▂▁▂▁▂▁▂▁▂▂▁▁▁▂▁▁▁▁▂▂▂▂▂▁▁▂▂▁
wandb:      total_loss █▄▅▄▄▄▄▄▃▃▂▂▂▁▁▂▂▂▁▂▃▂▂▂▁▁▂▂▂▁▂▂▁▁▁▂▁▂▁▂
wandb: 
wandb: Run summary:
wandb:       data_time 0.01183
wandb:     eta_seconds 70715.95312
wandb:     global_step 1776
wandb:    loss_box_reg 0.50362
wandb:        loss_cls 1.0359
wandb:              lr 0.01
wandb: num_pos_anchors 176.0
wandb:            time 0.79176
wandb:      total_loss 1.51775
wandb: 
wandb: Synced deft-sun-353: https://wandb.ai/drigoni/CATSS/runs/2mctb4gm
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20221019_172729-2mctb4gm/logs
Traceback (most recent call last):
  File "/ceph/hpc/scratch/user/eudavider/repository/DynamicHead/train_net.py", line 344, in <module>
    launch(
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/launch.py", line 67, in launch
    mp.spawn(
  File "/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/ceph/hpc/home/eudavider/.conda/envs/dynamicHead/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/ceph/hpc/scratch/user/eudavider/repository/DynamicHead/train_net.py", line 334, in main
    return trainer.train()
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 287, in run_step
    self._write_metrics(loss_dict, data_time)
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 302, in _write_metrics
    SimpleTrainer.write_metrics(loss_dict, data_time, prefix)
  File "/ceph/hpc/home/eudavider/.local/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 338, in write_metrics
    raise FloatingPointError(
FloatingPointError: Loss became infinite or NaN at iteration=1776!
loss_dict = {'loss_cls': nan, 'loss_box_reg': nan}

srun: error: gn23: task 0: Exited with exit code 1


Job done.
Date:  mer 19 ott 2022, 18.00.09, CEST
